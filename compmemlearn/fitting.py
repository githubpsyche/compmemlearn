# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/misc/08_Comparing_CMR_Sampling_Rules_Murdock1970.ipynb (unless otherwise specified).

__all__ = ['icmr_murd_likelihood', 'icmr_murd_objective_function', 'cmr_rep_likelihood', 'cmr_rep_objective_function',
           'icmr_rep_likelihood', 'icmr_rep_objective_function', 'cmr_murd_likelihood', 'cmr_murd_objective_function',
           'apply_and_concatenate']

# Cell

import numpy as np
from numba import njit
from .models import Instance_CMR

@njit(fastmath=True, nogil=True)
def icmr_murd_likelihood(data_to_fit, item_counts, encoding_drift_rate,
                    start_drift_rate, recall_drift_rate, shared_support,
                    item_support, learning_rate, primacy_scale, primacy_decay,
                    stop_probability_scale, stop_probability_growth,
                    choice_sensitivity, context_sensitivity, feature_sensitivity):
    """
    Generalized cost function for fitting the InstanceCMR model optimized
    using the numba library.

    Output scales inversely with the likelihood that the model and specified
    parameters would generate the specified trials. For model fitting, is
    usually wrapped in another function that fixes and frees parameters for
    optimization.

    **Arguments**:
    - data_to_fit: typed list of int64-arrays where rows identify a unique
        trial of responses and columns corresponds to a unique recall index.
    - A configuration for each parameter of `Instance_CMR`.

    **Returns** the negative sum of log-likelihoods across specified trials
    conditional on the specified parameters and the mechanisms of InstanceCMR.
    """

    result = 0.0
    for i in range(len(item_counts)):
        item_count = item_counts[i]
        trials = data_to_fit[i]

        model = Instance_CMR(item_count, item_count, encoding_drift_rate,
                          start_drift_rate, recall_drift_rate, shared_support,
                          item_support, learning_rate, primacy_scale,
                          primacy_decay, stop_probability_scale,
                          stop_probability_growth, choice_sensitivity,
                           context_sensitivity, feature_sensitivity)

        model.experience(np.eye(item_count, item_count + 1, 1))

        likelihood = np.ones((len(trials), item_count))

        for trial_index in range(len(trials)):
            trial = trials[trial_index]

            model.force_recall()
            for recall_index in range(len(trial) + 1):

                # identify index of item recalled; if zero then recall is over
                if recall_index == len(trial) and len(trial) < item_count:
                    recall = 0
                else:
                    recall = trial[recall_index]

                # store probability of and simulate recalling item w this index
                activation_cue = np.hstack(
                    (np.zeros(model.item_count + 1), model.context))
                likelihood[trial_index, recall_index] = \
                    model.outcome_probabilities(activation_cue)[recall]

                if recall == 0:
                    break
                model.force_recall(recall)

            # reset model to its pre-retrieval (but post-encoding) state
            model.force_recall(0)
        result -= np.sum(np.log(likelihood))

    return result

# Cell

def icmr_murd_objective_function(data_to_fit, fixed_parameters, free_parameters):
    """
    Generates and returns an objective function for input to support search
    through parameter space for ICMR model fit using an optimization function.

    Arguments:
    - fixed_parameters: dictionary mapping parameter names to values they'll
        be fixed to during search, overloaded by free_parameters if overlap
    - free_parameters: list of strings naming parameters for fit during search
    - data_to_fit: array where rows identify a unique trial of responses and
        columns corresponds to a unique recall index

    Returns a function that accepts a vector x specifying arbitrary values for
    free parameters and returns evaluation of icmr_likelihood using the model
    class, all parameters, and provided data.
    """

    return lambda x: icmr_murd_likelihood(data_to_fit, **{**fixed_parameters, **{
        free_parameters[i]:x[i] for i in range(len(x))}})

# Cell
# hide

import numpy as np
from numba import njit
from .models import Classic_CMR

@njit(fastmath=True, nogil=True)
def cmr_rep_likelihood(
        trials, presentations, list_types, list_length, encoding_drift_rate, start_drift_rate,
        recall_drift_rate, shared_support, item_support, learning_rate,
        primacy_scale, primacy_decay, stop_probability_scale,
        stop_probability_growth, choice_sensitivity, familiarity_scale, sampling_rule):
    """
    Generalized cost function for fitting the InstanceCMR model optimized
    using the numba library.

    Output scales inversely with the likelihood that the model and specified
    parameters would generate the specified trials. For model fitting, is
    usually wrapped in another function that fixes and frees parameters for
    optimization.

    **Arguments**:
    - data_to_fit: typed list of int64-arrays where rows identify a unique
        trial of responses and columns corresponds to a unique recall index.
    - A configuration for each parameter of `InstanceCMR` as delineated in
        `Formal Specification`.

    **Returns** the negative sum of log-likelihoods across specified trials
    conditional on the specified parameters and the mechanisms of InstanceCMR.
    """

    likelihood = np.ones((len(trials), list_length))

    # we can use the same model for list types 1 and 2
    stable_models = [Classic_CMR(
            list_length, list_length, encoding_drift_rate, start_drift_rate,
            recall_drift_rate, shared_support, item_support, learning_rate,
            primacy_scale, primacy_decay, stop_probability_scale,
            stop_probability_growth, choice_sensitivity, familiarity_scale, sampling_rule),
                    Classic_CMR(
            int(list_length/2), list_length, encoding_drift_rate, start_drift_rate,
            recall_drift_rate, shared_support, item_support, learning_rate,
            primacy_scale, primacy_decay, stop_probability_scale,
            stop_probability_growth, choice_sensitivity, familiarity_scale, sampling_rule)]
    stable_models[0].experience(np.eye(list_length, list_length))
    stable_models[1].experience(np.eye(int(list_length/2), int(list_length/2))[np.repeat(np.arange(int(list_length/2)), 2)])

    for trial_index in range(len(trials)):

        item_count = np.max(presentations[trial_index])+1

        if list_types[trial_index] > 2:
            model = Classic_CMR(
                item_count, list_length, encoding_drift_rate, start_drift_rate,
                recall_drift_rate, shared_support, item_support, learning_rate,
                primacy_scale, primacy_decay, stop_probability_scale,
                stop_probability_growth, choice_sensitivity, familiarity_scale, sampling_rule)

            model.experience(np.eye(item_count, item_count)[presentations[trial_index]])
        else:
            model = stable_models[list_types[trial_index]-1]

        trial = trials[trial_index]

        model.force_recall()
        for recall_index in range(len(trial) + 1):

            # identify index of item recalled; if zero then recall is over
            if recall_index == len(trial) and len(trial) < item_count:
                recall = 0
            elif trial[recall_index] == 0:
                recall = 0
            else:
                recall = presentations[trial_index][trial[recall_index]-1] + 1

            # store probability of and simulate recalling item with this index
            likelihood[trial_index, recall_index] = \
                model.outcome_probabilities(model.context)[recall]

            if recall == 0:
                break
            model.force_recall(recall)

        # reset model to its pre-retrieval (but post-encoding) state
        model.force_recall(0)

    return -np.sum(np.log(likelihood))

def cmr_rep_objective_function(data_to_fit, presentations, list_types, list_length, fixed_parameters, free_parameters):
    """
    Generates and returns an objective function for input to support search
    through parameter space for ICMR model fit using an optimization function.

    Arguments:
    - fixed_parameters: dictionary mapping parameter names to values they'll
        be fixed to during search, overloaded by free_parameters if overlap
    - free_parameters: list of strings naming parameters for fit during search
    - data_to_fit: array where rows identify a unique trial of responses and
        columns corresponds to a unique recall index

    Returns a function that accepts a vector x specifying arbitrary values for
    free parameters and returns evaluation of icmr_likelihood using the model
    class, all parameters, and provided data.
    """
    return lambda x: cmr_rep_likelihood(data_to_fit, presentations, list_types, list_length, **{**fixed_parameters, **{
        free_parameters[i]:x[i] for i in range(len(x))}})

# Cell
# hide

from .models import Instance_CMR
import numpy as np
from numba import njit

@njit(fastmath=True, nogil=True)
def icmr_rep_likelihood(
        trials, presentations, list_types, list_length, encoding_drift_rate, start_drift_rate,
        recall_drift_rate, shared_support, item_support, learning_rate,
        primacy_scale, primacy_decay, stop_probability_scale,
        stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity):
    """
    Generalized cost function for fitting the InstanceCMR model optimized
    using the numba library.

    Output scales inversely with the likelihood that the model and specified
    parameters would generate the specified trials. For model fitting, is
    usually wrapped in another function that fixes and frees parameters for
    optimization.

    **Arguments**:
    - data_to_fit: typed list of int64-arrays where rows identify a unique
        trial of responses and columns corresponds to a unique recall index.
    - A configuration for each parameter of `InstanceCMR` as delineated in
        `Formal Specification`.

    **Returns** the negative sum of log-likelihoods across specified trials
    conditional on the specified parameters and the mechanisms of InstanceCMR.
    """

    likelihood = np.ones((len(trials), list_length))

    # we can use the same model for list types 1 and 2
    stable_models = [Instance_CMR(
            list_length, list_length, encoding_drift_rate, start_drift_rate,
            recall_drift_rate, shared_support, item_support, learning_rate,
            primacy_scale, primacy_decay, stop_probability_scale,
            stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity),
                    Instance_CMR(
            int(list_length/2), list_length, encoding_drift_rate, start_drift_rate,
            recall_drift_rate, shared_support, item_support, learning_rate,
            primacy_scale, primacy_decay, stop_probability_scale,
            stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity)]
    stable_models[0].experience(np.eye(list_length, list_length + 1, 1))
    stable_models[1].experience(np.eye(int(list_length/2), int(list_length/2) + 1, 1)[np.repeat(np.arange(int(list_length/2)), 2)])

    for trial_index in range(len(trials)):

        item_count = np.max(presentations[trial_index])+1

        if list_types[trial_index] > 2:
            model = Instance_CMR(
                item_count, list_length, encoding_drift_rate, start_drift_rate,
                recall_drift_rate, shared_support, item_support, learning_rate,
                primacy_scale, primacy_decay, stop_probability_scale,
                stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity)

            model.experience(np.eye(item_count, item_count + 1, 1)[presentations[trial_index]])
        else:
            model = stable_models[list_types[trial_index]-1]

        trial = trials[trial_index]

        model.force_recall()
        for recall_index in range(len(trial) + 1):

            # identify index of item recalled; if zero then recall is over
            if recall_index == len(trial) and len(trial) < item_count:
                recall = 0
            elif trial[recall_index] == 0:
                recall = 0
            else:
                recall = presentations[trial_index][trial[recall_index]-1] + 1

            # store probability of and simulate recalling item with this index
            activation_cue = np.hstack(
                    (np.zeros(model.item_count + 1), model.context))
            likelihood[trial_index, recall_index] = \
                model.outcome_probabilities(activation_cue)[recall]

            if recall == 0:
                break
            model.force_recall(recall)

        # reset model to its pre-retrieval (but post-encoding) state
        model.force_recall(0)

    return -np.sum(np.log(likelihood))

def icmr_rep_objective_function(data_to_fit, presentations, list_types, list_length, fixed_parameters, free_parameters):
    """
    Generates and returns an objective function for input to support search
    through parameter space for ICMR model fit using an optimization function.

    Arguments:
    - fixed_parameters: dictionary mapping parameter names to values they'll
        be fixed to during search, overloaded by free_parameters if overlap
    - free_parameters: list of strings naming parameters for fit during search
    - data_to_fit: array where rows identify a unique trial of responses and
        columns corresponds to a unique recall index

    Returns a function that accepts a vector x specifying arbitrary values for
    free parameters and returns evaluation of icmr_likelihood using the model
    class, all parameters, and provided data.
    """
    return lambda x: icmr_rep_likelihood(data_to_fit, presentations, list_types, list_length, **{**fixed_parameters, **{
        free_parameters[i]:x[i] for i in range(len(x))}})

# Cell
# hide

import numpy as np
from numba import njit
from .models import Classic_CMR

@njit(nogil=True) #@njit(fastmath=True, nogil=True)
def cmr_murd_likelihood(
    data_to_fit, item_counts, encoding_drift_rate, start_drift_rate,
    recall_drift_rate, shared_support, item_support, learning_rate,
    primacy_scale, primacy_decay, stop_probability_scale,
    stop_probability_growth, choice_sensitivity, familiarity_scale,
    sampling_rule):
    """
    Cost function for fitting the CMR model.

    Output scales inversely with the likelihood that the model and specified
    parameters would generate the specified trials. For model fitting, is
    usually wrapped in another function that fixes and frees parameters for
    optimization.

    **Arguments**:
    - trials: int64-array where rows identify a unique trial of responses and
      columns corresponds to a unique recall index.
    - A configuration for each parameter of `CMR`.

    **Returns** the negative sum of log-likelihoods across specified trials
    conditional on the specified parameters and the mechanisms of InstanceCMR.
    """

    result = 0.0
    for i in range(len(item_counts)):
        item_count = item_counts[i]
        trials = data_to_fit[i]

        model = Classic_CMR(item_count, item_count, encoding_drift_rate,
                    start_drift_rate, recall_drift_rate, shared_support,
                    item_support, learning_rate, primacy_scale,
                    primacy_decay, stop_probability_scale,
                    stop_probability_growth, choice_sensitivity,
                            familiarity_scale, sampling_rule)

        model.experience(np.eye(item_count, item_count))

        likelihood = np.ones((len(trials), item_count))

        for trial_index in range(len(trials)):
            trial = trials[trial_index]

            model.force_recall()
            for recall_index in range(len(trial) + 1):

                # identify index of item recalled; if zero then recall is over
                if recall_index == len(trial) and len(trial) < item_count:
                    recall = 0
                else:
                    recall = trial[recall_index]

                # store probability of and simulate recall of indexed item
                likelihood[trial_index, recall_index] = \
                    model.outcome_probabilities(model.context)[recall] + 10e-7
                #if np.isnan(np.sum(np.log(likelihood))):
                #    assert(False)
                #    likelihood[trial_index, recall_index] = \
                #        model.outcome_probabilities(model.context)[recall] + 10e-7

                if recall == 0:
                    break
                model.force_recall(recall)

            # reset model to its pre-retrieval (but post-encoding) state
            model.force_recall(0)

        result -= np.sum(np.log(likelihood))

    return result

# Cell
# hide

def cmr_murd_objective_function(data_to_fit, fixed_parameters, free_parameters):
    """
    Configures cmr_likelihood for search over specified free/fixed parameters.
    """
    return lambda x: cmr_murd_likelihood(data_to_fit, **{**fixed_parameters, **{
        free_parameters[i]:x[i] for i in range(len(x))}})

# Cell
import pandas as pd

def apply_and_concatenate(function, df1, df2, contrast_name='contrast', labels='AB'):
    """
    Concatenates the results of a function applied to two dataframes and creates a new column identifying the contrast.
    """
    return pd.concat([function(df1), function(df2)], keys=labels, names=[contrast_name]).reset_index()