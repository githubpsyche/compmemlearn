# AUTOGENERATED! DO NOT EDIT! File to edit: library/analyses/Stop_Probability_by_Serial_Position.ipynb (unless otherwise specified).

__all__ = ['alternative_contiguity', 'fast_sem_crp', 'plot_sem_crp', 'fast_crp', 'flex_mixed_crp', 'plot_crp',
           'masked_crp', 'randomize_dataset', 'indices_of_repeated_items', 'alternative_contiguity_test',
           'alternative_contiguity_control', 'sim_alternative_contiguity_test', 'fast_pfr', 'flex_mixed_pfr',
           'plot_pfr', 'fast_rpl', 'plot_rpl', 'rpl', 'fast_sem_pfr', 'plot_sem_pfr', 'recall_by_all_study_positions',
           'fast_spc', 'flex_mixed_spc', 'plot_spc', 'fast_csp', 'plot_csp']

# Cell

from numba import njit
from numba import int32
import numpy as np

@njit(nogil=True)
def alternative_contiguity(trials, presentations, lag_threshold = 3, max_repeats = 2):

    list_length = len(presentations[0])
    lag_range = list_length - 1
    total_actual_lags = np.zeros((max_repeats, lag_range * 2 + 1)) # extended dimension to split by pres positions
    total_possible_lags = np.zeros((max_repeats, lag_range * 2 + 1))
    terminus = np.sum(trials != 0, axis=1) # number of recalls per trial
    recall_by_study_position = recall_by_all_study_positions(trials, presentations, max_repeats)

    for trial_index in range(len(trials)):
        previous_item = 0
        item_count = np.max(presentations[trial_index]) + 1
        possible_items = np.arange(item_count) # initial pool of possible recalls, 1-indexed
        possible_positions = np.zeros((item_count, max_repeats), dtype=int32)

        # we track possible positions using presentations and alt_presentations
        for item in range(item_count):
            pos = np.nonzero(presentations[trial_index] == item)[0] + 1
            possible_positions[item, :len(pos)] = pos

        for recall_index in range(terminus[trial_index]):

            current_item = presentations[trial_index][trials[trial_index, recall_index]-1]

            # track possible and actual lags;
            # focus only on transitions from items with > 1 study positions
            # and only when those multiple study positions have lag over lag
            if recall_index > 0 and np.count_nonzero(
                possible_positions[previous_item]) > 1 and (
                possible_positions[previous_item][1] - possible_positions[previous_item][0] >= lag_threshold):

                # item indices don't help track lags anymore
                # so more complex calculation needed to identify possible lags given previous item
                current_index = np.nonzero(possible_items==current_item)[0]

                index = 0
                for x in range(len(recall_by_study_position)):
                    for y in range(len(recall_by_study_position)):
                        if possible_positions[previous_item, y] > 0:

                            possible_lags = possible_positions[
                                possible_items, x] - possible_positions[previous_item, y]

                            # if tracked position is 0, then we don't actually want to count it in our lags
                            possible_lags[possible_positions[possible_items, x] == 0] = 0

                            # we track actual lag at each iteration
                            actual_lag = possible_lags[current_index] + lag_range
                            total_actual_lags[y][actual_lag] += 1

                            # we track possible lag at each iteration
                            possible_lags += lag_range
                            total_possible_lags[y][possible_lags] += 1

                        index += 1

            # update pool to exclude recalled item (updated to still identify 1-indexed item)
            previous_item = current_item
            possible_items = possible_items[possible_items != previous_item]


    # small correction to avoid nans and commit to excluding multiply-tracked single presentations
    total_actual_lags[:, lag_range] = 0
    for i in range(max_repeats):
        total_possible_lags[i][total_actual_lags[i]==0] += 1

    return total_actual_lags/total_possible_lags

# Cell
#| code-summary: -- specify semantic contiguity effect analysis code

from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy.spatial.distance import squareform
from sentence_transformers import util
from numba import njit

@njit(nogil=True)
def fast_sem_crp(trials, distances, item_count, edges):

    total_actual_transitions = np.zeros(len(edges))
    total_possible_transitions = np.zeros(len(edges))
    terminus = np.sum(trials != 0, axis=1)

    # tabulate bin totals for actual and possible distance transitions
    for trial_index in range(len(trials)):
        previous_item = 0
        possible_items = np.arange(item_count)

        # track possible and actual transitions across recalls
        for recall_index in range(terminus[trial_index]):
            current_item = trials[trial_index, recall_index]-1

            if recall_index > 0:

                # compute edge bin for distance between previous and each possible item
                index = 0
                for possible_index in range(len(possible_items)):
                    possible_item = possible_items[possible_index]
                    distance = distances[trial_index, possible_item, previous_item]
                    distance_bin = np.sum(edges < distance, dtype=np.int32)
                    total_possible_transitions[distance_bin] += 1
                    if possible_item == current_item:
                        total_actual_transitions[distance_bin] += 1

            # update pool to exclude recalled item (updated to still identify 1-indexed item)
            previous_item = current_item
            possible_items = possible_items[possible_items != previous_item]

    return total_actual_transitions/total_possible_transitions

# Cell
#| code-summary: -- specify semantic contiguity effect plotting code

def plot_sem_crp(
    data, trial_query, string_embeddings, contrast_name=None, labels=None, axis=None):

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, _, string_ids = events_metadata(events)[:4]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        list_length = list_lengths[chose]
        string_ids = string_ids[chose]
        trial_mask = trial_mask[chose]

        # set edges based on distances between unique string_ids in current dataset
        if data_index == 0:
            distances = 1-util.pytorch_cos_sim(string_embeddings, string_embeddings).numpy()
            distances[np.eye(len(distances), dtype=int)] = np.nan
            edges = np.nanpercentile(distances, np.linspace(1, 100, 10))

        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            # compute distances over sampled trials
            item_count = np.shape(string_ids)[1]
            distances = np.zeros((np.sum(subject_specific_trial_mask), item_count, item_count))
            for trial_index, trial in enumerate(trials[subject_specific_trial_mask]):
                trial_embeddings = string_embeddings[string_ids[subject_specific_trial_mask][trial_index]]
                cosine_scores = 1-util.pytorch_cos_sim(trial_embeddings, trial_embeddings).numpy()
                np.fill_diagonal(cosine_scores, np.nan)
                distances[trial_index, :len(trial_embeddings), :len(trial_embeddings)] = cosine_scores

            res = fast_sem_crp(trials[subject_specific_trial_mask], distances, item_count, edges)
            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "bin": edges,
                    "prob": res,
                    contrast_name: labels[data_index],
                }
            ))

    result = pd.concat(result).reset_index()
    sns.lineplot(ax=axis, data=result, x='bin', y='prob', err_style='bars', hue=contrast_name, legend=False)
    axis.set(xlabel='Semantic Distance', ylabel='Conditional Recall Rate')

    if contrast_name:
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

    return axis, result

# Cell
#| code-summary: -- specify lag-contiguity effect analysis code

from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
from numba import njit
import seaborn as sns
import pandas as pd
import numpy as np

@njit(fastmath=True, nogil=True)
def fast_crp(trials, item_count):

    lag_range = item_count - 1
    total_actual_lags = np.zeros(lag_range * 2 + 1)
    total_possible_lags = np.zeros(lag_range * 2 + 1)
    terminus = np.sum(trials != 0, axis=1)

    # compute actual serial lag b/t recalls
    actual_lags = trials[:, 1:] - trials[:, :-1]
    actual_lags += lag_range

    # tabulate bin totals for actual and possible lags
    for i in range(len(trials)):
        possible_items = np.arange(item_count) + 1
        previous_item = 0

        for recall_index in range(terminus[i]):

            # track possible and actual lags
            if recall_index > 0:
                total_actual_lags[actual_lags[i, recall_index-1]] += 1

                # exploit equivalence b/t item index and study position to track possible lags
                possible_lags = possible_items - previous_item
                possible_lags += lag_range
                total_possible_lags[possible_lags] += 1

            # update pool of possible items to exclude recalled item
            previous_item = trials[i, recall_index]
            possible_items = possible_items[possible_items != previous_item]

    # small correction to avoid nans
    total_possible_lags[total_actual_lags==0] += 1

    return total_actual_lags/total_possible_lags

# Cell
#| code-summary: -- specify lag-contiguity effect analysis code

@njit(nogil=True)
def flex_mixed_crp(trials, presentations):

    list_length = len(presentations[0])
    max_repeats = int(np.ceil(np.max(np.bincount(
        presentations.ravel())/len(presentations))))
    lag_range = list_length - 1
    total_actual_lags = np.zeros(lag_range * 2 + 1)
    total_possible_lags = np.zeros(lag_range * 2 + 1)
    terminus = np.sum(trials != 0, axis=1) # number of recalls per trial
    recall_by_study_position = recall_by_all_study_positions(trials, presentations, max_repeats)

    for trial_index in range(len(trials)):

        previous_item = 0
        item_count = np.max(presentations[trial_index]) + 1
        possible_items = np.arange(item_count) # initial pool of possible recalls, 1-indexed
        possible_positions = np.zeros((item_count, max_repeats), dtype=np.int32)

        # we track possible positions using presentations and alt_presentations
        for item in range(item_count):
            pos = np.nonzero(presentations[trial_index] == item)[0] + 1
            possible_positions[item, :len(pos)] = pos

        for recall_index in range(terminus[trial_index]):

            current_item = presentations[trial_index][trials[trial_index, recall_index]-1]

            # track possible and actual lags
            if recall_index > 0:

                # item indices don't help track lags anymore
                # so more complex calculation needed to identify possible lags given previous item
                current_index = np.nonzero(possible_items==current_item)[0]
                possible_lags = np.zeros((len(recall_by_study_position) ** 2, len(possible_items)), dtype=np.int32)

                index = 0
                for x in range(len(recall_by_study_position)):
                    for y in range(len(recall_by_study_position)):
                        if possible_positions[previous_item, y] > 0:

                            possible_lags[index] = possible_positions[
                                possible_items, x] - possible_positions[previous_item, y]

                            # if tracked position is 0, then we don't actually want to count it in our lags
                            possible_lags[index][possible_positions[possible_items, x] == 0] = 0

                        index += 1
                possible_lags += lag_range

                total_actual_lags[possible_lags[:, current_index].flatten()] += 1
                total_possible_lags[possible_lags.flatten()] += 1


            # update pool to exclude recalled item (updated to still identify 1-indexed item)
            previous_item = current_item
            possible_items = possible_items[possible_items != previous_item]

    # small correction to avoid nans and commit to excluding multiply-tracked single presentations
    total_actual_lags[lag_range] = 0
    total_possible_lags[total_actual_lags==0] += 1

    return total_actual_lags/total_possible_lags

# Cell
#| code-summary: -- specify lag-contiguity effect analysis code

def plot_crp(data, trial_query, contrast_name=None, labels=None, axis=None, max_lag=5, handle_repetitions=None, mask=None):

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, presentations = events_metadata(events)[:3]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        list_length = list_lengths[chose]
        presentations = presentations[chose]
        trial_mask = trial_mask[chose]

        lag_interval = np.arange(-max_lag, max_lag+1)
        lag_range = list_length -1
        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            if mask is None:
                if handle_repetitions is None:
                    handle_repetitions = not (presentations == np.arange(list_length)).all()

                if not handle_repetitions:
                    res = fast_crp(trials[subject_specific_trial_mask], list_length)
                else:
                    res = flex_mixed_crp(
                        trials[subject_specific_trial_mask], presentations[subject_specific_trial_mask])
            else:
                res = masked_crp(trials[subject_specific_trial_mask], list_length, mask)

            res[lag_range] = np.nan
            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "lag": lag_interval,
                    "prob": res[lag_range-max_lag:lag_range+max_lag+1],
                    contrast_name: labels[data_index],
                    "positive": lag_interval > 0,
                }
            ))

    result = pd.concat(result).reset_index()
    sns.lineplot(
        ax=axis, data=result, x='lag', y='prob', err_style='bars', hue=contrast_name, legend=False, style="positive", dashes=False)

    axis.set(xlabel='Lag', ylabel='Conditional Resp. Prob.')
    axis.set_xticks(np.arange(-max_lag, max_lag+1, 1))
    axis.set_ylim((0, 1))

    if contrast_name:
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

    return axis, result

# Cell
#| code-summary: -- specify masked lag-contiguity effect analysis code

from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
from numba import njit
import seaborn as sns
import pandas as pd
import numpy as np

@njit(fastmath=True, nogil=True)
def masked_crp(trials, item_count, mask):

    lag_range = item_count - 1
    total_actual_lags = np.zeros(lag_range * 2 + 1)
    total_possible_lags = np.zeros(lag_range * 2 + 1)
    terminus = np.sum(trials != 0, axis=1)

    # compute actual serial lag b/t recalls
    actual_lags = trials[:, 1:] - trials[:, :-1]
    actual_lags += lag_range

    # tabulate bin totals for actual and possible lags
    for i in range(len(trials)):
        possible_items = np.arange(item_count) + 1
        previous_item = 0

        for recall_index in range(terminus[i]):

            # track possible and actual lags
            if recall_index > 0 and mask[i, recall_index-1]:
                total_actual_lags[actual_lags[i, recall_index-1]] += 1

                # exploit equivalence b/t item index and study position to track possible lags
                possible_lags = possible_items - previous_item
                possible_lags += lag_range
                total_possible_lags[possible_lags] += 1

            # update pool of possible items to exclude recalled item
            previous_item = trials[i, recall_index]
            possible_items = possible_items[possible_items != previous_item]

    # small correction to avoid nans
    total_possible_lags[total_actual_lags==0] += 1

    return total_actual_lags/total_possible_lags

# Cell

from numba import njit, prange
import numpy as np

@njit(parallel=True)
def randomize_dataset(recall_rate_by_serial_position, sample_size):
    """
    To control for serial position effects of repeated items,
    analyses are often performed on randomized recall sequences with recall probabilities
    matched to the serial position curve of controls lists. Recall of each item is then
    calculated randomly and independently across trials.
    """

    samples = np.random.rand(sample_size, len(recall_rate_by_serial_position)) < recall_rate_by_serial_position
    result = np.zeros((sample_size, len(recall_rate_by_serial_position)))

    for i in prange(sample_size):
        sample = samples[i].nonzero()[0] + 1
        np.random.shuffle(sample)
        result[i, :len(sample)] = sample

    return result


# Cell

import pandas as pd
import numpy as np

def indices_of_repeated_items(presentation_sequence):

    values, counts = np.unique(presentation_sequence, return_counts=True)
    repeated_items = {v: np.where(presentation_sequence == v)[0] for v in values if counts[v] > 1}

    return {key:repeated_items[key] for key in repeated_items}


def alternative_contiguity_test(mixed_presentations, mixed_recalls, lag_threshold, repetition_count):
    relevant_lags = list(range(-int(lag_threshold/2), int(lag_threshold/2+1)))
    del relevant_lags[int(lag_threshold/2)]

    actual_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]
    possible_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]

    for trial_index in range(len(mixed_presentations)):

        # sequence of item indices ordered as they were studied
        presentation = mixed_presentations[trial_index]

        # sequence of initial study positions ordered as they were recalled
        trial_by_study_position = mixed_recalls[trial_index]

        # sequence of item indices ordered as they were recalled
        trial_by_item_index = presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]

        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items
        i_and_j = indices_of_repeated_items(presentation)

        # then for each unique repeated item in the study list,
        for repeated_item in i_and_j:

            # search for relevant item(s) in recall sequence and skip if not found
            look_for = trial_by_item_index == presentation[i_and_j[repeated_item][0]]
            for k in range(1, repetition_count):
                look_for = np.logical_or(
                    look_for, trial_by_item_index == presentation[i_and_j[repeated_item][k]])
            recall_positions = np.where(look_for)[0]

            if np.size(recall_positions) == 0:
                continue

            # check each position the item was observed (always just 1 position; we loop for parallelism w control)
            for recall_position in recall_positions:

                # also skip if no successive recall was made,
                if np.size(trial_by_item_index) == recall_position + 1:
                    continue

                # build list of study positions for items recalled up to repeated item
                prior_lags = [[] for each in range(repetition_count)]
                for i in range(recall_position):

                    # if considered item is also repeated, we track lags wrt to all presentations
                    if trial_by_item_index[i] in i_and_j:
                        for considered in range(len(i_and_j[trial_by_item_index[i]])):
                            for focal in range(repetition_count):
                                prior_lags[focal].append(
                                    int(i_and_j[trial_by_item_index[i]][considered] - i_and_j[repeated_item][focal]))
                    else:
                        for k in range(repetition_count):
                            prior_lags[k].append(int(trial_by_study_position[i] - i_and_j[repeated_item][k]))

                # transition of a given lag is possible if lag not present in prior_lags
                for lag in relevant_lags:
                    for k in range(repetition_count):

                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][-1] < lag_threshold
                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):
                            continue

                        if lag not in prior_lags[k]:
                            possible_lags[k][relevant_lags.index(lag)] += 1

                # track each serial lag of actually transitioned-to item
                if trial_by_item_index[recall_position+1] in i_and_j:
                    positions = i_and_j[trial_by_item_index[recall_position+1]]
                    for transition_study_position in positions:
                        for k in range(repetition_count):

                            # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold
                            if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):
                                continue

                            lag = int(transition_study_position - i_and_j[repeated_item][k])
                            if lag in relevant_lags:
                                actual_lags[k][relevant_lags.index(lag)] += 1
                else:
                    transition_study_position = trial_by_study_position[recall_position+1]-1
                    for k in range(repetition_count):

                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold
                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):
                            continue

                        lag = int(transition_study_position-i_and_j[repeated_item][k])
                        if lag in relevant_lags:
                            actual_lags[k][relevant_lags.index(lag)] += 1

    result = []
    for k in range(repetition_count):

        for i in range(len(possible_lags[k])):
            if possible_lags[k][i] == 0:
                possible_lags[k][i] += 1

        result.append(pd.DataFrame(
            {'lag': relevant_lags, 'prob': np.divide(actual_lags[k], possible_lags[k]),
            'actual': actual_lags[k], 'possible': possible_lags[k]}))

    return pd.concat(result, keys=['From Position {}'.format(i+1) for i in range(repetition_count)], names=['locus'])


def alternative_contiguity_control(
    mixed_presentations, control_presentations, control_recalls, lag_threshold, repetition_count):
    relevant_lags = list(range(-int(lag_threshold/2), int(lag_threshold/2+1)))
    del relevant_lags[int(lag_threshold/2)]

    actual_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]
    possible_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]

    for trial_index in range(len(mixed_presentations)):

        # sequence of item indices ordered as they were studied
        presentation = mixed_presentations[trial_index]
        control_presentation = control_presentations[trial_index]

        # sequence of initial study positions ordered as they were recalled
        trial_by_study_position = control_recalls[trial_index]

        # sequence of item indices ordered as they were recalled
        trial_by_item_index = control_presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]

        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items
        i_and_j = indices_of_repeated_items(presentation)

        # then for each unique repeated item in the study list,
        for repeated_item in i_and_j:

            # search for relevant item(s) in recall sequence and skip if not found
            look_for = trial_by_item_index == control_presentation[i_and_j[repeated_item][0]]
            for k in range(1, repetition_count):
                look_for = np.logical_or(
                    look_for, trial_by_item_index == control_presentation[i_and_j[repeated_item][k]])
            recall_positions = np.where(look_for)[0]

            for recall_position in recall_positions:

                # also skip if no successive recall was made,
                if np.size(trial_by_item_index) == recall_position + 1:
                    continue

                # build list of study positions for items recalled up to repeated item
                prior_lags = [[] for each in range(repetition_count)]
                for i in range(recall_position):
                    if trial_by_item_index[i] in i_and_j:
                        for considered in range(len(i_and_j[trial_by_item_index[i]])):
                            for focal in range(repetition_count):
                                prior_lags[focal].append(
                                    int(i_and_j[trial_by_item_index[i]][considered] - i_and_j[repeated_item][focal]))
                    else:
                        for k in range(repetition_count):
                            prior_lags[k].append(int(trial_by_study_position[i] - i_and_j[repeated_item][k]))

                # transition of a given lag is possible if lag not present in prior_lags
                for lag in relevant_lags:
                    for k in range(repetition_count):
                        if lag not in prior_lags[k]:
                            possible_lags[k][relevant_lags.index(lag)] += 1

                # track each serial lag of actually transitioned-to item
                if trial_by_item_index[recall_position+1] in i_and_j:
                    positions = i_and_j[trial_by_item_index[recall_position+1]]
                    for transition_study_position in positions:
                        for k in range(repetition_count):
                            lag = int(transition_study_position - i_and_j[repeated_item][k])
                            if lag in relevant_lags:
                                actual_lags[k][relevant_lags.index(lag)] += 1
                else:
                    transition_study_position = trial_by_study_position[recall_position+1]-1
                    for k in range(repetition_count):
                        lag = int(transition_study_position-i_and_j[repeated_item][k])
                        if lag in relevant_lags:
                            actual_lags[k][relevant_lags.index(lag)] += 1

    result = []
    for k in range(repetition_count):
        result.append(pd.DataFrame(
            {'lag': relevant_lags, 'prob': np.divide(actual_lags[k], possible_lags[k]),
            'actual': actual_lags[k], 'possible': possible_lags[k]}))

    return pd.concat(result, keys=['From Position {}'.format(i+1) for i in range(repetition_count)], names=['locus'])

def sim_alternative_contiguity_test(presentations, experiment_count, lag_threshold, repetition_count, encoding_drift_rate,
    start_drift_rate, recall_drift_rate, shared_support, item_support, learning_rate,
    primacy_scale, primacy_decay, stop_probability_scale,
    stop_probability_growth, choice_sensitivity, delay_drift_rate,
    drift_familiarity_scale, mfc_familiarity_scale, mcf_familiarity_scale, sampling_rule):
    """
    Apply organizational analyses to visually compare the behavior of the model
    with these parameters against specified dataset.
    """

    results = []
    # generate simulation data from model
    for experiment in range(experiment_count):

        sim = np.zeros(np.shape(presentations), dtype=int)
        for trial_index in range(len(presentations)):
            presentation = presentations[trial_index]

            item_count = np.max(presentation)+1
            model = Classic_CMR( item_count, len(presentations),  encoding_drift_rate,
                start_drift_rate, recall_drift_rate, shared_support, item_support, learning_rate,
                primacy_scale, primacy_decay, stop_probability_scale,
                stop_probability_growth, choice_sensitivity, delay_drift_rate, drift_familiarity_scale,
                 mfc_familiarity_scale, mcf_familiarity_scale, sampling_rule)

            # simulate study events
            model.experience(np.eye(model.item_count, model.item_count)[presentation])

            # simulate and add recall events to trials array
            recalled = model.free_recall()
            xsorted = np.argsort(presentation)
            ypos = np.searchsorted(presentation[xsorted], recalled)
            sim[trial_index, :len(recalled)] = xsorted[ypos]+1

        # apply contiguity test
        results.append(alternative_contiguity_test(presentations, sim, lag_threshold, repetition_count))

    return pd.concat(results, keys=list(range(experiment_count)), names=['experiment']).reset_index()

# Cell
#| code-summary: -- specify serial position effect analysis code

from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
from numba import njit
import seaborn as sns
import pandas as pd
import numpy as np

@njit(nogil=True)
def fast_pfr(trials, item_count):
    return np.bincount(trials[:, 0], minlength=item_count + 1)[1:] / len(trials)

@njit(nogil=True)
def flex_mixed_pfr(trials, presentations):
    "Variant that works for any number of item repetitions"

    list_length = len(presentations[0])
    result = np.zeros(list_length, dtype=np.int32)
    all_study_positions = recall_by_all_study_positions(trials, presentations)
    first_recalls = all_study_positions[:, :, :1]

    for trial_index in range(len(trials)):
        for i in range(list_length):
            result[i] += i+1 in first_recalls[:, trial_index]

    return result/len(trials)

def plot_pfr(data, trial_query, contrast_name=None, labels=None, axis=None):

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, presentations = events_metadata(events)[:3]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        list_length = list_lengths[chose]
        presentations = presentations[chose]
        trial_mask = trial_mask[chose]

        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            if (presentations == np.arange(list_length)).all():
                pfr = fast_pfr(trials[subject_specific_trial_mask], list_length)
            else:
                pfr = flex_mixed_pfr(
                    trials[subject_specific_trial_mask], presentations[subject_specific_trial_mask])

            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "input": np.arange(1, list_length + 1),
                    "recall": pfr,
                    contrast_name: labels[data_index],
                }
            ))

    result = pd.concat(result).reset_index()

    sns.lineplot(
        ax=axis, data=result, x='input', y='recall', err_style='bars', hue=contrast_name)
    axis.set(xlabel='Serial Position', ylabel='Probability of First Recall')
    axis.set_xticks(np.arange(1, list_length+int(list_length/10), int(list_length/10)))
    axis.set_ylim((0, 1))

    if contrast_name:
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

    return axis, result

# Cell

from numba import njit, prange
import numpy as np

@njit(nogil=True, parallel=True)
def fast_rpl(study_positions_in_recall_order, presentations, max_lag=8):

    assert(len(presentations) == len(study_positions_in_recall_order))

    total_presented, total_retrieved = np.zeros(max_lag+2), np.zeros(max_lag+2)

    for trial_index in prange(len(presentations)):
        presented, retrieved = np.zeros(max_lag+2), np.zeros(max_lag+2)
        trial = study_positions_in_recall_order[trial_index]
        trial = trial[trial > 0]-1

        for item in np.unique(presentations[trial_index]):
            for idx, val in np.ndenumerate(presentations[trial_index]):
                if val == item:
                    locationA = idx[0]
                    break

            lag = 0
            if locationA < len(presentations[trial_index]):
                for idx, val in np.ndenumerate(presentations[trial_index][locationA+1:]):
                    if val == item:
                        lag = 1 + idx[0]
                        break

            presented[lag] += 1
            retrieved[lag] += locationA in trial

        total_presented += presented
        total_retrieved += retrieved

    return total_retrieved/total_presented

# Cell

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from .datasets import events_metadata, generate_trial_mask


def plot_rpl(data, trial_query, contrast_name='', labels=None, axis=None):

    lags = ['N/A', '0', '1-2', '3-5', '6-8']

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, presentations = events_metadata(events)[:3]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        presentations = presentations[chose]
        trial_mask = trial_mask[chose]

        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            subject_result = fast_rpl(trials[subject_specific_trial_mask], presentations[subject_specific_trial_mask])
            binned = np.zeros(5)
            binned[0] = subject_result[0]
            binned[1] = subject_result[1]
            binned[2] = (subject_result[2] + subject_result[3])/2
            binned[3] = (subject_result[4] + subject_result[5] + subject_result[6])/3
            binned[4] = (subject_result[7] + subject_result[8] + subject_result[9])/3

            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "lag": lags,
                    "recall": binned,
                    contrast_name: labels[data_index],
                }
            ))

    result = pd.concat(result).reset_index()

    if contrast_name:
        sns.pointplot(ax=axis, data=result, x='lag', y='recall',  join=False, hue=contrast_name)
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    else:
        sns.pointplot(ax=axis, data=result, x='lag', y='recall',  join=False)
    axis.set(xlabel='Study Position', ylabel='Recall Rate')

    return axis

# Cell

import pandas as pd

def rpl(presentations, trials, subjects, trial_count, list_length, max_lag=8):
    #subjects = len(np.unique(events.subject))
    #trial_count = np.max(events.list)
    #list_length = np.max(events.input)
    #lags = ['N/A'] + list(range(max_lag+1))
    lags = ['N/A', '0', '1-2', '3-5', '6-8']

    result = {'subject': [], 'lag': [], 'prob': []}

    for subject in range(subjects):

        subject_result = fast_rpl(
            trials[subject*trial_count:(subject+1)*trial_count], presentations[subject*trial_count:(subject+1)*trial_count], max_lag)

        binned = np.zeros(5)
        binned[0] = subject_result[0]
        binned[1] = subject_result[1]
        binned[2] = (subject_result[2] + subject_result[3])/2
        binned[3] = (subject_result[4] + subject_result[5] + subject_result[6])/3
        binned[4] = (subject_result[7] + subject_result[8] + subject_result[9])/3

        result['subject'] += [subject+1]*len(lags)
        result['lag'] += lags
        result['prob'] += binned.tolist()

    return pd.DataFrame(result)

# Cell
#| code-summary: --specify semantic probability of first recall analysis code


from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy.spatial.distance import squareform
from sentence_transformers import util
from numba import njit

@njit(nogil=True)
def fast_sem_pfr(item_index_by_trial, item_index_by_presentation, distances, edges, buffer=2):

    first_recalls = item_index_by_trial[:, 0]
    first_presentations = item_index_by_presentation[:, :1+buffer]
    last_presentations = item_index_by_presentation[:, -1-buffer:]
    total_actual_distances = np.zeros(len(edges))

    # For each item recalled first in a trial that is outside an initial presentation buffer,
    # compute its similarity to the initially studied item in each trial.
    for i in range(buffer, len(first_recalls)):
        if first_recalls[i] < 0:
            continue
        if np.sum(first_recalls[i] == first_presentations[i]) > 0:
            continue
        if np.sum(first_recalls[i] == last_presentations[i]) > 0:
            continue
        distanceA = distances[first_recalls[i], first_presentations[i, 0]]
        distanceB = distances[first_recalls[i], last_presentations[i, 0]]
        distance_bin = np.sum(edges < (distanceA + distanceB)/2 , dtype=np.int32)
        total_actual_distances[distance_bin] += 1

    return total_actual_distances / np.sum(total_actual_distances)

# Cell
#| code-summary: -- specify semantic contiguity effect plotting code

def plot_sem_pfr(data, trial_query, string_embeddings, contrast_name=None, labels=None, axis=None):

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, _, item_index_by_pres, item_index_by_trial = events_metadata(events)[:6]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        list_length = list_lengths[chose]
        item_index_by_trial = item_index_by_trial[chose]
        item_index_by_pres = item_index_by_pres[chose]
        trial_mask = trial_mask[chose]

        # set edges based on distances between unique item_index_by_trial in current dataset
        if data_index == 0:
            distances = 1-util.pytorch_cos_sim(string_embeddings, string_embeddings).numpy()
            distances[np.eye(len(distances), dtype=int)] = np.nan
            edges = np.nanpercentile(distances, np.linspace(1, 100, 10))

        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            res = fast_sem_pfr(item_index_by_trial, item_index_by_pres, distances, edges)
            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "bin": edges,
                    "prob": res,
                    contrast_name: labels[data_index],
                }
            ))

    result = pd.concat(result).reset_index()
    sns.lineplot(ax=axis, data=result, x='bin', y='prob', err_style='bars', hue=contrast_name, legend=False)
    axis.set(xlabel='Semantic Distance', ylabel='Probability of First Recall')

    if contrast_name:
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

    return axis, result


# Cell
#| code-summary: -- specify serial position effect analysis code

from .datasets import events_metadata, generate_trial_mask
import matplotlib.pyplot as plt
from numba import njit
import seaborn as sns
import pandas as pd
import numpy as np

@njit(nogil=True)
def recall_by_all_study_positions(recall_by_first_study_position, presentations, max_repeats=3):

    trials_shape = np.shape(recall_by_first_study_position)
    result = np.zeros(
            (max_repeats, trials_shape[0], trials_shape[1]), dtype=np.int32)

    for trial_index in range(len(recall_by_first_study_position)):

        trial = recall_by_first_study_position[trial_index]
        presentation = presentations[trial_index]

        for recall_index in range(len(trial)):

            if trial[recall_index] == 0:
                continue

            presentation_positions = np.nonzero(
                presentation[trial[recall_index] - 1] == presentation)[0] + 1

            result[:len(presentation_positions), trial_index, recall_index] = presentation_positions

    return result

@njit(nogil=True)
def fast_spc(trials, item_count):
    return np.bincount(trials.flatten(), minlength=item_count+1)[1:]/len(trials)

@njit(nogil=True)
def flex_mixed_spc(trials, presentations):
    "Variant of fast_spc that works with lists that contain an arbitrary number of item repetitions."

    list_length = len(presentations[0])
    result = np.zeros(list_length, dtype=np.int32)
    all_study_positions = recall_by_all_study_positions(trials, presentations)

    for trial_index in range(len(trials)):
        for study_position in range(list_length):
            result[study_position] += study_position+1 in all_study_positions[:,trial_index]

    return result/len(trials)

def plot_spc(data, trial_query, contrast_name=None, labels=None, axis=None):

    if axis is None:
        plt.figure()
        axis = plt.gca()

    if labels is None:
        labels = [''] * len(data)

    result = []
    for data_index, events in enumerate(data):

        # generate and subset trials array and list of list_lengths based on trial_query
        trials, list_lengths, presentations = events_metadata(events)[:3]
        trial_mask = generate_trial_mask(events, trial_query)
        chose = [i for i in range(len(trial_mask)) if np.sum(trial_mask[i]) != 0]
        assert(len(chose) == 1)
        chose = chose[0]
        trials = trials[chose]
        list_length = list_lengths[chose]
        presentations = presentations[chose]
        trial_mask = trial_mask[chose]

        for subject in pd.unique(events.subject):
            subject_specific_trial_mask = np.logical_and(
                generate_trial_mask(events, f"subject == {subject}")[chose], trial_mask
            )

            if np.sum(subject_specific_trial_mask) == 0:
                continue

            if (presentations == np.arange(list_length)).all():
                spc = fast_spc(trials[subject_specific_trial_mask], list_length)
            else:
                spc = flex_mixed_spc(
                    trials[subject_specific_trial_mask], presentations[subject_specific_trial_mask])
            result.append(pd.DataFrame.from_dict(
                {
                    "subject": subject,
                    "input": np.arange(1, list_length + 1),
                    "recall": spc,
                    contrast_name: labels[data_index],
                }
            ))

    result = pd.concat(result).reset_index()

    # sns.lineplot(
    #     ax=axis, data=result, x='input', y='recall', err_style='bars', hue=contrast_name)
    sns.lineplot(
        ax=axis, data=result, x='input', y='recall', err_style='bars', hue=contrast_name)
    axis.set(xlabel='Serial Position', ylabel='Probability of Recall')
    axis.set_xticks(np.arange(1, list_length+int(list_length/10), int(list_length/10)))
    axis.set_ylim((0, 1))

    if contrast_name:
        axis.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

    return axis, result

# Cell

from numba import njit
import numpy as np

@njit(fastmath=True, nogil=True)
def fast_csp(trials, item_count):

    # numerator is number of trials with zero in current column position
    numerator = np.zeros(item_count+1)

    # denominator is number of trials without zero in previous column positions
    denominator = np.zeros(item_count+1)

    stop_positions = trials == 0
    for i in range(len(trials)):

        # add 1 to index of final recall
        numerator[np.argmax(stop_positions[i])] += 1

        # add 1 to each index up through final recall
        denominator[:np.argmax(stop_positions[i])+1] += 1

    denominator[denominator==0] += 1
    return numerator/denominator

# Cell
import seaborn as sns
from psifr import fr

def plot_csp(data, **facet_kws):

    trials = pd.pivot_table(
        data, index=['subject', 'list'],
        columns=['output'], values='input',
        fill_value=0).to_numpy()

    sns.lineplot(
        data=csp(data, trials),
        x='output', y='prob', **facet_kws)