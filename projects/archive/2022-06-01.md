Hi, so the issue I had to fix concerned how my likelihood function handles low probability events. Python's scientific computing library seems to round off to 0 more readily than I expected, screwing up the parameter search a little. This is more of a problem for ICMR than PCMR because to trigger the bug, you need to take a large exponent of a small number. My previous controls to enforce non-zero event probabilities weren't strong enough, but I've found one that works. 

Bug fix helped a lot and fits are updated but unfortunately some further refactoring is apparently still required before I can show simulated summary statistics. Think I'll get that done within the next couple of hours though. Apologies for the delay.

Some notes I've been holding back...

The issue I had to fix concerned how my likelihood function handles low probability events. Python's scientific computing library seems to round off to 0 more readily than I expected, screwing up the parameter search a little. This is more of a problem for ICMR than PCMR because to trigger the bug, you need to take a large exponent of a small number. My previous controls to enforce non-zero event probabilities weren't strong enough, but I've found one that works. 

I now have fitting results for all the model variants we might mention in our paper. Across these, I'm tracking wAIC scores, paired t-test results, and proportion of times each model outperformed the other. Basically, every evaluation metric applied in either Stanton & Nosofsky (2002) or Morton & Polyn (2016). I can share these results to you; the matching simulation results will be ready soon.

ATM, the variant of ICMR with two stacks of memory (instead of one), trace-based activation scaling (instead of echo), and application of learning rate scaling AFTER trace-based scaling (instead of before) performs best and most consistently across the datasets I've explored. In control list datasets (no item repetitions) such as PEERS and the Murdock 1962 variable list length datasets, ICMR performs about as well or better than PCMR depending on the metric you prefer. But some aspects of my results still give me pause. These include:

1. **Echo-based variant of ICMR isn't comparable with PCMR in the way we expect**. Under the narrative that trace-based activation scaling is what distinguishes ICMR from PCMR, this shouldn't be the case. But our variants of ICMR with the same activation scaling mechanism as PCMR tend to perform worse than PCMR and all the other variants I've been weighing, even in the baseline comparison using PEERS.
2. **PCMR reliably beats best ICMR variant in control and mixed list conditions of Lohnas dataset, but not when fitted to whole dataset**. Our PEERS and Murdock 1962 datasets are perhaps better tests of model performance when lists exclude item repetitions, but for whatever reason PCMR does substantially better than ICMR in both the Lohnas dataset's control lists and its mixed lists including a mix of once- and twice-presented items. When fitting across all 4 conditions of the dataset (other conditions are massed and spaced repetition), ICMR comes out ahead overall, but the story of what's happening in that dataset for the two models is pretty mysterious.
3. **PCMR reliably beats best ICMR variant when fitting to whole HowaKaha05 dataset but not when fitting to any individual condition.** In this dataset, there are 90 study positions and all items are repeated 3 times. Conditions of HowaKaha05 dataset are massed repetition, short spacing (2-6), long spacing (6-20). PCMR does considerably better than alternatives in the long spacing condition, while our best ICMR variant comes out ahead to a smaller degree in the other two conditions.

#1 is the only *real* problem here. The ambiguous repetition effect results can potentially be clarified with follow-up analyses or could just identify an area for further research. And the Lohnas control list result is just not as compelling as the control list results we get for bigger datasets (PEERS, Murdock1962). 

But if echo-based activation scaling in ICMR can't match up with PCMR's performance, then there's a gap either in our theoretical analysis of how the architectures differ, or in my code. I think there's a strong case for the former that I can probably produce a toy example demonstrating. The gist is that in an instance model, two probes with the same similarity to the applicable training instance(s) will retrieve the same representation. But in a linear associator, I think this isn't true? Two probes with the same similarity to a training instance can retrieve different representations depending on which features the probes share with the instance.

This might explain why our echo-activation-scaling ICMR differs from PCMR even in control lists. However, it doesn't explain why our echo-activation-scaling ICMR differs from our trace-activation-scaling ICMR under the same conditions. The potential architectural 

***

Fitting results are mostly reasonable (workable?), with one big exception. My best performing variant of ICMR is comparable to or better than PCMR on the benchmark datasets where there are no item repetitions. Stuff across conditions of the repetition datasets is weird, but I think we can accommodate that with follow-up analyses or hand-waving, in the worst case.

The main issue that troubles the validity of our results is the variant of ICMR that doesn't do trace-based activation scaling, instead using the same activation scaling mechanism as PCMR on echo representations retrieved from memory. This is supposed to essentially be a prototype model implemented within the instance architecture, based on the reasoning that nonlinear scaling of trace activations is what makes instance models do recall-by-exemplar-comparison instead of recall-by-prototype-comparison. But I'm not getting results that reflect that.

If the important difference between ICMR and PCMR is nonlinear trace activation scaling, then e-ICMR should be statistically indistinguishable from PCMR in our model comparisons. But instead, across datasets, it performs worse than both PCMR and our ICMR w/ trace activation scaling.

This is a problem for two reasons. On the one hand, the model comparison across datasets implies that echo-scaling ICMR is still meaningfully different from PCMR even though neither model does trace-based scaling. I've suggested that there might be theoretical reasons this could be the case. The gist is that in an instance model, two probes with the same similarity to the applicable training instance(s) will retrieve the same representation. But in a linear associator, I think the same isn't the case? In a linear associator, I think two probes with the same similarity to a training instance can retrieve different representations depending on which features the probes share with the instance.

But that would only explain the differences between echo-scaling ICMR and PCMR. While we might expect plenty of subtle differences between PCMR and even the most similar possible ICMR implementation because of the memory representations they use, in this comparison the only difference in my code between the models is when an exponent gets applied. But we're also seeing reliable differences between echo-scaling ICMR and trace-scaling ICMR, even in our control list datasets where there are no item repetitions. Theoretically speaking, this shouldn't happen in control lists because when each item only occurs once in the list, there's no applicable difference between recall-by-prototype-comparison and the sort of recall-by-exemplar-comparison enabled by ICMR. And yet, scaling activations by trace instead of echo makes a big difference in our model fits.

So either there's a problem in what my code is doing, or there's something we're still missing on a theoretical level about what trace-based activation scaling does.