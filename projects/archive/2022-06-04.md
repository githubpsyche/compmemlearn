Once again, I obsess over project organization instead of a closed path from where I am to a paper.

But I think this will help clarity.

I want separate repositories so I can avoid distracting collaborators with files unrelated to a project. A specific scenario where this is relevant is when I'm sharing overleaf projects with my boss.

At the same time, I don't want redundant content between repositories. Are submodules the way here, or a distraction?

Probably a distraction. I can't have a projects-wide obsidian vault anyway, right? Though maybe I can set the vault to contain all markdown and compiled PDFs, etc? Yeah, it is perhaps possible to set my vault to contain all results/writings/etc.

What is the cross-project workflow I'm looking for here? 

A single repository I can clone anywhere. Library defined in one place. Reports defined in another, including paper drafts and incremental bullshit.

Ahhh now I remember. By separating repositories and using submodules, I can develop within compmemlearn, but pass Sean specific draft repos. Ok.

So I work on all the projects' dependencies within compmemlearn. Paper drafts are submodules within a reports subdirectory. Obsidian vault is also the reports subdirectory and will thus only contain results and report source files.

I'll keep using notebooks to dynamically generate reports that I can append to draft documents. 

0. Develop plans, writings, and literature reviews in projects subdirectory in compmemlearn.
1. Code analysis functions and workflows ("report templates") in library subdirectory in compmemlearn.
2. Pass audience-focused analysis outcomes and writings to submodules stored in projects subdirectory in compmemlearn
3. Compile selected writings and analysis outcomes in submodules into tex file(s) and pdf(s) using quarto and sync the repository with overleaf

Am I sticking with Jupyter notebooks for my library? For the moment. I don't have a strong enough reason to spend time shifting elsewhere. 

What about sharing project code? I can't just direct readers to compmemlearn, right?

Once project is *done*, I should effectively take notebooks I've been using to generate reports, and for any dependency involving compmemlearn, pull it out and define it directly in the notebook, using nbdev to convert the result into an independent library. But that's only necessary at the final step. And I can do it w/o notebooks if I want.

So I'm putting projects inside compmemlearn so that...
1. I can have one vault for writings across projects
2. It's easier to programmatically or manually transfer files in project directories

What do I achieve next? Do the reorganization. All I have to do is get the repos synced. Delete from local.

But why don't I just get Sean the current results?
I'd say the notebook isn't "clean" enough.
And I still want to address some of my concerns about the results. Particular the Murdock1962 cross-LL results and the zero rounding issue. But also the Lohnas 2016 spacing effect plots. I'm also looking to resolve my ambivalence about model complexity vs model performance. But that's something Sean should do for me. It kind of requires merging results between my single store and dual store model notebooks. Oh and I wonder if adapting Neal's fitting process is really important.

The murdock1962 bug seems largely fixed. The spacing effect in Lohnas2014 bug...maybe isn't a bug. It fits with my intuition that CMR doesn't predict a monotonic spacing effect like Lohnas 2014 asserts.

So that leaves what? Notebook cleanliness. More robust fitting process. Logsumexp trick. Once I clear that up (supposing I bother), I can forward results to Sean. He'll also want to see the "give away first recall" results. Ugh, how tedious. Am I choosing unnecessary work?

It's a small investment if I just try it. What's the easiest way? Add model implementation. Add fitting method. Incremental try out.

And the fitting method? Big test is whether I can confirm zero difference between ICMR 2_0_0 and ICMR_2_0_1. Ugh, I'm not going worry about this any further.

So next is what? Notebook cleanliness. If I get it to where the results can just be injected into a document at will, then markdown files can focus on discussing content. Alright?

I'll then sort of re-generate results, set up the submodules, update my paper based on what I have, send everything to Sean, and then turn to the extension(s): repetition effects, semantic effects, including recognition.