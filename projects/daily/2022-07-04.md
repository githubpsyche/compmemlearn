If narrative recall is more like serial recall, what does that bode for my ambitions?

Nearest next increment is probably the Landscape CMR variant. It has dynamic semantic connections and I want to learn how much meaning my previous clustering results had. If I find that using the model's connections instead of semantic cmr's doesn't improve fits, then this likely implies that the serial recall thesis excludes a lot of opportunities for deeper model innovation w/o adding serial recall specific concepts. I might also examine semantic_cmr's performance when I enforce semantic scale above .1. 

On the other hand, i could focus on adding analyses. In particular I might focus on repetition effects again and culminate all that into novel semantic analyses.

I will probably do both. Researching serial recall analyses is valuable. Maybe a paper about the unique way narrative reading participants interpret free recall task prompts would be valuable. But it's not the paper I'm dreaming of. 

I shouldn't just be in the business of dream papers though, right? Maybe. But it's probably important to plan how I'll build a complete publication record to support my longterm research goals.

I want to figure out what narrative comprehension has to do with memory search, and use that as a jumping off point to build deeper insights into phenomena that overlap between domains, like semantic organization and repetition effects.

Medium term goal is a case for a retrieved context account of reading comprehension. CMR already does a surprisingly good job free recall wise. I need other constraining phenomena, specifically strong tests of online processing.  processing. Once I get a stable approach to that, 

Hmm. I feel like like I'm being a little unfocused. What is the paper I'm aiming for? What is the report I should provide to Sean to ensure he's behind me? I should focus on clean-up for the moment.

What is the short term goal given that semantic cmr doesn't do better than base? Dossn't this imply that semantic connections are worthless? Landscape model's vonnections were able to do a good job of predicting clustering over the base model. But perhaps it was just because the landscape model learned temporal associations. The only way to know for sure is to set up a head-to-head contest. If it wins or helps CMR do better compared to semantic cmr, then dynamic semantic connections provide more than temporal order information. If it doesn't, then I don't have a path forward on narrative recall using my current tools.

I could alternatively focus on finding behavioral artifacts of dynamic semantic connections. 

Let's think about it this way -- what would actually make for a clear argument within a paper? There are so many assumptions embedded in the Landscape model beyond "dynamic semantic connections" -- and "dynamic semantic connections" is so vague a concept anyway -- that I probably wouldn't make the case this way. Instead, the idea is that the landscape model might provide inspiration for further model development. Also, maybe I'll present CMR as a successor to the landscape model -- eventually. 

What's the more compelling result? Base CMR can outdo landscape model? Showing that would get me to "publishable", right? Not necessarily -- particularly with the caveats I have in mind. I'd need theory constraints apart from "free recall" data to add enough that's new. And those theory constraints would look a lot like the analyses I'd otherwise implement. 

So if I focus on the analysis route, what would be the plan? Find ways to reveal in free recall data some basic assumptions of theories of reading comprehension. For example, "dynamic semantic connections driven by backward inference" can be measured using an extension of that Lohnas neighbor contiguity analysis. What else?

Cycles. In the landscape model, units are activated together if they are in the same cycle. 

I think I can safely say that these are the only unique principles in the model. But what about what the landscape model excludes? Activations works a lot like context — decayed support for previous cycles, a cap on overall support, maximal support for the current cycle. Dynamics are a little different — I may be able to implement RCT within those other constraints.

Learning is the same (outer product sums). And experiencing otherwise proceeds the same. Recall proceeds by item cue instead of context cue; that is arguably the main difference. Plus representation content is quite different (basically eye matrices at first).

Okay, so then what? I want to add support for and find a way to test for cycles. Clearest test is probably a test of transition rates between adjacent items in same cycle vs out. Is there a way to fit segments this way? Not convincingly. But I can learn model assumptions this way.

I expect clustering in recall of words by idea unit and of idea units by cycle.
And I expect heightened contiguity between idea units and serial neighbors of highly similar idea units.
And I expect heightened contiguity between serial neighbors of idea units and serial neighbors of highly similar idea units.
...And I expect heightened contiguity between idea units and highly similar idea units themselves.

I have not found the latter. Maybe I should actually expect confusion errors? That would be trouble for the equivalence thesis. But perhaps the connections change substantially in one context but not the other.

So where am I going with this? It's cleaner to test model assumptions, not the model itself. 
These are syntactic clustering and similarity-driven reinstatement. 

Latter doesn't necessarily predict semantic contiguity. Even though I feel like it should. 

Next step remains to document and address similarity-driven context reinstatement in list memory. If I can provr the phenomenon exists, specify a mechanism that accounts for it, and show this improves model performance...that's at least a poster. Carrying these insights to tackle narrative recall would be a next step.

What about the idea that I need to report what I have so far to Sean?
