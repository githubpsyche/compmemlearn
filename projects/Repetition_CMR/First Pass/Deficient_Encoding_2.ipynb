{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": []
   },
   "source": [
    "# <center>An Independent Effect of Deficient Processing for List Memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Background\n",
    "The big idea of the deficient encoding hypothesis as an account of the spacing effect is that:\n",
    "1. Item familiarity (\"close acquaintance\") impairs attention and memory of subsequent encoding.\n",
    "2. Repeated experience of an item increases familiarity with that item\n",
    "3. On the other hand, familiarity decreases with intervening experience, allowing better memory of repetitions\n",
    "    \n",
    "The MINERVA-DE model (Collins et al, 2020) implements a model with these mechanisms to account for various effects in recognition memory. In the model, a short-term memory store called primary memory is maintained with a long-term secondary memory. When a word is attended, its familiarity is computed relative to the items in primary memory. The more familiar a studied item is to existing items in primary memory, the less well it is encoded in secondary memory. This process is called discrepancy encoding, because encoding is strongest for items that are discrepant with information already stored in primary memory.\n",
    "\n",
    "I was interested in motivating a similar mechanism within the framework of retrieved context theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "- Review spacing effect\n",
    "- Review retrieved context theory and work relating it to spacing effect\n",
    "- Review deficient encoding account of effect\n",
    "- State that an account of repetition effects integrating these frameworks has so far gone unexplored.\n",
    "- Draw a diagram outlining what such an account might look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Hypothesis\n",
    "If during a free recall task\n",
    "1. Short-term familiarity mediates memory for item repetitions as outlined above,\n",
    "2. Short-term familarity itself is mediated by a representation of temporal context reflecting \"a recency-weighted average of information related to recently presented stimuli\", and\n",
    "3. Studying an item prompts retrieval and reinstatement of previous contextual associations\n",
    "\n",
    "Then for an item $A$ originally presented at position $i$ during study, repetition of an item $B$ originally presented at position $i+1$ should enhance short-term familiarity with $A$ by prompting retrieval of shared contextual associations. This in turn, should drive an independent spacing effect between positions of the second presentation of $B$ and a later second presentation of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### ...But a Problem\n",
    "This conceptualization effectively ties short-term familarity to contextual variability, making it impossible to tease apart the two accounts. Unfortunately, I tried testing this hypothesis as posed anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'repfr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5348/3934125292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrepfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprepare_repdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m trials, events, list_length, presentations, list_types, rep_data, subjects = prepare_repdata(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'repfr'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from repfr.datasets import prepare_repdata\n",
    "\n",
    "trials, events, list_length, presentations, list_types, rep_data, subjects = prepare_repdata(\n",
    "    '../data/repFR.mat')\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Approach\n",
    "\n",
    "I searched for evidence of this pattern using data from Lohnas (2014), particularly in trials consisting of pure spaced lists with length 40, consisting of items presented twice at lags 1-8, where lag is defined as the number of intervening items between a repeated item's presentations. In the pure spaced lists, spacings of repeated items were chosen so that each of the lags 1-8 occurred with equal probability.\n",
    "\n",
    "I identified all trials in the dataset with $A, B, ..., B, A$ subsequences, with no multiple presentations of a single item between $AB$ and $BA$. I also required at least one intervening item between presentations of $B$.\n",
    "\n",
    "For comparison and to control for possible serial position or spacing effects, I matched identified trials to ones in the dataset where serial position, subject id, and lag between presentations of $A$ were identical, but a lag was present between second presentations of $B$ and of $A$: $A, B, ..., B, ..., A$. Where multiple matching trials could be found for a given $A, B, ..., B, A$, we enforced downstream comparison outcomes to reflect an weighted average of all of them.\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Matching Features\n",
    "We define a new function that finds matching subsequences this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def match_features(target_group, list_type, startPos, subject, lagA):\n",
    "    \n",
    "    result = []\n",
    "    for trial_index, sequence in enumerate(presentations):\n",
    "    \n",
    "        # test for matching list_type\n",
    "        if list_types[trial_index] != list_type:\n",
    "            continue\n",
    "            \n",
    "        # test for matching subject\n",
    "        if subjects[trial_index] != subject:\n",
    "            continue\n",
    "\n",
    "        for item in np.unique(sequence):\n",
    "            list_positions = np.where(sequence == item)[0]\n",
    "\n",
    "            # no use considering items presented just once\n",
    "            if (len(list_positions) == 1):\n",
    "                continue\n",
    "\n",
    "            # test for matching startPos\n",
    "            if list_positions[0] != startPos:\n",
    "                continue\n",
    "\n",
    "            # test for matching lag\n",
    "            lag = list_positions[1] - list_positions[0] - 1\n",
    "            subsequence = sequence[list_positions[0]+1:list_positions[1]]\n",
    "            if lag != lagA:\n",
    "                continue\n",
    "                \n",
    "            # track and avoid sequences where # unique items != lag - 1\n",
    "            multiply_presented_items = lag - len(\n",
    "                np.unique(subsequence))\n",
    "            if multiply_presented_items != 1:\n",
    "                continue\n",
    "            \n",
    "            # if item at i+1 and j-1 are the same, that's target group\n",
    "            if subsequence[0] == subsequence[-1]:\n",
    "                group = 0\n",
    "            else:\n",
    "                group = 1\n",
    "                \n",
    "            if group != target_group:\n",
    "                continue\n",
    "                \n",
    "            if group == 0:\n",
    "                lagB, lagB1, lagB2 = -1, -1, -1\n",
    "            else:\n",
    "                # find positions of multiply repeated item within subsequence\n",
    "                for candidate in subsequence:\n",
    "                    sub_positions = np.where(subsequence == candidate)[0]\n",
    "                    if len(sub_positions) != 1:\n",
    "                        lagB = sub_positions[1] - sub_positions[0] - 1\n",
    "                        lagB1 = sub_positions[0]\n",
    "                        lagB2 = len(subsequence) - sub_positions[1]\n",
    "                        break\n",
    "            \n",
    "            result.append(\n",
    "                [trial_index, list_types[trial_index], subjects[trial_index], \"Lag Between Second B and A\", item, \n",
    "                 item+1 in trials[trial_index], list_positions[0], \n",
    "                 lag, lagB, lagB1, lagB2])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Building a DataFrame\n",
    "For every subsequence in group 0 that we can find matching subsequences for in other groups (should we care if it's in all other groups? Figure that out later), we'll sample from the retrieved subsequences 100 times to build a dataframe with our controls.\n",
    "\n",
    "I wonder if I can type this stuff and get a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "result = []\n",
    "\n",
    "for trial_index, sequence in enumerate(presentations):\n",
    "    \n",
    "    if list_types[trial_index] != 3:\n",
    "        continue\n",
    "    \n",
    "    for item in np.unique(sequence):\n",
    "        list_positions = np.where(sequence == item)[0]\n",
    "\n",
    "        # no use considering items presented just once\n",
    "        if (len(list_positions) == 1):\n",
    "            continue\n",
    "        \n",
    "        # also don't consider sequences with lag below a minimum\n",
    "        lag = list_positions[1] - list_positions[0] - 1\n",
    "        subsequence = sequence[list_positions[0]+1:list_positions[1]]\n",
    "\n",
    "        if lag < 3:\n",
    "            continue\n",
    "            \n",
    "        # track and avoid sequences where # unique items < lag - 1\n",
    "        multiply_presented_items = lag - len(\n",
    "            np.unique(subsequence))\n",
    "        if multiply_presented_items > 1:\n",
    "            continue\n",
    "            \n",
    "        # if item at i+1 and j-1 aren't the same, that's not the target group\n",
    "        if subsequence[0] != subsequence[-1]:\n",
    "            continue\n",
    "        \n",
    "        # we'll never include this group in analyses of B lags\n",
    "        lagB, lagB1, lagB2 = lag-2, 0, 0\n",
    "        \n",
    "        # look for matched trials in alternative group to include in dataframe\n",
    "        matched_trials1 = match_features(\n",
    "            1, 3, list_positions[0], subjects[trial_index], lag)\n",
    "        \n",
    "        # if we can't find matched trials, go next\n",
    "        if len(matched_trials1) == 0:\n",
    "            continue\n",
    "            \n",
    "        # otherwise weight downstream analyses appropriately\n",
    "        for i in range(100):\n",
    "            result.append(\n",
    "                [trial_index, list_types[trial_index], subjects[trial_index], \n",
    "                 \"No Lag Between Second B and A\", item, item+1 in trials[trial_index], list_positions[0], \n",
    "                 lag, lagB, lagB1, lagB2])\n",
    "            \n",
    "        # build a balanced sample from matched groups for comparison\n",
    "        if len(matched_trials1) == 0:\n",
    "            pass\n",
    "        elif len(matched_trials1) == 1:\n",
    "            for i in range(100):\n",
    "                result.append(matched_trials1[0])\n",
    "        else:\n",
    "            for i in range(100):\n",
    "                result.append(random.choice(matched_trials1))\n",
    "        \n",
    "result = pd.DataFrame(\n",
    "    result, columns=['trial', 'list_type', 'subject', 'group', 'item', 'recalled', 'startPos', \n",
    "                     'lagA', 'lagB', 'lagB1', 'lagB2'])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "result[result.group==\"Lag Between Second B and A\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.FacetGrid(result, col=\"lagB2\")\n",
    "g.map_dataframe(sns.histplot, x=\"lagA\", discrete=True, stat=\"probability\")\n",
    "g.set_axis_labels(\"Lag\", \"Proportion in Dataset\")\n",
    "plt.show()\n",
    "\n",
    "alt_group = result[result.group == \"Lag Between Second B and A\"]\n",
    "result.lagB2.corr(result.lagA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Comparison of Recall Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.barplot(data=result, x=\"group\", y=\"recalled\", estimator=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Lag Distributions by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(result, col=\"group\")\n",
    "g.map_dataframe(sns.histplot, x=\"lagA\", discrete=True, stat=\"probability\")\n",
    "g.set_axis_labels(\"Lag Between Presentations of A\", \"Proportion in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Serial Position by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(result, col=\"group\")\n",
    "g.map_dataframe(sns.histplot, x=\"startPos\", discrete=True, stat='probability')\n",
    "g.set_axis_labels(\"startPos\", \"Proportion in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Subject ID by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(result, col=\"group\")\n",
    "g.map_dataframe(sns.histplot, x=\"subject\", discrete=True, stat='probability')\n",
    "g.set_axis_labels(\"Subject\", \"Proportion in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "alt_group = result[result.group == \"Lag Between Second B and A\"]\n",
    "alt_group.lagB2.corr(alt_group.recalled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "The greater the lag between a second presentation of B and a second presentation of A, the greater the probability of recalling A. But there's a positive correlation between this second-presentation lag and the lag between presentations of A. So maybe that's just the regular old spacing effect. We aren't effectively controlling for subsequence length yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## <center>Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Discussion\n",
    "Within CMR, a formal model implementing retrieved context theory, a learning rate scalar modulates how strongly items are encoded into memory. The deficient encoding hypothesis calls for a mechanism to be added to CMR that modulates the value of this scalar based on the familarity of the currently encoded item. A hypothetical CMR-DE would thus track for each encoding index a measure of each item's familiarity based on the current state of context. The learning rate for memories of the current item and its contextual associations would then vary inversely with its familiarity.\n",
    "\n",
    "I attempted to test the deficient encoding hypothesis about repetition effects by focusing on the idea that familiarity might hinge on the contextual dynamics that organize memory according to CMR. Here, the main implication I focus on is the corollary hypothesis that familiarity-based memory impairment for a given item can be increased without necessarily experiencing that item. Instead, contextual states associated with the item from a previous presentation can be reinstated through repetition of items originally presented near said previous presentations that share many of item's contextual associations. \n",
    "\n",
    "I found the opposite of the relationship I was looking for, even after trying to control for relevant variables. Unfortunately, even if I did find the expected relationship, these analyses do not do anything to identify an unique role for the deficient processing account of item spacing that contextual variability or study-phase retrieval accounts can't fill. Worse, I think that as long as one conceptualizes deficient processing as something that depends on how retrievable an item is from context, or even just the serial position of the last time an item was encoded, then **in principle any effect on recall connected to deficient processing is just as explicable in terms of the contextual variability account of repetition effects**. So this may be a dead end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### References\n",
    "Collins, R. N., Milliken, B., & Jamieson, R. K. (2020). MINERVA-DE: An instance model of the deficient processing theory. Journal of Memory and Language, 115, 104151.\n",
    "\n",
    "Siegel, L. L., & Kahana, M. J. (2014). A retrieved context account of spacing and repetition effects in free recall. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(3), 755."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 2,
      "defaultCellHeight": 60,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
