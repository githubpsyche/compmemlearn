{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012eb679",
   "metadata": {},
   "source": [
    "# Parameter Shifting Visualizations\n",
    "> figures demonstrating how the recency and contiguity effects shift as you shift parameters around in the 2 models\n",
    "\n",
    "Presuppose fitted parameters from the MurdockOkada1970 dataset. Vary various parameters around their fitted values and plot the resulting simulation data in SPCs, CRPs, PFRs. \n",
    "\n",
    "I'll try to work out the code for this in a way that makes it easy to pick the varied parameters and value ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a46620",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We'll need each relevant parameter configuration, helper functions, and models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from psifr import fr\n",
    "from InstanceCMR import InstanceCMR\n",
    "from PrototypeCMR import PrototypeCMR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_okadata(path):\n",
    "    \"\"\"\n",
    "    Prepares data formatted like `data/MurdData_clean.mat` for fitting.\n",
    "\n",
    "    Loads data from `path` with same format as `data/MurdData_clean.mat` and \n",
    "    returns a selected dataset as an array of unique recall trials and a \n",
    "    dataframe of unique study and recall events organized according to `psifr`\n",
    "    specifications.  \n",
    "\n",
    "    **Arguments**:  \n",
    "    - path: source of data file  \n",
    "    - dataset_index: index of the dataset to be extracted from the file\n",
    "\n",
    "    **Returns**:\n",
    "    - trials: int64-array where rows identify a unique trial of responses and \n",
    "        columns corresponds to a unique recall index.  \n",
    "    - merged: as a long format table where each row describes one study or \n",
    "        recall event.  \n",
    "    - list_length: length of lists studied in the considered dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path) as f:\n",
    "        oka_data = f.read()\n",
    "\n",
    "    counter = 0\n",
    "    trials = []\n",
    "    subjects = []\n",
    "    list_length = 20\n",
    "\n",
    "    for line in oka_data.split('\\n'):\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # build subjects array\n",
    "        if counter == 0:\n",
    "            subjects.append(int(line.strip().split('    ')[1]))\n",
    "\n",
    "        # build trials array\n",
    "        if counter == 1:\n",
    "\n",
    "            trial = [int(each) for each in line.strip().split('    ')]\n",
    "            trial = [each for each in trial if each <= 20]\n",
    "            already = []\n",
    "            for each in trial:\n",
    "                if each not in already:\n",
    "                    already.append(each)\n",
    "            trial = already\n",
    "            \n",
    "            while len(trial) < 13:\n",
    "                trial.append(0)\n",
    "\n",
    "            trials.append(trial)\n",
    "\n",
    "        # keep track of which row we are on for the given trial\n",
    "        counter += 1\n",
    "        if counter == 3:\n",
    "            counter = 0\n",
    "\n",
    "    trials = np.array(trials).astype('int64')\n",
    "    \n",
    "    data = []\n",
    "    for trial_index, trial in enumerate(trials):\n",
    "\n",
    "        # every time the subject changes, reset list_index\n",
    "        if not data or data[-1][0] != subjects[trial_index]:\n",
    "            list_index = 0\n",
    "        list_index += 1\n",
    "\n",
    "        # add study events\n",
    "        for i in range(list_length):\n",
    "            data += [[subjects[trial_index], \n",
    "                      list_index, 'study', i+1, i+1]]\n",
    "\n",
    "        # add recall events\n",
    "        for recall_index, recall_event in enumerate(trial):\n",
    "            if recall_event != 0:\n",
    "                data += [[subjects[trial_index], list_index, \n",
    "                          'recall', recall_index+1, recall_event]]\n",
    "\n",
    "    data = pd.DataFrame(data, columns=[\n",
    "        'subject', 'list', 'trial_type', 'position', 'item'])\n",
    "    merged = fr.merge_free_recall(data)\n",
    "    return trials, merged, list_length\n",
    "\n",
    "murd_trials, murd_events, murd_length = prepare_okadata('data/mo1970.txt')\n",
    "\n",
    "murd_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e08974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'choice_sensitivity']\n",
    "\n",
    "parameters = {\n",
    "    'item_count':murd_length,\n",
    "    'presentation_count': murd_length,\n",
    "    'sampling_rule': 0\n",
    "}\n",
    "\n",
    "cmr_fit = np.array([ 0.67729029,  0.0789752 ,  0.84475351,  0.32843236,  0.04606376,\n",
    "        0.25014697,  4.09477771, 35.20917629,  0.03838687,  0.29442883,\n",
    "        5.03376164])\n",
    "\n",
    "cmr_params = {**parameters, **{free_parameters[i]:cmr_fit[i] for i in range(len(cmr_fit))}}\n",
    "cmr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd0f50",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'feature_sensitivity']\n",
    "\n",
    "parameters = {\n",
    "    'item_count':murd_length,\n",
    "    'presentation_count': murd_length,\n",
    "    'context_sensitivity': 1,\n",
    "    'choice_sensitivity': 1,\n",
    "}\n",
    "\n",
    "icmr_fit = np.array([7.04157544e-01, 2.22044605e-16, 8.42679777e-01, 6.84111237e-04,\n",
    "       3.31835533e-02, 1.01371142e-02, 4.34918696e+00, 1.43883032e+00,\n",
    "       2.98134948e-02, 3.42612961e-01, 2.39278982e+00])\n",
    "\n",
    "icmr_params = {**parameters, **{free_parameters[i]:icmr_fit[i] for i in range(len(icmr_fit))}}\n",
    "icmr_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25708fd8",
   "metadata": {},
   "source": [
    "## Simulation Demo\n",
    "Let's confirm that I can (efficiently) simulate the model okay and plot an example serial position curve before I try scaling up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f19af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_data(model, experiment_count, first_recall_item=None):\n",
    "    \"\"\"\n",
    "    Initialize a model with specified parameters and experience sequences and \n",
    "    then populate a psifr-formatted dataframe with the outcomes of performing `free recall`. \n",
    "    \n",
    "    **Required model attributes**:\n",
    "    - item_count: specifies number of items encoded into memory\n",
    "    - context: vector representing an internal contextual state\n",
    "    - experience: adding a new trace to the memory model\n",
    "    - free_recall: function that freely recalls a given number of items or until recall stops\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode items\n",
    "    try:\n",
    "        model.experience(np.eye(model.item_count, model.item_count + 1, 1))\n",
    "    except ValueError:\n",
    "        # so we can apply to CMR\n",
    "        model.experience(np.eye(model.item_count, model.item_count))\n",
    "\n",
    "    # simulate retrieval for the specified number of times, tracking results in df\n",
    "    data = []\n",
    "    for experiment in range(experiment_count):\n",
    "        data += [[experiment, 0, 'study', i + 1, i] for i in range(model.item_count)]\n",
    "    for experiment in range(experiment_count):\n",
    "        if first_recall_item is not None:\n",
    "            model.force_recall(first_recall_item)\n",
    "        data += [[experiment, 0, 'recall', i + 1, o] for i, o in enumerate(model.free_recall())]\n",
    "\n",
    "    data = pd.DataFrame(data, columns=['subject', 'list', 'trial_type', 'position', 'item'])\n",
    "    merged = fr.merge_free_recall(data)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "model = PrototypeCMR(**cmr_params)\n",
    "model = InstanceCMR(**icmr_params)\n",
    "events = simulate_data(model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e61a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = events.query('study').pivot_table(\n",
    "    index=['subject', 'input'], values=['recall']).reset_index()\n",
    "spc.reset_index(level=0, inplace=True)\n",
    "spc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0103e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n",
    "g = sns.lineplot(data=spc, x='input', y='recall',  palette='pastel')\n",
    "plt.xlabel('Study Position')\n",
    "plt.ylabel('Probability Recall');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871444f",
   "metadata": {},
   "source": [
    "It's snappy enough that I don't have to try jit-compiling the data simulation function. Now to scale up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a9795",
   "metadata": {},
   "source": [
    "## Approach\n",
    "How am I gonna tackle this? Simulate data for each unique parameter configuration and then extract relevant analysis dataframe. Add a new column identified varied values across simulations so I can hue or facet based on the variable. \n",
    "\n",
    "Which analyses am I interested in doing? Uh, let's start with encoding drift rate and just one parameter varied per simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "minimum = 0\n",
    "maximum = .5\n",
    "interval = .1\n",
    "\n",
    "score_ranges = {\n",
    "    'item_support': np.arange(.01, .1, .02),\n",
    "}\n",
    "\n",
    "for model in [InstanceCMR]:\n",
    "\n",
    "    if model.__name__ == 'PrototypeCMR':\n",
    "        parameters = cmr_params\n",
    "    else:\n",
    "        parameters = icmr_params\n",
    "\n",
    "    for varied_parameter in score_ranges.keys():\n",
    "        crps = []\n",
    "        spcs = []\n",
    "        pfrs = []\n",
    "\n",
    "        for parameter_value in tqdm(score_ranges[varied_parameter]):\n",
    "\n",
    "            # simulate data with this parameter value modified\n",
    "            sub_params = parameters.copy()\n",
    "            sub_params[varied_parameter] = parameter_value\n",
    "            subset = simulate_data(model(**sub_params), 200)\n",
    "\n",
    "            # accumulate spcs, crps, pfrs\n",
    "            spc = subset.query('study').pivot_table(\n",
    "            index=['subject', 'input'], values=['recall'])\n",
    "            spc[varied_parameter] = parameter_value\n",
    "            spcs.append(spc)\n",
    "\n",
    "            crp = fr.lag_crp(subset)\n",
    "            crp[varied_parameter] = parameter_value\n",
    "            crps.append(crp)\n",
    "\n",
    "            pfr = fr.pnr(subset).query('output <= 1')\n",
    "            pfr[varied_parameter] = parameter_value\n",
    "            pfrs.append(pfr)\n",
    "\n",
    "        # concatenate result into a single table\n",
    "        spc = pd.concat(spcs).reset_index()\n",
    "        crp = pd.concat(crps).reset_index()\n",
    "        pfr = pd.concat(pfrs).reset_index()\n",
    "\n",
    "        sns.set(style='darkgrid')\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "        sns.lineplot(ax=axes[0], data=spc, x='input', y='recall', hue=varied_parameter, ci=None)\n",
    "        #axes[0].set_xlabel('Study Position')\n",
    "        #axes[0].set_ylabel('Probability Recall')\n",
    "        axes[0].set_title('SPC')\n",
    "        axes[0].legend([], []);\n",
    "        #plt.show()\n",
    "\n",
    "        max_lag = 10\n",
    "        filt_neg = f'{-max_lag} <= lag < 0'\n",
    "        filt_pos = f'0 < lag <= {max_lag}'\n",
    "\n",
    "        sns.lineplot(ax=axes[1], data=crp.query(filt_neg), x='lag', y='prob', hue=varied_parameter, ci=None)\n",
    "        sns.lineplot(ax=axes[1], data=crp.query(filt_pos), x='lag', y='prob', hue=varied_parameter, ci=None)\n",
    "\n",
    "        #axes[1].set_xlabel('Lag')\n",
    "        #axes[1].set_ylabel('conditional response probability')\n",
    "        axes[1].legend(np.round(score_ranges[varied_parameter], 5));\n",
    "        axes[1].set_title('Lag-CRP')\n",
    "        #plt.show()\n",
    "\n",
    "        sns.lineplot(ax=axes[2], data=pfr, x='input', y='prob', hue=varied_parameter, ci=None)\n",
    "        #axes[2].set_xlabel('Study Position')\n",
    "        #axes[2].set_ylabel('Probability of First Recall')\n",
    "        axes[2].set_title('PFR')\n",
    "        axes[2].legend([], []);\n",
    "\n",
    "        fig.suptitle(varied_parameter.replace('_', ' ').upper())\n",
    "        plt.savefig('results/{}_{}.svg'.format(model.__name__, varied_parameter))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104af6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
