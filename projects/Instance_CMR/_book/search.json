[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Instance-Based Account of Context Maintenance and Retrieval",
    "section": "",
    "text": "In retrieved context accounts of memory search like the Context Maintenance and Retrieval (CMR) model (Polyn, Norman, and Kahana 2009), representations of studied items in a free recall experiment are associated with states of an internal representation of temporal context that slowly evolves across the study period. These mechanisms enable models to account for organizational effects in recall sequences, such as the tendency for related items to be recalled successively. Retrieved context models tend to specify these dynamics in terms of a simplified neural network, as building a single prototypical pattern of associations between each item and context (and vice versa) across experience. By contrast, models of categorization and other memory phenomena have increasingly converged on instance-based architectures (Hintzman 1984) that conceptualize memory as a stack of trace vectors that each represent discrete experiences and support recall through parallel, nonlinear activation based on similarity to a probe. To investigate the consequences of this distinction we present an instance-based specification of CMR that encodes associations between studied items and context by instantiating memory traces corresponding to each experience, and drives recall through context-based coactivation of those traces. We analyze the model’s ability to account for traditional phenomena that have been used as support for the original prototypical specification of CMR, evaluate conditions under which the specifications might behave differently, and explore the model’s capacity for integration with existing instance-based models to elucidate a broader collection of memory phenomena.\n\n\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129."
  },
  {
    "objectID": "00_Introduction.html",
    "href": "00_Introduction.html",
    "title": "2  Instance and Prototype Accounts of Abstraction",
    "section": "",
    "text": "A central task of memory is to relate features of current experience with relevant and useful information from past experience; however, stored information relevant to a probe is often distributed across multiple learning episodes. To account for our ability to retrieve this information, models of memory search often specify some mechanism for abstraction – selective generalization across recurrent features of past experience (Yee 2019). Abstraction involves identifying and highlighting features common across experiential episodes while disregarding or suppressing reinstatement more idiosyncratic properties. Since this capacity is central to how memory systems retrieve relevant information from stored experience, much work has explored how humans carry it out.\nDepending on how they characterize the process of abstraction, memory models can often be categorized as prototype- or instance-based. Prototype-based models conceptualize abstraction as a process enacted during encoding; new experiences are conceived as updating memory representations to reflect prototypical features that are common across past experiences. Connectionist models such as the multilayer perceptron are typically examples of prototype-based models (Jamieson et al. 2018). Instead of being stored as separate records in memory, learning examples presented to a connectionist model each update a prototypical pattern of weights that eventually map memory probes to responses.\nInstance-based models do store learning exampls as separate records in memory. The model architecture was originally identified to help understand how category learning might be possible without explicit storage of so-called abstract ideas (Hintzman 1984, 1986, 1988). Instance-based models posit that memory encoding primarily involves accumulating a record of every experience as separate traces in memory. Abstraction over stored instances later occurs at retrieval rather than during encoding, and unfolds through comparison of a memory cue with each instance stored in memory. The abstractive representation finally retrieved is a blend of the content in each stored instance, weighted such that information in the instances most similar to the probe is substantially more prominent than information in instances that are dissimilar to the probe. Because instance-based models preserve a discrete record of all relevant events in memory, they can often selectively retrieve information about even rare past events with high flexibility. \nInstance-based accounts of memory have faced scrutiny for implying that the number of stored instances in memory can increase without limit and are all contacted upon retrieval, respectively placing extraordinary capacity and processing demands on the human nervous system (e.g., Kahana 2020). However, at the same time as instance-based models have been critiqued for their architectural lack of data compression at storage, the way abstractive representations exclude idiosyncratic features of individual learning episodes to reflect a center of tendency across them is similarly recurrently cited as a limitation of prototype-based models. In research on categorization for example, ‘exemplar-similarity’ models (Nosofsky and Zaki 2002; Stanton, Nosofsky, and Zaki 2002) outperform comparable prototype-based models by representing categories as sets of stored instances paired with a process for comparison against probes. A related analysis extends these findings to also critique prototype-based accounts of semantic memory. Jamieson et al. (2018) found that because prototype-based distributional semantic models such as latent semantic analysis (Dumais 2004) and Word2Vec (Church 2017) “collapse the many contexts in which a word occurs to a single best-fitting representation”, they lose the ability to represent rare senses of homonymous and polymsemous words. Consequently, prototype-based models exhibited measureably worse performance accounting for word similarity patterns in various natural language simulations compared to an instance-based account of semantic memory based on the MINERVA 2 multiple traces memory model (Hintzman 1984). In the context of successes like these across diverse research conditions, instance-based accounts of memory have become increasingly prominent.\n\n2.0.1 Models of Free Recall are Traditionally Prototype-Based\nWhile instance-based models have organized formal work in a variety of research subfields, models of memory search primarily focused on accounting for performance on the free recall task largely countervail this pattern. In the free recall task paradigm, research participants are presented a sequence of items — usually a word list — to memorize during a study phase. Later, after a delay or perhaps some distraction task, participants are prompted to recall as many items from the list as possible, in whatever order they come to mind. Since participants largely organize the course of retrieval themselves in the response phase of a free recall task, work by researchers to characterize the organization of responses measured under the paradigm (Postman 1971; Puff 1979) have provided important constraints on accounts of the representations and mechanisms underlying search through memory to retrieve information.\nIn particular, three consistent regularities across experiments have received especial emphasis in accounts of performance on the free recall task (Kahana 2020). The serial position effect identifies a nonlinear, U-shaped relationship between the position of an item within a study list — its serial position — and its probability of retrieval after encoding (Murdock Jr 1962). Researchers typically distinguish between the enhanced retrieval probabilities for early and terminal items; the advantage for the initially presented items is called the primacy effect, while the normally larger advantage for the few presented items is called the recency effect.\nA similar but distinct pattern constraining accounts of memory search is found in analyses relating an item’s serial position with the probability that it will be recalled first in the retrieval phase of experiments. Pivotally, in list-based free recall tasks, participants tend to initiate recall with the most recently studied items from the list; however, in a serial recall task where participants are instructed to recall items in the order in which they were presented rather than freely, participants tend to successfully recall the earliest presented items first (for example in Golomb et al. 2008). This difference implies that while participants maintain and can access memories of item positions to perform a serial recall task, memory search and retrieval is organized by other features of experience.\nPrimacy and recency effects demonstrate that the temporal structure of the list affects the memorability of the items within it. This temporal structure can also be seen in the organization of responses throughout the response sequence, not just for initial and terminal items or recall positions. Free recall task data exhibits a pattern called temporal contiguity where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment. To quantify this pattern, researchers measure across trials the conditional probability of retrieving items given increasing inter-item lags between the serial positions of considered items and the serial position of the item last recalled. These lag-based condition response probability (lag-CRP) analyses find that subjects reliably tend to make transitions between temporally contiguous items (that is, items presented near one another) during free recall. Furthermore, they exhibit a forward bias, recalling contiguous items presented after the last recalled item more frequently than items presented before (Kahana 1996).\nTo account for these phenomena, the formal literature has largely converged on retrieved context theories of memory search (for example, Howard and Kahana 2002; Polyn, Norman, and Kahana 2009; Morton and Polyn 2016). Generally, according to these theories, as items are encoded into a memory system, an internal representational of temporal context is also maintained that dynamically updates itself to reflect a weighted summary of recent experience. As each item is studied, a Hebbian learning mechanism associates the item’s features to the current state of the context representation. Once associated, item features can cue retrieval of associated contextual features, and vice versa. When the retrieval phase comes, the current contextual representation can drive memory search by activating a blend of associated item features. This prompts a retrieval competition in which a particular item is selected and retrieved. Correspondingly, retrieving an item reactivates its associated contextual features, updating context before the next recall attempt. The retrieved context supports the neighbors of the just-recalled item, which gives rise to temporal organization.\nWith these basic mechanisms, retrieved-context models have been used to explain many phenomena, including serial and temporal organizational effects in list-learning tasks (Polyn, Norman, and Kahana 2009; Siegel and Kahana 2014; Schwartz et al. 2005), and broader domains such as financial decision making (Wachter and Kahana 2019), emotion regulation (Talmi, Lohnas, and Daw 2019), and neural signal dynamics within the medial temporal lobe (Kragel, Morton, and Polyn 2015). Further model development has integrated retrieved context accounts of memory search with theories of semantic knowledge (Morton and Polyn 2016) and changes related to healthy aging (Healey and Kahana 2016).\nThe framework used to implement most retrieved context models of memory search acts like a prototype model. These models typically encode memories associating contextual states and item features by updating connection weights within a simplified neural network. Through Hebbian learning, where co-activation of item and contextual features increase weights associating those features, the network accumulates a collapsed average representation reflecting the history of context and item interactions across experience. During retrieval, the network can be probed with a contextual cue to retrieve an item feature representation (or vice versa) based on a linear function of the cue’s content and stored context-to-item weights.\nIn contrast, an instance-based alternative would track this history by storing a discrete record of each experience with its unique temporal context in memory to perform abstraction over only at the point of retrieval. Previous instance-based accounts of performance on various tasks have emphasized a role of some sort of temporal contextual representation in organizing performance. Indeed, the original presentation of MINERVA 2, the first major instance-based memory modeling architecture, included a representation of list context as a feature in stored memory instances to model source-specific frequency judgments from memory (Hintzman 1984). (Lehman and Malmberg 2013) proposed an instance-based buffer model that accounts for patterns like recency and the position position effect in terms of storage and retrieval of traces containing information about item and contextual co-occurrences. Most recently, Logan (2021) introduced the Context Retrieval and Updating (CRU) model, which extends retrieved context theories’ conceptualization of context as a recency-weighted history of previously presented items to account for performance on whole report, serial recall, and copy typing tasks. Nonetheless, it remains unclear whether differences reported in related memory literatures between the performance of prototype- and instance-based memory models might similarly distinguish models of memory search.\n\n\n2.0.2 Research Approach\nIn this paper, I show that the mechanisms proposed by the influential Context Maintanence and Retrieval (CMR) model of memory search (Morton and Polyn 2016) can be realized within either a prototypical or instance-based model architecture without substantially impacting performance across various experimental conditions. This instance-based CMR (InstanceCMR) extends the established MINERVA 2 multiple traces model (Hintzman 1984, 1986, 1988) to support context-based memory search and simulate performance on the free recall task. I fit InstanceCMR and its original prototype-based counterpart (prototypeCMR) to the sequences of individual responses made by participants in three distinct free recall task datasets.I find that the models account for retrieval performance with similar effectiveness despite architectural differences, including over data manipulating the lengths of study lists between trials and other data manipulating the number of times particular items are studied within trials.\nAnalyses of the two specifications for CMR suggest that these outcomes can be largely explained by the model’s assumption that feature representations corresponding to studied items in free recall experiments are orthogonal — activation of each unit on an item feature layer corresponds to one item. This ensures that context-to-feature associations built via experience of one item do not overlap with associations built through experience of some other distinct item. Correspondingly, the influence of relevant experiences on the content of abstractive representations retrieved via these associations can be selectively enhanced while simultaneously suppressing the influence of less relevant experiences, without any interference. This capacity to nonlinearly modulate the influence of selected learning episodes on recall based on the content of a probe approximates trace-based activation functions realized within instance-based models, sidestepping issues reported about prototype-based memory models in other literatures.\n\n\n\n\nChurch, Kenneth Ward. 2017. “Word2Vec.” Natural Language Engineering 23 (1): 155–62.\n\n\nDumais, Susan T. 2004. “Latent Semantic Analysis.” Annual Review of Information Science and Technology 38 (1): 188–230.\n\n\nGolomb, Julie D, Jonathan E Peelle, Kelly M Addis, Michael J Kahana, and Arthur Wingfield. 2008. “Effects of Adult Aging on Utilization of Temporal and Semantic Associations During Free and Serial Recall.” Memory & Cognition 36 (5): 947–56.\n\n\nHealey, M Karl, and Michael J Kahana. 2016. “A Four-Component Model of Age-Related Memory Change.” Psychological Review 123 (1): 23.\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\n———. 1986. “\"Schema Abstraction\" in a Multiple-Trace Memory Model.” Psychological Review 93 (4): 411.\n\n\n———. 1988. “Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model.” Psychological Review 95 (4): 528.\n\n\nHoward, Marc W, and Michael J Kahana. 2002. “A Distributed Representation of Temporal Context.” Journal of Mathematical Psychology 46 (3): 269–99.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nKahana, Michael J. 1996. “Associative Retrieval Processes in Free Recall.” Memory & Cognition 24 (1): 103–9.\n\n\n———. 2020. “Computational Models of Memory Search.” Annual Review of Psychology 71: 107–38.\n\n\nKragel, James E, Neal W Morton, and Sean M Polyn. 2015. “Neural Activity in the Medial Temporal Lobe Reveals the Fidelity of Mental Time Travel.” Journal of Neuroscience 35 (7): 2914–26.\n\n\nLehman, Melissa, and Kenneth J Malmberg. 2013. “A Buffer Model of Memory Encoding and Temporal Correlations in Retrieval.” Psychological Review 120 (1): 155.\n\n\nLogan, Gordon D. 2021. “Serial Order in Perception, Memory, and Action.” Psychological Review 128 (1): 1.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40.\n\n\nMurdock Jr, Bennet B. 1962. “The Serial Position Effect of Free Recall.” Journal of Experimental Psychology 64 (5): 482.\n\n\nNosofsky, Robert M, and Safa R Zaki. 2002. “Exemplar and Prototype Models Revisited: Response Strategies, Selective Attention, and Stimulus Generalization.” Journal of Experimental Psychology: Learning, Memory, and Cognition 28 (5): 924.\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129.\n\n\nPostman, Leo. 1971. “Organization and Interference.” Psychological Review 78 (4): 290.\n\n\nPuff, C Richard. 1979. Memory Organization and Structure. Academic Press.\n\n\nSchwartz, Greg, Marc W Howard, Bing Jing, and Michael J Kahana. 2005. “Shadows of the Past: Temporal Retrieval Effects in Recognition Memory.” Psychological Science 16 (11): 898–904.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nStanton, Roger D, Robert M Nosofsky, and Safa R Zaki. 2002. “Comparisons Between Exemplar Similarity and Mixed Prototype Models Using a Linearly Separable Category Structure.” Memory & Cognition 30 (6): 934–44.\n\n\nTalmi, Deborah, Lynn J Lohnas, and Nathaniel D Daw. 2019. “A Retrieved Context Model of the Emotional Modulation of Memory.” Psychological Review 126 (4): 455.\n\n\nWachter, Jessica A, and Michael Jacob Kahana. 2019. “A Retrieved-Context Theory of Financial Decisions.” National Bureau of Economic Research.\n\n\nYee, Eiling. 2019. “Abstraction and Concepts: When, How, Where, What and Why?” Taylor & Francis."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "3  References",
    "section": "",
    "text": "Church, Kenneth Ward. 2017. “Word2Vec.” Natural Language Engineering 23 (1): 155–62.\n\n\nDumais, Susan T. 2004. “Latent Semantic Analysis.” Annual Review of Information Science and Technology 38 (1): 188–230.\n\n\nGolomb, Julie D, Jonathan E Peelle, Kelly M Addis, Michael J Kahana, and Arthur Wingfield. 2008. “Effects of Adult Aging on Utilization of Temporal and Semantic Associations During Free and Serial Recall.” Memory & Cognition 36 (5): 947–56.\n\n\nHealey, M Karl, and Michael J Kahana. 2016. “A Four-Component Model of Age-Related Memory Change.” Psychological Review 123 (1): 23.\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\n———. 1986. “\"Schema Abstraction\" in a Multiple-Trace Memory Model.” Psychological Review 93 (4): 411.\n\n\n———. 1988. “Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model.” Psychological Review 95 (4): 528.\n\n\nHoward, Marc W, and Michael J Kahana. 2002. “A Distributed Representation of Temporal Context.” Journal of Mathematical Psychology 46 (3): 269–99.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nKahana, Michael J. 1996. “Associative Retrieval Processes in Free Recall.” Memory & Cognition 24 (1): 103–9.\n\n\n———. 2020. “Computational Models of Memory Search.” Annual Review of Psychology 71: 107–38.\n\n\nKragel, James E, Neal W Morton, and Sean M Polyn. 2015. “Neural Activity in the Medial Temporal Lobe Reveals the Fidelity of Mental Time Travel.” Journal of Neuroscience 35 (7): 2914–26.\n\n\nLehman, Melissa, and Kenneth J Malmberg. 2013. “A Buffer Model of Memory Encoding and Temporal Correlations in Retrieval.” Psychological Review 120 (1): 155.\n\n\nLogan, Gordon D. 2021. “Serial Order in Perception, Memory, and Action.” Psychological Review 128 (1): 1.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40.\n\n\nMurdock Jr, Bennet B. 1962. “The Serial Position Effect of Free Recall.” Journal of Experimental Psychology 64 (5): 482.\n\n\nNosofsky, Robert M, and Safa R Zaki. 2002. “Exemplar and Prototype Models Revisited: Response Strategies, Selective Attention, and Stimulus Generalization.” Journal of Experimental Psychology: Learning, Memory, and Cognition 28 (5): 924.\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129.\n\n\nPostman, Leo. 1971. “Organization and Interference.” Psychological Review 78 (4): 290.\n\n\nPuff, C Richard. 1979. Memory Organization and Structure. Academic Press.\n\n\nSchwartz, Greg, Marc W Howard, Bing Jing, and Michael J Kahana. 2005. “Shadows of the Past: Temporal Retrieval Effects in Recognition Memory.” Psychological Science 16 (11): 898–904.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nStanton, Roger D, Robert M Nosofsky, and Safa R Zaki. 2002. “Comparisons Between Exemplar Similarity and Mixed Prototype Models Using a Linearly Separable Category Structure.” Memory & Cognition 30 (6): 934–44.\n\n\nTalmi, Deborah, Lynn J Lohnas, and Nathaniel D Daw. 2019. “A Retrieved Context Model of the Emotional Modulation of Memory.” Psychological Review 126 (4): 455.\n\n\nWachter, Jessica A, and Michael Jacob Kahana. 2019. “A Retrieved-Context Theory of Financial Decisions.” National Bureau of Economic Research.\n\n\nYee, Eiling. 2019. “Abstraction and Concepts: When, How, Where, What and Why?” Taylor & Francis."
  },
  {
    "objectID": "01_Classic_CMR.html",
    "href": "01_Classic_CMR.html",
    "title": "3  The Prototype-Based Account of Context Maintenance and Retrieval",
    "section": "",
    "text": "Code\nfrom compmemlearn.models import Classic_CMR\n\nRetrieved context theories explain memory search in terms of interactions between between two representations across experience: one of temporal context (a context layer, \\(C\\)) and another of features of studied items (an item layer, \\(F\\)). While this paper introduces an instance-based account of these interactions, we here specify a variant of the original prototype-based context maintenance and retrieval (CMR) model (Polyn, Norman, and Kahana 2009) to support comparison against this account. The instance-based model we emphasize tracks the history of interactions between context and item features by storing a discrete record of each experience in memory for later inspection. In contrast, PrototypeCMR maintains a simplified neural network whose connection weights accumulate a center of tendency representation reflecting context and item interactions across experience.\n\nParameters and structures specifying CMR\n\n\n\n\n\n\n\n\nStructure Type\nSymbol\nName\nDescription\n\n\n\n\nArchitecture\n\n\n\n\n\n\n\\(C\\)\ntemporal context\nA recency-weighted average of encoded items\n\n\n\n\\(F\\)\nitem features\nCurrent pattern of item feature unit activations\n\n\n\n\\(M^{FC}\\)\n\nencoded feature-to-context associations\n\n\n\n\\(M^{CF}\\)\n\nencoded context-to-feature associations\n\n\nContext Updating\n\n\n\n\n\n\n\\({\\beta}_{enc}\\)\nencoding drift rate\nRate of context drift during item encoding\n\n\n\n\\({\\beta}_{start}\\)\nstart drift rate\nAmount of start-list context retrieved at start of recall\n\n\n\n\\({\\beta}_{rec}\\)\nrecall drift rate\nRate of context drift during recall\n\n\nAssociative Structure\n\n\n\n\n\n\n\\({\\alpha}\\)\nshared support\nAmount of support items initially have for one another\n\n\n\n\\({\\delta}\\)\nitem support\nInitial pre-experimental contextual self-associations\n\n\n\n\\({\\gamma}\\)\nlearning rate\nAmount of experimental context retrieved by a recalled item\n\n\n\n\\({\\phi}_{s}\\)\nprimacy scale\nScaling of primacy gradient on trace activations\n\n\n\n\\({\\phi}_{d}\\)\nprimacy decay\nRate of decay of primacy gradient\n\n\nRetrieval Dynamics\n\n\n\n\n\n\n\\({\\tau}\\)\nchoice sensitivity\nExponential weighting of similarity-driven activation\n\n\n\n\\({\\theta}_{s}\\)\nstop probability scale\nScaling of the stop probability over output position\n\n\n\n\\({\\theta}_{r}\\)\nstop probability growth\nRate of increase in stop probability over output position\n\n\n\n\n3.0.1 Initial State\nAssociative connections built within prototypeCMR are represented by matrices \\(M^{FC}\\) and \\(M^{CF}\\).\nTo summarize pre-experimental associations built between relevant item features and possible contextual states, we initialize \\(M^{FC}\\) according to:\n\\[\nM^{FC}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n          0 \\text{, if } i \\neq j\n   \\end{alignedat} \\end{cases}\n \\qquad(3.1)\\]\nThis connects each unit on \\(F\\) to a unique unit on \\(C\\). Used this way, \\(\\gamma\\) controls the relative contribution of pre-experimentally acquired associations to the course of retrieval compared to experimentally acquired associations. Correspondingly, context-to-feature associations tracked by \\(M^{CF}\\) are set according to:\n\\[\nM^{CF}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\delta \\text{, if } i=j \\\\\\\n          \\alpha \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(3.2)\\]\nLike \\(\\gamma\\) with respect to \\(M^{FC}\\), the \\(\\delta\\) parameter controls the contribution of pre-experimental context-to-feature associations relative to experimentally acquired ones. Since context-to-feature associations organizes the competition of items for retrieval, the \\(\\alpha\\) parameter specifies a uniform baseline extent to which items support one another in that competition.\nContext is initialized with a state orthogonal to any of those pre-experimentally associated with an relevant item feature. Feature representations corresponding to items are also assumed to be orthonormal with respect to one another such that each unit on \\(F\\) corresponds to one item.\n\n\n3.0.2 Encoding Phase\nWhenever an item \\(i\\) is presented for study, its corresponding feature representation \\(f_i\\) is activated on \\(F\\) and its contextual associations encoded into \\(M^{FC}\\) are retrieved, altering the current state of context \\(C\\).\nThe input to context is determined by:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.3)\\]\nand normalized to have length 1. Context is updated based on this input according to:\n\\[ \nc_i = \\rho_ic_{i-1} + \\beta_{enc} c_{i}^{IN}\n \\qquad(3.4)\\]\nwith \\(\\beta\\) (for encoding we use \\(\\beta_{enc}\\)) shaping the rate of contextual drift with each new experience, and \\(\\rho\\) enforces the length of \\(c_i\\) to 1 according to:\n\\[ \n\\rho_i = \\sqrt{1 + \\beta^2\\left[\\left(c_{i-1} \\cdot c^{IN}_i\\right)^2 - 1\\right]} - \\beta\\left(c_{i-1} \\cdot\nc^{IN}_i\\right)\n \\qquad(3.5)\\]\nAssociations between each \\(c_i\\) and \\(f_i\\) are built through Hebbian learning:\n\\[\n\\Delta M^{FC}_{exp} = \\gamma c_i f^{'}_i\n \\qquad(3.6)\\]\nand\n\\[\n\\Delta M^{CF}_{exp} = \\phi_i f_i c^{'}_i\n \\qquad(3.7)\\]\nwhere \\(\\phi_i\\) enforces a primacy effect, scales the amount of learning based on the serial position of the studied item according to\n\\[ \n\\phi_i = \\phi_se^{-\\phi_d(i-1)} + 1\n \\qquad(3.8)\\]\nThis function decays over time, such that \\(\\phi_{s}\\) modulates the strength of primacy while \\(\\phi_{d}\\) modulates the rate of decay.\nThis extended Hebbian learning process characterizes how PrototypeCMR performs abstraction. When each item is encoded with a particular temporal context, representations are updated to aggregate a prototypical summary of the item’s temporal contextual associations in \\(M^{FC}\\) and vice versa in \\(M^{CF}\\).\n\n\n3.0.3 Retrieval Phase\nTo help the model account for the primacy effect, we assume that between the encoding and retrieval phase of a task, the content of \\(C\\) has drifted some amoung back toward its pre-experimental state and set the state of context at the start of retrieval according to following, with \\(\\rho\\) calculated as specified above:\n\\[ \nc_{start} = \\rho_{N+1}c_N + \\beta_{start}c_0\n \\qquad(3.9)\\]\nAt each recall attempt, the current state of context is used as a cue to attempt retrieval of some studied item. An activation \\(a\\) is solicited for each item according to:\n\\[ \na = M^{CF}c\n \\qquad(3.10)\\]\nEach item gets a minimum activation of \\(10^{-7}\\). To determine the probability of a given recall event, we first calculate the probability of stopping recall - returning no item and ending memory search. This probability varies as a function of output position \\(j\\):\n\\[\nP(stop, j) = \\theta_se^{j\\theta_r}\n \\qquad(3.11)\\]\nIn this way, \\(\\theta_s\\) and \\(\\theta_r\\) control the scaling and rate of increase of this exponential function. Given that recall is not stopped, the probability \\(P(i)\\) of recalling a given item depends mainly on its activation strength according\n\\[\nP(i) = (1-P(stop))\\frac{a^{\\tau}_i}{\\sum_{k}^{N}a^{\\tau}_k}\n \\qquad(3.12)\\]\n\\(\\tau\\) here shapes the contrast between well-supported and poorly supported items: exponentiating a large activation and a small activation by a large value of \\(\\tau\\) widens the difference between those activations, making recall of the most activated item even more likely. Small values of \\(\\tau\\) can alternatively driven recall likelihoods of differentially activated items toward one another.\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.13)\\]\nContext is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\n\n\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129."
  },
  {
    "objectID": "02_Instance_CMR.html",
    "href": "02_Instance_CMR.html",
    "title": "4  Context Maintenance and Retrieval within an Instance-Based Architecture",
    "section": "",
    "text": "from compmemlearn.models import Instance_CMR\nPrototype-based implementations of the retrieved context account of memory search generally suppose that learned item and contextual associations are encoded into abstractive prototype representations according to a Hebbian learning process and then retrieved based on activation from a cue. The memory architecture investigated in this paper alternatively supposes that learning episodes are stored as discrete instances in memory and only abstracted over at retrieval. Within previous examples of this architecture (e.g., Hintzman 1984; Jamieson et al. 2018), stored instances are represented as vectors stacked within a \\(m\\) by \\(n\\) memory matrix \\(M\\). In model variations where vectors are not composed of binary values, at retrieval each trace is activated in parallel based on a positively accelerated transformation of its cosine similarity to a probe \\(p\\):\n\\[\na(p)_i = \\left({\\frac {\\sum^{j=n}_{j=1}{p_j \\times M_{ij}}} {\\sqrt{\\sum^{j=n}_{j=1}{p^2_j}}\n        \\sqrt{\\sum^{j=n}_{j=1}{M^2_{ij}}}}}\\right)^{\\tau}\n \\qquad(4.1)\\]\nWithin this architecture, the parameter \\(\\tau\\) exponentially scales this acceleration, effectively controlling the selectivity of retrieval by modulating the difference in activations between highly and less relevant traces. A sum of stored traces weighted by these nonlinearly scaled activations – called an echo, \\(E(p)\\), is taken to build an abstractive representation for retrieval:\n\\[\nE(p) = \\sum^{i=m}_{i=1}\\sum^{j=n}_{j=1}a(p)_i \\times M_{ij}\n \\qquad(4.2)\\]\nOur instance-based implementation of the context maintenance and retrieval model (InstanceCMR) realizes the retrieved context account of memory search (as articulated by Morton and Polyn 2016) by extending this instance-based architecture to capture how retrieved context theory avers that item and temporal contextual associations evolve and organize retrieval. To make comparison of architectures as straightforward as possible, mechanisms were deliberately specified to be as similar to those of the original prototypical specification as possible except where required by the constraints of the instance-based architecture.\n\nParameters and structures specifying InstanceCMR\n\n\n\n\n\n\n\n\nStructure Type\nSymbol\nName\nDescription\n\n\n\n\nArchitecture\n\n\n\n\n\n\n\\(M\\)\nmemory\nArray of accumulated memory traces\n\n\n\n\\(C\\)\ntemporal context\nA recency-weighted average of encoded items\n\n\n\n\\(F\\)\nitem features\nCurrent pattern of item feature unit activations\n\n\nContext Updating\n\n\n\n\n\n\n\\({\\beta}_{enc}\\)\nencoding drift rate\nRate of context drift during item encoding\n\n\n\n\\({\\beta}_{start}\\)\nstart drift rate\nAmount of start-list context retrieved at start of recall\n\n\n\n\\({\\beta}_{rec}\\)\nrecall drift rate\nRate of context drift during recall\n\n\nAssociative Structure\n\n\n\n\n\n\n\\({\\alpha}\\)\nshared support\nAmount of support items initially have for one another\n\n\n\n\\({\\delta}\\)\nitem support\nInitial pre-experimental contextual self-associations\n\n\n\n\\({\\gamma}\\)\nlearning rate\nAmount of experimental context retrieved by a recalled item\n\n\n\n\\({\\phi}_{s}\\)\nprimacy scale\nScaling of primacy gradient on trace activations\n\n\n\n\\({\\phi}_{d}\\)\nprimacy decay\nRate of decay of primacy gradient\n\n\nRetrieval Dynamics\n\n\n\n\n\n\n\\({\\tau}\\)\nchoice sensitivity\nExponential weighting of similarity-driven activation\n\n\n\n\\({\\theta}_{s}\\)\nstop probability scale\nScaling of the stop probability over output position\n\n\n\n\\({\\theta}_{r}\\)\nstop probability growth\nRate of increase in stop probability over output position\n\n\n\n\n4.0.1 Model Architecture\nPrototypical CMR stores associations between item feature representations (represented a pattern of weights in an item layer \\(F\\)) and temporal context (represented in a contextual layer \\(C\\)) by integrating prototypical mappings between the representations via Hebbian learning over the course of encoding. In contrast, InstanceCMR tracks the history of interactions between context and item features by storing a discrete record of each experience, even repeated ones, as separate traces within in a memory store for later inspection. Memory for each experience is encoded as a separate row in an \\(m\\) by \\(n\\) memory matrix \\(M\\) where rows correspond to memory traces and columns correspond to features. Each trace representing a pairing \\(i\\) of a presented item’s features \\(f_i\\) and the temporal context of its presentation \\(c_i\\) is encoded as a concatenated vector:\n\\[\nM_i = (f_i, c_i)\n \\qquad(4.3)\\]\n\n\n4.0.2 Initial State\nStructuring \\(M\\) as a stack of concatenated item and contextual feature vectors \\((f_i, c_i)\\) makes it possible to define pre-experimental associations between items and contextual states similarly to the pattern by which PrototypeCMR’s pre-experimental associations are specified in equations 3.1 and 3.2. To set pre-experimental associations, a trace is encoded into memory \\(M\\) for each relevant item. Each entry \\(j\\) for each item feature component of pre-experimental memory traces trace \\(f_{pre}(i)\\) is set according to\n\\[\nf_{pre(i, j)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n          0 \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(4.4)\\]\nThis has the effect of relating each unit on \\(F\\) to a unique unit on \\(C\\) during retrieval. As within prototypical CMR, the \\(\\gamma\\) parameter controls the strength of these pre-experimental associations relative to experimental associations.\nSimilarly to control pre-experimental context-to-item associations, the content of each entry \\(j\\) for the contextual component of each pre-experimental trace \\(c_{pre(i,j)}\\) is set by:\n\\[\nc_{pre(i,j)} = \\begin{cases} \\begin{alignedat}{2} \\delta \\text{, if } i=j \\\\\\\n          \\alpha \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(4.5)\\]\nHere, \\(\\delta\\) works similarly to \\(\\gamma\\) to connect indices on \\(C\\) to the corresponding index on \\(F\\) during retrieval from a partial or mixed cue. The \\(\\alpha\\) parameter additionally allows all the items to support one another in the recall competition in a uniform manner.\nBefore list-learning, context \\(C\\) is initialized with a state orthogonal to the pre-experimental context associated with the set of items via the extra index that the representation vector has relative to items’ feature vectors. Following the convention established for prototypical specifications of CMR, item features are further assumed to be orthonormal with respect to one another such that each unique unit on \\(F\\) corresponds to one item.\n\n\n4.0.3 Encoding Phase\nIn a broad sense, the initial steps of item encoding within InstanceCMR proceed similarly to the process in PrototypeCMR. Just as with PrototypeCMR, when an item \\(i\\) is presented during the study period, its corresponding feature representation \\(f_i\\) is activated on \\(F\\) and its contextual associations encoded into \\(M^{FC}\\) are retrieved by presenting \\(f_i\\) as a probe to memory. InstanceCMR, however, performs retrieval by applying an extension of the basic two-step echo \\(E\\) mechanism outlined in equations 4.1 and 4.2.\nThe extension of the original mechanism differentiates between item- and context-based retrieval. When probes include item feature information (\\(p_f \\neq 0\\)), activation for traces encoded during the experiment are modulated by \\(\\gamma\\) to control the contribution of experimentally-accumulated associations to retrieved representations relative to pre-experimental associations:\n\\[\n(, c^{IN}) = E(f_i, 0) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\gamma} \\times a(f_i, 0)_j \\times M_{jk}\n \\qquad(4.6)\\]\nThe contextual features of the retrieved echo determine contextual input; this retrieved pre-experimental context is normalized to have length 1. Upon retrieval of \\(c^{IN}\\), the current state of context is updated the same way as it is under the prototype-based framework, applying equations 3.4 and 3.5 to drift \\(c\\) toward \\(c^{IN}\\) and enforce its length to 1, respectively.\nAfter context is updated, the current item \\(f_i\\) and the current state of context \\(c_i\\) become associated in memory \\(M\\) by storing a concatenation of the two vectors as a new trace \\((f_i, c_i)\\). This mechanism reserves abstraction over learning episodes for cue-based retrieval rather than at the point of encoding as in PrototypeCMR.\n\n\n4.0.4 Retrieval Phase\nFollowing the lead of the classic prototype-based implementation of CMR, before retrieval InstanceCMR reinstates some pre-list context according to 3.9. Similarly, at each recall attempt \\(i\\), we calculate the probability of stopping recall (where no item is recalled and search is terminated) based on output position according to 3.11.\nTo determine the probability of recalling an item given that recall does not terminate, first the current state of context is applied as a retrieval cue to retrieve an item feature presentation \\(f_{rec}\\), again applying a modification of the echo-based retrieval mechanism characteristic of instance-based models that modulates trace activations before aggregation into an echo representation:\n\\[\n(f_{rec},) = E(0, c_i) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\phi}_j \\times a(0, c_i)_j \\times M_{jk}\n \\qquad(4.7)\\]\nwhere \\({\\phi}_i\\) scales the amount of learning, simulating increased attention to initial items in a list that has been proposed to explain the primacy effect. \\({\\phi}_i\\) depends on the serial position \\(i\\) of the studied item the same as it does in PrototypeCMR (equation 3.8), with the free parameters \\({\\phi}_s\\) and \\({\\phi}_d\\) respectively controlling the magnitude and decay of the corresponding learning-rate gradient.\nSince item feature representations are presumed to be orthogonal for the purposes of the model, the content of \\(f_{rec}\\) can be interpreted as a measure of the relative support in memory for retrieval of each item \\(i\\), setting the probability distribution of item recalls \\(P(i)\\) to\n\\[\nP(i) = (1-P(stop))\\frac{f_{rec}}{\\sum_{k}^{N}f_{rec}}\n \\qquad(4.8)\\]\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to Eq. 4.6. Context is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\nAn important difference between equation 4.8 and that applied in our specification of PrototypeCMR to compute \\(P(i)\\) (equation 3.12) is that \\(\\tau\\) is not applied as an exponent to retrieval supports to shape the contrast between well-supported and poorly supported items. Instead, instance-based models apply this transformation to discrete trace activations before aggregation of an echo representation. This difference still achieves the effect of ultimately either widening or shrinking the difference between item supports driving retrieval, but is not trivial. Its consequences are explored in later sections.\n\n\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40."
  },
  {
    "objectID": "03_Methods.html",
    "href": "03_Methods.html",
    "title": "5  Analysis Approach",
    "section": "",
    "text": "Our simulation analyses were designed to determine whether instance-based and prototype-based instantiations of CMR can similarly account for behavioral performance in the free recall task. This includes key benchmark phenomena such as the temporal contiguity and serial position effects, as well as for the overall sequence of responses generated by participants. We used a likelihood-based model comparison technique introduced by Kragel, Morton, and Polyn (2015) that assesses model variants based on how accurately they can predict the specific sequence in which items were recalled. For each model, we related this technique with an optimization technique called differential evolution (Storn and Price 1997) to search for the parameter configuration that maximize the likelihood of the considered data. Likelihoods assigned to datasets by models and their respective optimized parameters in turn support comparison of their effectiveness accounting for the recall sequences exhibited by participants in the data. Visualization of datasets compared to those of simulation outputs given these models with these parameters similarly help compare how well models realize temporal contiguity and serial position effects.\n\n5.0.1 Likelihood-based model comparison\nTo evaluate how effectively each model accounts for the responses in our datasets, we applied a likelihood-based model comparison technique introduced by Kragel, Morton, and Polyn (2015) that assesses model variants based on how accurately they can predict the specific sequence in which items are recalled. According to this method, repeated items and intrusions (responses naming items not presented in the list) are included from participants’ recall sequences. Given an arbitrary parameter configuration and a sequences of recalls to predict, a model simulates encoding of each item presented in the corresponding study list in its respective order. Then, beginning with the first item the participant recalled in the trial, the probability assigned by the model to the recall event is recorded. Next, the model simulates retrieval of that item, and given its updated state is used to similarly predict the next event in the recall sequence - either retrieval of another item, or termination of recall - and so on until retrieval terminates. The probability that the model assigns to each event in the recall sequence conditional on previous trial events are thus all recorded. These recorded probabilities are then log-transformed and summed to obtain the log-likelihood of the entire sequence. Across an entire dataset containing multiple trials, sequence log-likelihoods can be summed to obtain a log-likelihood of the entire dataset given the model and its parameters. Higher log-likelihoods assigned to datasets by a model correspond to better effectiveness accounting for those datasets.\n\n\n5.0.2 Parameter Optimization\nTo find the parameter configuration for each model that maximizes its predicted likelihood of observed data, we applied the optimization technique called differential evolution (Storn and Price 1997) as implemented in the Python library scipy. Differential evolution maintains a population of possible parameter configurations; at each update, the algorithm mutates each population member by stochastically mixing them with other members of the population. If the new configuration of a member is an improvement over its previous configuration, then it becomes part of the updated population. Otherwise, the new parameter configuration is discarded. Through repetition of this process, gradually driving the population toward configurations that maximize the log-likelihood of the observed data assigned by the considered model. This maximal log-likelihood and its corresponding parameter configurations form the basis of comparison between models.\nWhen exploring how effectively the model accounts for qualitative benchmark phenomena in free recall performance such as the temporal contiguity and serial position effects, we optimized parameter configurations and evaluated performance across all subjects in the considered dataset, except where otherwise noted. For direct comparison of the log-likelihoods of recall sequences, however, we search for optimal parameters and perform comparison at the subject level, considering distributions of log-likelihood values calculated between subjects when contrasting model versions.\n\n\n5.0.3 Summary Statistics\nIn each comparison, we use and visualize a set of summary statistics to characterize the the recall performance of both participants and of each considered model version. To make calculation of these summary statistics with respect to a model possible, we first had to simulate recall sequences using each model. We simulated 1000 unique trials for each unique study list in a considered dataset. For each trial, we simulated encoding of each list item into memory. Next, we simulated free recall according to model specifications outlined above, proceeding stochastically in ecah trial based on the probability distribution computed for each recall attempt until termination. Summary statistics characterizing a model were computed across all relevant simulations.\nOur main analyses focus on the three consistent regularities across experiments reviewed above that have received especial emphasis in accounts of performance on the free recall task. To examine the extent to which datasets and model versions realize the serial position effect, we measured and visualized for each study (serial) position in study lists the rate at which items were retrieved across recall sequences. Relative retrieval rates for early-presented items reflect the magnitude of any primacy effect, while those for items in more terminal study positions measure the recency effect. To measure items retrieved at the initiation of recall across trials, we similarly measured and visualized for each serial position in study lists the rate at which items were retrieved first across each recall sequence.\nWe were similarly interested in the extent to which temporal contiguity where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment – was exhibited across recall sequences in our considered datasets and models. To quantify this pattern, we followed the tradition of applying lag-based condition response probability (lag-CRP) analyses. Here, “lag” refers to the number of positions between two item presentations in a study list. Lag-CRP analyses measure the probability of making a recall transition of a particular positive or negative lag, conditional on transition to recall at that lag being possible. Under high temporal contiguity, recall transitions are largely to items with low lag from the last retrieved item and more rarely to items with high lag. Examining conditional response probabilities as a function of lag thus helps characterize the temporal organization of recall across trials.\n\n\n\n\nKragel, James E, Neal W Morton, and Sean M Polyn. 2015. “Neural Activity in the Medial Temporal Lobe Reveals the Fidelity of Mental Time Travel.” Journal of Neuroscience 35 (7): 2914–26.\n\n\nStorn, Rainer, and Kenneth Price. 1997. “Differential Evolution–a Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.” Journal of Global Optimization 11 (4): 341–59."
  },
  {
    "objectID": "04_Baseline_Comparison.html",
    "href": "04_Baseline_Comparison.html",
    "title": "6  Simulation of Murdock and Okada (1970)",
    "section": "",
    "text": "We start by comparing how our prototype- and instance-based implementations of CMR account for behavior in a classic experiment where each item is presented just once per study phase. For these simulations, we used the dataset reported by Murdock and Okada (1970). Each of 72 undergraduates performed 20 trials with study lists each consisting of 20 unique words visually presented at either 60 or 120 words per minute. Given a particular subject, words were unique both within and across trials, and randomly selected from the Toronto Word Pool (Friendly et al. 1982), a widely-used collection of high frequency nouns, adjectives, and verbs.\nWhile the major focus of the original report by Murdock and Okada (1970) was to investigate inter-response times in single-trial free recall, here we focus consideration on the content of recorded recall sequences. Because it excludes within-list repetitions of studied items, this dataset presents the opportunity to compare model performance under simplified conditions. Since items’ feature representations are assumed orthogonal under considered variants of CMR, retrieving a pattern of contextual associations given an item-based cue only requires abstraction over the cued item’s pre-experimental and single experimental contextual associations. Interpretation of apparent differences in performance across model variants thus focus primarily on mechanisms for context-based item representation retrieval.\n\ncode – load dependencies and data\nfrom compmemlearn.fitting import murdock_objective_function, apply_and_concatenate\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom compmemlearn.datasets import prepare_murdock1970_data, simulate_data\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nfrom numba.core import types\nfrom numba import njit\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmurd_trials0, murd_events0, murd_length0 = prepare_murdock1970_data('../../data/mo1970.txt')\n\nWe compared the original prototype-based implementation of CMR against our novel instance-based implementation. First we evaluated each model variant based on their ability to predict the specific sequences of recalls exhibited by each participant. Considering all 20 trials performed by each participant in the dataset, we applied the differential evolution optimization technique to find for each model the parameter configuration that maximized the likelihood of recorded recall sequences. We obtained a unique optimal parameter configuration for each unique participant and each considered model variant. To measure the goodness-of-fit for each parameter configuration and corresponding model, Figure 6.1 plots the log-likelihood of each participant’s recall sequences given each model variant’s corresponding optimized parameter configuration. The distribution of log-likelihood scores between participants for the PrototypeCMR and InstanceCMR model variants only marginally differ, suggesting little meaningful difference between variants in their effectiveness accounting for participant recall performance across the dataset.\n\n\ncode – 1) fit PrototypeCMR participant-by-participant\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nsubject_trial_count = 20 # Each subject gets 20 trials/lists a piece.\ncmr_results = []\n\nfor subject, start_index in enumerate(range(0, len(murd_trials0), subject_trial_count)):\n    print(subject, start_index)\n\n    # cost function to be minimized\n    # ours scales inversely with the probability that the data could have been \n    # generated using the specified parameters and our model\n    cost_function = murdock_objective_function(\n        List([murd_trials0[start_index:start_index+subject_trial_count]]), \n        List([murd_length0]),\n        init_cmr,\n        {'sampling_rule': 0, 'mfc_familiarity_scale': 0, \n         'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n        cmr_free_parameters)\n\n    cmr_results.append(differential_evolution(cost_function, cmr_bounds, disp=False))\n    print(cmr_results[-1].fun)\n\n\n\n\ncode – 2) fit InstanceCMR participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n)\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nsubject_trial_count = 20 # Each subject gets 20 trials/lists a piece.\nicmr_results = []\n\nfor subject, start_index in enumerate(range(0, len(murd_trials0), subject_trial_count)):\n    print(subject, start_index)\n\n    # cost function to be minimized\n    # ours scales inversely with the probability that the data could have been \n    # generated using the specified parameters and our model\n    cost_function = murdock_objective_function(\n    List([murd_trials0[start_index:start_index+subject_trial_count]]),  \n    List([murd_length0]),\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\n    icmr_results.append(differential_evolution(cost_function, icmr_bounds, disp=False))\n    print(icmr_results[-1].fun)\n\n\n\n\ncode – 3) plot distribution of log-likelihoods across individual subjects\nplt.style.use('default')\n\nindividual_fits = [result.fun for result in cmr_results] + [result.fun for result in icmr_results]\nlabels = ['PrototypeCMR'] * len(cmr_results) + ['InstanceCMR'] * len(icmr_results)\nindividual_df = pd.DataFrame(individual_fits, index=labels, columns=['Fit']).reset_index()\nindividual_df.columns = ['Model', 'Fit']\n\nsns.set(style=\"darkgrid\")\n\ng = sns.catplot(x='Model', y='Fit', data=individual_df, kind='violin', inner='stick')\nsns.swarmplot(x=\"Model\", y=\"Fit\", color=\"k\", size=3, data=individual_df, ax=g.ax)\ng.ax.set_ylabel('Individual-Level Fitted Model Log-Likelihoods');\nplt.savefig('individual_murdock1970.pdf', bbox_inches=\"tight\")\n\nsummary_table = pd.DataFrame(group.describe().rename(columns={'Fit':name}).squeeze()\n            for name, group in individual_df.groupby('Model')).T.to_markdown()\n\n\n\n\n\n\n\nCode\nprint(summary_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstanceCMR\nPrototypeCMR\n\n\n\n\ncount\n72\n72\n\n\nmean\n297.128\n296.249\n\n\nstd\n52.7989\n52.8859\n\n\nmin\n156.753\n150.988\n\n\n25%\n258.206\n260.956\n\n\n50%\n299.741\n299.763\n\n\n75%\n331.797\n331.946\n\n\nmax\n387.742\n390.806\n\n\n\n\n\nFigure 6.1: Distribution of log-likelihood scores of recall sequences exhibited by each subject under each considered model across list-lengths (Murdock and Okada 1970)\n\n\nAs a follow-up, we also compared how readily each model could account for organizational summary statistics in the dataset. We found for each model variant the optimal parameter configuration maximizing the likelihood of the entire dataset rather than participant-by-participant. Using each fitted model variant, we simulated 1000 unique free recall trials and measured summary statistics from the result. Figure 6.2 plots for each model against the corresponding statistics collected over the dataset how recall probability varies as a function of serial position, how the probability of recalling an item first varies as a function of serial position, and how the conditional recall probabability of an item varies as a function of its serial lag from the previously recalled item. Recapitulating our comparison of log-likelihood distributions fitted over discrete participants, we found that both our prototype-based and instance-based CMR implementations account for these benchmark organizational summary statistics across the full dataset to similar extents. To build on this finding of broad model equivalence with respect to the results reported by Murdock and Okada (1970), we consider the model variants under broader experimental conditions.\n\n\ncode – 1) fit CMR to entire dataset rather than participant-by-participant\ncost_function = murdock_objective_function(\n    List([murd_trials0]),  \n    List([murd_length0]),\n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, \n     'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\ncmr_result = differential_evolution(cost_function, cmr_bounds, disp=True)\n\n\n\n\ncode – 2) fit Instance_CMR to entire dataset rather than participant-by-participant\ncost_function = murdock_objective_function(\n    List([murd_trials0]),  \n    List([murd_length0]),\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\n\n\n\ncode – 3) compose simulated spc, lag-crp, pfr from overall fitting results\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(cmr_result.x)):\n    fitted_parameters[cmr_free_parameters[i]] = cmr_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nmodel = Classic_CMR(murd_length0, murd_length0, fitted_parameters)\n\nsim_df = simulate_data(model, 1000)\ntrue_df = murd_events0.copy()\n\ncmr_spc = apply_and_concatenate(fr.spc, sim_df, true_df, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp = apply_and_concatenate(fr.lag_crp, sim_df, true_df, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr = apply_and_concatenate(fr.pnr, sim_df, true_df, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr = cmr_pfr.query('output <= 1')\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(icmr_result.x)):\n    fitted_parameters[icmr_free_parameters[i]] = icmr_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['choice_sensitivity'] = 1\nfitted_parameters['feature_sensitivity'] = 1\n\nmodel = Instance_CMR(murd_length0, murd_length0, fitted_parameters)\n\nsim_df = simulate_data(model, 1000)\nicmr_spc = apply_and_concatenate(fr.spc, sim_df, true_df, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_lag_crp = apply_and_concatenate(fr.lag_crp, sim_df, true_df, 'source', ['InstanceCMR', 'data'])\nicmr_pfr = apply_and_concatenate(fr.pnr, sim_df, true_df, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_pfr = icmr_pfr.query('output <= 1')\n\n\n\ncode – 4) plot each summary statistic corresponding to each configured model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='darkgrid')\n\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 15/2), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0, 0], data=icmr_spc, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 0].set_xticks(np.arange(1, 21, 2))\naxes[0, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 0], data=cmr_spc, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[1, 0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[1, 0].set_xticks(np.arange(1, 21, 2))\naxes[1, 0].set_ylim((0, 1))\n\n# lag crp curve\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[0, 1], data=icmr_lag_crp.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[0, 1], data=icmr_lag_crp.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[0, 1].set(xlabel='Item\\'s Lag In Study List From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[0, 1].set_xticks(np.arange(-5, 6, 1))\naxes[0, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 1].set(xlabel='Item\\'s Lag In Study List From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 1].set_xticks(np.arange(-5, 6, 1))\naxes[1, 1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=icmr_pfr, x='input', y='prob', err_style='bars', ax=axes[0, 2], hue='source')\naxes[0, 2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[0, 2].set_xticks(np.arange(1, 21, 2))\naxes[0, 2].set_ylim((0, 1))\n\nsns.lineplot(data=cmr_pfr, x='input', y='prob', err_style='bars', ax=axes[1, 2], hue='source')\naxes[1, 2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[1, 2].set_xticks(np.arange(1, 21, 2))\naxes[1, 2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.tight_layout(pad=3)\nplt.savefig('overall_murdock1970.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\nFigure 6.2: Comparison of summary statistics between each model against observed data (Murdock and Okada 1970)\n\n\n\n\n\n\nFriendly, Michael, Patricia E Franklin, David Hoffman, and David C Rubin. 1982. “The Toronto Word Pool: Norms for Imagery, Concreteness, Orthographic Variables, and Grammatical Usage for 1,080 Words.” Behavior Research Methods & Instrumentation 14 (4): 375–99.\n\n\nMurdock, Bennet B, and Ronald Okada. 1970. “Interresponse Times in Single-Trial Free Recall.” Journal of Experimental Psychology 86 (2): 263."
  },
  {
    "objectID": "05_Variable_List_Lengths.html",
    "href": "05_Variable_List_Lengths.html",
    "title": "7  Simulation of Murdock Jr (1962)",
    "section": "",
    "text": "A significant feature of the context maintenance and retrieval (CMR) model is its capacity to account for the relative scale-invariance of serial position effects with respect to list length. Murdock Jr (1962) found that changing list lengths across trials in a free recall experiment impacted neither the shape of observed primacy effects during recall nor on the slope of apparent recency effects, though other features of recall sequences did change, such as the overall retrieval probability for initially encoded items as well as the list-list asymptote. Building on these observations, Polyn, Norman, and Kahana (2009) found that the CMR model could account for these effects of list length on the shape of the serial position curve in free recall using a single parameter configuration.\nHere we investigate whether our prototype- and instance-based implementations of CMR can similarly account for recall performance across different list lengths when fitted to predict the sequences of items recalled in our considered dataset. For these comparisons, we leverage a subset of the original behavioral data reported by Murdock Jr (1962). In the considered subset, 15 subjects performed 240 trials with study lists each consisting of either 20, 30, or 40 unique words presented sequentially – 80 trials per list length.\n\ncode – load dependencies and data\nfrom compmemlearn.fitting import murdock_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_murdock1962_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport matplotlib.pyplot as plt\nfrom numba.core import types\nfrom numba import njit\nimport seaborn as sns\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\n\nmurd_trials0, murd_events0, murd_length0 = prepare_murdock1962_data(\n    '../../data/MurdData_clean.mat', 0)\nmurd_trials1, murd_events1, murd_length1 = prepare_murdock1962_data(\n    '../../data/MurdData_clean.mat', 1)\nmurd_trials2, murd_events2, murd_length2 = prepare_murdock1962_data(\n    '../../data/MurdData_clean.mat', 2)\n\nFor each model variant and each participant, we found through differential evolution optimization the parameter configuration maximizing the likelihood assigned by the model to each recall sequence in all relevant trials, whether with list length of 20 or 30 or 40. The log-likelihoods of the data corresponding to each participant and model variant are plotted in Figure 7.1, with a table providing summary statistics. The distribution of log-likelihood scores between participants for the PrototypeCMR and InstanceCMR model variants only marginally differ, suggesting little meaningful difference between variants in their effectiveness predicting recall sequences, even when using a single parameter configuration per participant to account for performance across variable list lengths.\n\n\ncode – 1) fit PrototypeCMR participant-by-participant\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nsubject_trial_count = 80 # Each subject gets 80 trials/lists a piece.\nindividual_cmr_results = []\n\nfor subject, start_index in enumerate(range(0, len(murd_trials0), subject_trial_count)):\n    print(subject, start_index)\n\n    # cost function to be minimized\n    # ours scales inversely with the probability that the data could have been \n    # generated using the specified parameters and our model\n    cost_function = murdock_objective_function(\n        List([murd_trials0[start_index:start_index+subject_trial_count], \n              murd_trials1[start_index:start_index+subject_trial_count], \n              murd_trials2[start_index:start_index+subject_trial_count]]), \n        List([murd_length0, murd_length1, murd_length2]),\n        init_cmr,\n        {'sampling_rule': 0, 'mfc_familiarity_scale': 0, \n         'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n        cmr_free_parameters)\n\n    individual_cmr_results.append(differential_evolution(cost_function, cmr_bounds, disp=False))\n    print(individual_cmr_results[-1].fun)\n\n\n\n\ncode – 2) fit InstanceCMR participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nsubject_trial_count = 80 # Each subject gets 20 trials/lists a piece.\nindividual_icmr_results = []\n\nfor subject, start_index in enumerate(range(0, len(murd_trials0), subject_trial_count)):\n    print(subject, start_index)\n\n    # cost function to be minimized\n    # ours scales inversely with the probability that the data could have been \n    # generated using the specified parameters and our model\n    cost_function = murdock_objective_function(\n    List([murd_trials0[start_index:start_index+subject_trial_count], \n        murd_trials1[start_index:start_index+subject_trial_count], \n        murd_trials2[start_index:start_index+subject_trial_count]]),  \n    List([murd_length0, murd_length1, murd_length2]),\n    init_icmr,\n    {'choice_sensitivity':1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\n    individual_icmr_results.append(differential_evolution(\n        cost_function, icmr_bounds, disp=False))\n    print(individual_icmr_results[-1].fun)\n\n\n\n\ncode – 3) plot distribution of log-likelihoods across individual subjects and render summary statistics as table\nplt.style.use('default')\n\nindividual_fits = [result.fun for result in individual_icmr_results] + [result.fun for result in individual_cmr_results]\nlabels = ['InstanceCMR'] * len(individual_icmr_results) + ['PrototypeCMR'] * len(individual_cmr_results)\nindividual_df = pd.DataFrame(individual_fits, index=labels, columns=['Fit']).reset_index()\nindividual_df.columns = ['Model', 'Fit']\n\nsns.set(style=\"darkgrid\")\n\ng = sns.catplot(x='Model', y='Fit', data=individual_df, kind='violin', inner='stick')\nsns.swarmplot(x=\"Model\", y=\"Fit\", color=\"k\", size=3, data=individual_df, ax=g.ax)\ng.ax.set_ylabel('Individual-Level Fitted Model Log-Likelihoods');\nplt.savefig('individual_murdock1962.pdf', bbox_inches=\"tight\")\nplt.show()\n\nsummary_table = pd.DataFrame(group.describe().rename(columns={'Fit':name}).squeeze()\n            for name, group in individual_df.groupby('Model')).T.to_markdown()\n\n\n\n\n\n\nCode\nfrom IPython.display import display, Markdown\n\ndisplay(Markdown(\"\"\"\n::: {{#fig-murd62fits layout-ncol=2 layout-valign=\"center\"}}\n\n![](individual_murdock1962.pdf)\n\n{}\n\nDistribution of log-likelihood scores of recall sequences exhibited by each subject under each considered model across list-lengths [@murdock1962serial]\n:::\n\"\"\".format(summary_table)))\n\n\n\n\n\n\n\n\n\n\n\n\nInstanceCMR\nPrototypeCMR\n\n\n\n\ncount\n15\n15\n\n\nmean\n5295.34\n5280.07\n\n\nstd\n547.19\n553.484\n\n\nmin\n4464.46\n4387.61\n\n\n25%\n4863.55\n4865.08\n\n\n50%\n5285.28\n5280.25\n\n\n75%\n5619.48\n5592.88\n\n\nmax\n6358.67\n6352.11\n\n\n\n\n\nFigure 7.1: Distribution of log-likelihood scores of recall sequences exhibited by each subject under each considered model across list-lengths (Murdock Jr 1962)\n\n\nConsidering log-likelihoods alone though leaves ambiguous whether the influence of list length on serial position and related organizational effects are effectively accounted for by both models. To find out, we again focused scrutiny on the prototype-based and instance-based implementations of CMR. We fit each model based on the likelihood assigned to all recall sequences across the dataset rather than by subject or list length. Summary statistics including recall probability as a function of serial position, probability of first recall as a function of serial position, and conditional recall probability as a function of serial lag from the previously recalled item were computed based on simulation of free recall data using the model variants with their fitted parameters. Separate analyses simulated trials with study list lengths of 20 and of 30 items, with summary statistics tracked separately. Figure 7.2 plots the results of these simulations against statistics from corresponding subsets of the behavioral data from (Murdock Jr 1962), with unique sets of plots for both model variants and list lengths. As with previous analyses, we found that both our prototype-based and instance-based CMR implementations account for these benchmark organizational summary statistics across the considered data to similar extents.\n\n\ncode – 1) fit CMR to entire dataset rather than participant-by-participant\ncost_function = murdock_objective_function(\n    List([murd_trials0, murd_trials1, murd_trials2]),  \n    List([murd_length0, murd_length1, murd_length2]),\n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, \n     'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\ncmr_result = differential_evolution(cost_function, cmr_bounds, disp=True)\n\n\n\n\ncode – 2) fit Instance_CMR to entire dataset rather than participant-by-participant\ncost_function = murdock_objective_function(\n    List([murd_trials0, murd_trials1, murd_trials2]),  \n    List([murd_length0, murd_length1, murd_length2]),\n    init_icmr,\n    {'feature_sensitivity': 1, 'choice_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\nprint(icmr_result)\n\n\n\ncode – 3) compose simulated spc, lag-crp, pfr from overall fitting results\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(cmr_result.x)):\n    fitted_parameters[cmr_free_parameters[i]] = cmr_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nmodel0 = Classic_CMR(murd_length0, murd_length0, fitted_parameters)\nmodel1 = Classic_CMR(murd_length1, murd_length1, fitted_parameters)\nmodel2 = Classic_CMR(murd_length2, murd_length2, fitted_parameters)\n\nsim_df0 = simulate_data(model0, 1000)\nsim_df1 = simulate_data(model1, 1000)\nsim_df2 = simulate_data(model2, 1000)\ntrue_df0 = murd_events0.copy()\ntrue_df1 = murd_events1.copy()\ntrue_df2 = murd_events2.copy()\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\ncmr_spc1 = apply_and_concatenate(fr.spc, sim_df1, true_df1, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp1 = apply_and_concatenate(fr.lag_crp, sim_df1, true_df1, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr1 = apply_and_concatenate(fr.pnr, sim_df1, true_df1, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr1 = cmr_pfr1.query('output <= 1')\n\ncmr_spc2 = apply_and_concatenate(fr.spc, sim_df2, true_df2, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp2 = apply_and_concatenate(fr.lag_crp, sim_df2, true_df2, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr2 = apply_and_concatenate(fr.pnr, sim_df2, true_df2, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr2 = cmr_pfr2.query('output <= 1')\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(icmr_result.x)):\n    fitted_parameters[icmr_free_parameters[i]] = icmr_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['choice_sensitivity'] = 1\nfitted_parameters['feature_sensitivity'] = 1\n\nmodel0 = Instance_CMR(murd_length0, murd_length0, fitted_parameters)\nmodel1 = Instance_CMR(murd_length1, murd_length1, fitted_parameters)\nmodel2 = Instance_CMR(murd_length2, murd_length2, fitted_parameters)\n\nsim_df0 = simulate_data(model0, 1000)\nsim_df1 = simulate_data(model1, 1000)\nsim_df2 = simulate_data(model2, 1000)\ntrue_df0 = murd_events0.copy()\ntrue_df1 = murd_events1.copy()\ntrue_df2 = murd_events2.copy()\n\nicmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['InstanceCMR', 'data'])\nicmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_pfr0 = icmr_pfr0.query('output <= 1')\n\nicmr_spc1 = apply_and_concatenate(fr.spc, sim_df1, true_df1, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_lag_crp1 = apply_and_concatenate(fr.lag_crp, sim_df1, true_df1, 'source', ['InstanceCMR', 'data'])\nicmr_pfr1 = apply_and_concatenate(fr.pnr, sim_df1, true_df1, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_pfr1 = icmr_pfr1.query('output <= 1')\n\nicmr_spc2 = apply_and_concatenate(fr.spc, sim_df2, true_df2, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_lag_crp2 = apply_and_concatenate(fr.lag_crp, sim_df2, true_df2, 'source', ['InstanceCMR', 'data'])\nicmr_pfr2 = apply_and_concatenate(fr.pnr, sim_df2, true_df2, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_pfr2 = icmr_pfr2.query('output <= 1')\n\n\n\ncode – 4) plot each summary statistic corresponding to InstanceCMR and then PrototypeCMR\nsns.set(style='darkgrid')\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12/1.5), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0, 0], data=icmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[0, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 1], data=icmr_spc1, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 1].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[0, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 2], data=icmr_spc2, x='input', y='recall', err_style='bars', hue='source', legend=True)\naxes[0, 2].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[0, 2].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1, 0], data=icmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 0], data=icmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 0].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 0].set_xticks(np.arange(-5, 6, 1))\naxes[1, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 1], data=icmr_lag_crp1.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 1], data=icmr_lag_crp1.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 1].set(xlabel=\"Lag From Last Recalled Item\", ylabel='Conditional Recall Rate')\naxes[1, 1].set_xticks(np.arange(-5, 6, 1))\naxes[1, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 2], data=icmr_lag_crp2.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 2], data=icmr_lag_crp2.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 2].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 2].set_xticks(np.arange(-5, 6, 1))\naxes[1, 2].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=icmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2, 0], hue='source', legend=False)\naxes[2, 0].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[2, 0].set_ylim((0, 1))\n\nsns.lineplot(data=icmr_pfr1, x='input', y='prob', err_style='bars', ax=axes[2, 1], hue='source', legend=False)\naxes[2, 1].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[2, 1].set_ylim((0, 1))\n\nsns.lineplot(data=icmr_pfr2, x='input', y='prob', err_style='bars', ax=axes[2, 2], hue='source', legend=False)\naxes[2, 2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[2, 2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\nplt.savefig('icmr_summary_murdock1962.pdf', bbox_inches=\"tight\")\nplt.show()\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12/1.5), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0, 0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[0, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 1], data=cmr_spc1, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 1].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[0, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 2], data=cmr_spc2, x='input', y='recall', err_style='bars', hue='source', legend=True)\naxes[0, 2].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[0, 2].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1, 0], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 0], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 0].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 0].set_xticks(np.arange(-5, 6, 1))\naxes[1, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp1.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp1.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 1].set(xlabel=\"Lag From Last Recalled Item\", ylabel='Conditional Recall Rate')\naxes[1, 1].set_xticks(np.arange(-5, 6, 1))\naxes[1, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 2], data=cmr_lag_crp2.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 2], data=cmr_lag_crp2.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 2].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 2].set_xticks(np.arange(-5, 6, 1))\naxes[1, 2].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2, 0], hue='source', legend=False)\naxes[2, 0].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[2, 0].set_ylim((0, 1))\n\nsns.lineplot(data=cmr_pfr1, x='input', y='prob', err_style='bars', ax=axes[2, 1], hue='source', legend=False)\naxes[2, 1].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[2, 1].set_ylim((0, 1))\n\nsns.lineplot(data=cmr_pfr2, x='input', y='prob', err_style='bars', ax=axes[2, 2], hue='source', legend=False)\naxes[2, 2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[2, 2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\nplt.savefig('cmr_summary_murdock1962.pdf', bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PrototypeCMR\n\n\n\n\n\n\n\n\n\n(b) InstanceCMR\n\n\n\n\nFigure 7.2: Comparison of summary statistics between each model against observed data (Murdock Jr 1962)\n\n\n\n\n\n\nMurdock Jr, Bennet B. 1962. “The Serial Position Effect of Free Recall.” Journal of Experimental Psychology 64 (5): 482.\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129."
  },
  {
    "objectID": "06_Item_Repetitions_Sims.html",
    "href": "06_Item_Repetitions_Sims.html",
    "title": "8  Repetition Effects",
    "section": "",
    "text": "While previous analyses evince that our prototype-based and instance-based implementations of CMR equivalently account for free recall performance when each unique item is presented just once during study, there is reason to suspect that the models might diverge when it comes to accounting for the effect of item repetition on later free recall.\nPrevious work (Siegel and Kahana 2014) has related CMR to two broad accounts of how item repetition influences memory and in particular drives the spacing effect, a monotonic relationship between recall probability and the size of the lag between item repetitions in a study list. Under the contextual-variability account (Anderson and Bower 1972), each time an item is studied, it’s associated in memory with the current state of a slowly drifting contextual representation. Depending on how spaced apart two presentations of an item might be, the contextual states they are associated with might either be very similar or very distinct. Later, participants use the current state of their contextual representation to probe their memories and retrieve items during free recall. When an item has been associated with diverse contextual states, it can correspondingly be retrieved using diverse possible cues. In this way, the improvements in recall we gain from spacing presentations of an item are explained in terms of variation in the range of possible cues that can trigger recall of that item. A study-phase retrieval account of the spacing effect alternatively emphasizes the consequences of studying a successively presented item. According to the account, when this happens we retrieve memories of the repeated item’s earlier occurrences and their associated contexts. When this happens, it’s proposed that retrieved information is memorally associated with information corresponding to the current presentation context.\nAnalyses of our instance-based implementation of CMR so far suggest it realizes these mechanisms similarly to the original prototype-based CMR. A potentially more relevant distinction between the models might instead turn on differences in how records of past experience are integrated for retrieval. InstanceCMR, like MINERVA 2, has the option to apply its nonlinear activation scaling parameter \\(\\tau\\) to activations of individual traces - that is, before integration into a unitary vector tracking retrieval support. However, CMR does not access trace activations and applies \\(\\tau\\) to the integrated echo representation result.\nThis distinction between instance-based and prototype-based architectures has been marshalled to explain model differences in other research contexts (e.g., Jamieson et al. 2018). In this context, however, the different between applying \\(\\tau\\) to trace activations or echo content is between enforcing quasi-linear or quasi-exponential effect of item repetition on subsequent recall probability. Suppose a constant sensitivity parameter \\(\\tau\\) and that two distinct experiences each contributed a support of \\(c\\) for a given feature unit in the current recall. Under trace-based sensitivity scaling, the retrieval support for that feature unit would be \\(c^{\\tau} + c^{\\tau}\\). But under echo-based sensitivity scaling, support would be \\({(c + c)}^{\\tau}\\), a much larger quantity.\nAnother way to illustrate this architectural difference is by simulation. We can have our prototype-based and each variant of our instance-based implementation of CMR simulate a traditional list-learning experiment with study of 20 unique items in order. Then, we can simulate repeated study of an arbitrary item in that list and measure the effect on the probability of retrieving that item through context-based recall for each successive repetition. Figure 8.1 plots the result of of this simulation over 1000 experiments for 50 item repetitions using PrototypeCMR and InstanceCMRand model parameters fitted using data from Murdock and Okada (1970). Model fitting over a different dataset might obviate these observed differences; however these simulations raise the possibility that with increasing item repetitions, prototype-based and instance-based implementations of CMR might support different predictions about the influence of item repetition on later recall probability, motivating further investigation.\n\n\n\n\n\n\n\n(a) PrototypeCMR\n\n\n\n\n\n\n\n\n\n(b) InstanceCMR\n\n\n\n\nFigure 8.1: Simulated effect of successive item repetitions on recall probability, by model, using parameters fitted over Murdock and Okada (1970) dataset.\n\n\n\nFitting Results\n\nFree recall datasets with high amounts of repetition to the extent simulated in the above example do not exist. However, to support an initial comparison of how models account for item repetition effects, we use data associated with Siegel and Kahana (2014)’s analyses. Within the dataset, 35 subjects performed delayed free recall of 48 lists over four sessions. Except for deliberate item repetitions, words were unique within each session and did not repeat in successive sessions. The semantic relatedness of words was also controlled below a value of .55 according to WAS (Steyvers, Shiffrin, and Nelson 2005). Across trials, lists were structured in four different ways:\n\nIn control lists, all items were only presented once.\nIn pure massed lists, items were presented twice, always in succession (e.g. 1, 1, 2, 2)\nIn pure spaced lists, items were presented twice with spacing of repetitions from 1 to 8 positions, with each spacing amount equiprobable.\nFinally, mixed lists feature once-presented, massed, and spaced items, with each spacing amount equiprobable\n\n\n\ncode – load dependencies and data\nfrom compmemlearn.fitting import murdock_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_murdock1962_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport matplotlib.pyplot as plt\nfrom numba.core import types\nfrom numba import njit\nimport seaborn as sns\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\n\nfrom compmemlearn.fitting import lohnas_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nimport pandas as pd\nfrom psifr import fr\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List\nimport numpy as np\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\n\nImportError: cannot import name 'cmr_murd_objective_function' from 'compmemlearn.fitting' (c:\\users\\gunnj\\compmemlearn\\compmemlearn\\fitting.py)\n\n\nAs with previous analyses, each model variant was fit once for each participant to identify the parameter configuration maximizing the likelihood of observed recall sequences given the considered model. The distribution of data log-likelihoods given each fitted model and participant are plotted in ?fig-lohnasfits, with median values for each model variant highlighted. Similarly to previous analyses, these value distributions were found largely similar. The median log-likelihood across participants for PrototypeCMR and InstanceCMR were found to be 1711.5 and 1713.8, respectively, suggesting that all considered model variants can predict recall sequences even when item repetitions occur during study with similar degrees of success.\nWhile follow-up analysis of summary statistics in previous analyses focused on benchmark phenomena such as serial position effects, inclusion of item repetitions in study designs complicates associated visualizations. Instead, we focused comparison on summary statistics measuring classical item repetition effects. In ?fig-lohnasspacings, we measure how effectively our prototype- and instance-based CMR implementations account for the spacing effect. Main model variants were fit across the entire dataset to optimize the likelihood of observed recall sequences. Then, 1000 times for each model, study phases of each trial in the mixed condition of the dataset were simulated and then followed with simulation of free recall. We then plot for both the behavioral data and simulated datasets, the rate at which items were recalled, binned based on the number of intervening items between repetitions. Recapitulating all previous analyses, we again found that both our prototype-based and main instance-based implementations of CMR predicted similar patterns of effects of repetition spacing on later item recall, despite architectural differences.\n\n\n\n\n\nAnderson, John R, and Gordon H Bower. 1972. “Recognition and Retrieval Processes in Free Recall.” Psychological Review 79 (2): 97.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMurdock, Bennet B, and Ronald Okada. 1970. “Interresponse Times in Single-Trial Free Recall.” Journal of Experimental Psychology 86 (2): 263.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nSteyvers, Mark, Richard M Shiffrin, and Douglas L Nelson. 2005. “Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory.”"
  },
  {
    "objectID": "06_Item_Repetitions_Lohnas.html",
    "href": "06_Item_Repetitions_Lohnas.html",
    "title": "9  Repetition Effects: Fitting",
    "section": "",
    "text": "Free recall datasets with high amounts of repetition to the extent simulated in the above example do not exist. However, to support an initial comparison of how models account for item repetition effects, we use data associated with Siegel and Kahana (2014)’s analyses. Within the dataset, 35 subjects performed delayed free recall of 48 lists over four sessions. Except for deliberate item repetitions, words were unique within each session and did not repeat in successive sessions. The semantic relatedness of words was also controlled below a value of .55 according to WAS (Steyvers, Shiffrin, and Nelson 2005). Across trials, lists were structured in four different ways:\n\nIn control lists, all items were only presented once.\nIn pure massed lists, items were presented twice, always in succession (e.g. 1, 1, 2, 2)\nIn pure spaced lists, items were presented twice with spacing of repetitions from 1 to 8 positions, with each spacing amount equiprobable.\nFinally, mixed lists feature once-presented, massed, and spaced items, with each spacing amount equiprobable\n\n\n\ncode – load dependencies and data\nfrom compmemlearn.fitting import murdock_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_murdock1962_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport matplotlib.pyplot as plt\nfrom numba.core import types\nfrom numba import njit\nimport seaborn as sns\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\n\nfrom compmemlearn.fitting import lohnas_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nimport pandas as pd\nfrom psifr import fr\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List\nimport numpy as np\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\n\nImportError: cannot import name 'cmr_murd_objective_function' from 'compmemlearn.fitting' (c:\\users\\gunnj\\compmemlearn\\compmemlearn\\fitting.py)\n\n\nAs with previous analyses, each model variant was fit once for each participant to identify the parameter configuration maximizing the likelihood of observed recall sequences given the considered model. The distribution of data log-likelihoods given each fitted model and participant are plotted in ?fig-lohnasfits, with median values for each model variant highlighted. Similarly to previous analyses, these value distributions were found largely similar. The median log-likelihood across participants for PrototypeCMR and InstanceCMR were found to be 1711.5 and 1713.8, respectively, suggesting that all considered model variants can predict recall sequences even when item repetitions occur during study with similar degrees of success.\nWhile follow-up analysis of summary statistics in previous analyses focused on benchmark phenomena such as serial position effects, inclusion of item repetitions in study designs complicates associated visualizations. Instead, we focused comparison on summary statistics measuring classical item repetition effects. In ?fig-lohnasspacings, we measure how effectively our prototype- and instance-based CMR implementations account for the spacing effect. Main model variants were fit across the entire dataset to optimize the likelihood of observed recall sequences. Then, 1000 times for each model, study phases of each trial in the mixed condition of the dataset were simulated and then followed with simulation of free recall. We then plot for both the behavioral data and simulated datasets, the rate at which items were recalled, binned based on the number of intervening items between repetitions. Recapitulating all previous analyses, we again found that both our prototype-based and main instance-based implementations of CMR predicted similar patterns of effects of repetition spacing on later item recall, despite architectural differences.\n\n\n\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nSteyvers, Mark, Richard M Shiffrin, and Douglas L Nelson. 2005. “Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory.”"
  },
  {
    "objectID": "07_Discussion.html",
    "href": "07_Discussion.html",
    "title": "9  Discussion",
    "section": "",
    "text": "In light of a collection of results across task domains distinguishing between the performance of prototype- and instance-based models of memory-based behavior, we searched for evidence of a similar distinction with respect to the free recall task paradigm. To do this, we specified and compared the original prototype-based implementation of an established model of task performance based in retrieved context theory (PrototypeCMR) against a parallel instance-based variant (CMR) across diverse experimental conditions, including a dataset featuring variable study list lengths across trials and a dataset featuring item repetitions within trials and variable item repetition spacing between trials. While our simulation analyses focused on the effect of item repetition on recall rates identified some hypothetical distinctions between model predictions, the model variants accounted for human performance on the free recall task with similar effectiveness in each dataset considered.\nOne clear conclusion we can draw from these analyses is that the success of the theoretical commitments made by the Context Maintenance and Retrieval model and models like it are likely not especially dependent on the architectures in which they are implemented. Instead, the main insights of retrieved context theories (at least as formalized by CMR) are highly portable, and can likely be esconced within any reasonable model architecture where memory search via temporal contextual representations might prove valuable. Establishing the portability of these successful theoretical principles across modeling approaches helps advance the historical effort in cognitive science to develop “a general account of memory and its processes in a working computational system to produce a common explanation of behavior rather than a set of lab-specific and domain-specific theories for different behaviors” (Newell 1973; Jamieson et al. 2018).\nThis finding has been increasingly validated lately in other work. Logan (2021) for example similarly embeds mechanisms for maintaining and organizing retrieval using temporal contextual representations within an instance-based architecture to simultaneously account for performance on substantively distinct variations of a task requiring participants to encode and report random strings in left-to-right order by typing them on a computer keyboard, including whole report, serial recall, and copy typing. Other projects more motivated by neuroscientific data (e.g. (Ketz, Morkonda, and O’Reilly 2013); (Schapiro et al. 2017)) embed mechanisms for context-based retrieval within detailed formal accounts of hippocampus functionality more complex than either the instance-based or hebbian associative network architectures considered in this work. Our head-to-head comparison of this instance-based account of context maintenance and retrieval against its more standard prototype-based counterpart and observation that both competitively explain free recall performance under varied conditions further evinces the architectural independence of the retrieved context account of memory search.\nHow do these results fit into the context of other work identifying substantive contrasts between instance- and prototype-based models? Research by Jamieson et al. (2018) comparing the model architectures’ capacity to account for semantic memory emphasizes that the main limitation of prototype-based models is the information that they discard or distort toward some center-of-tendency at encoding - idiosyncratic item or contextual features that do not reflect generalities across experience. With this information discarded or suppressed, memory cues selective for those idiosyncratic features cannot result in retrieval of relevant information. Instance-based models on the other hand are able to select information across learning episodes to include in an abstractive representation based on the content of a cue, enabling flexible retrieval of idiosyncratic features while suppressing more common but irrelevant features.\nThe trace-based application of instance models’ \\(\\tau\\) parameter is described as fundamental to the unique flexibility of instance-based models outlined by Jamieson et al. (2018), as it enables instance-based models to modulate the influence of particular memory traces in a retrieved echo representation nonlinearly based on the traces’ similarity to a probe. However, while the prototype-based semantic memory models examined by Jamieson et al. (2018) exclude a similar response scaling mechanism, the standard prototype-based specification of CMR does include one. Research on category learning (Nosofsky and Zaki 2002; Stanton, Nosofsky, and Zaki 2002) also contrasting prototype- and instance-based models of the behavior also identifies instance models’ characteristic response-scaling mechanism as crucial for accounting for deterministic patterns in memory performance under various research conditions. However, they also evaluate prototype-based models that, like CMR, do include response-scaling mechanisms – though by definition only instance-based models apply the mechanism to similarities computed between traces and probe representations. To compare the instance-based Exemplar-Generalization model against a prototype-based model with a similar response scaling mechanism, Nosofsky and Zaki (2002) focused on how the models differentially characterize generalization, in this case the category assignment of novel items excluded from initial training. Finding that participants mroe often classify items in the the same categories based on their similarity to one another rather than based on similarity to hypothetical prototype-representations, the instance-based Exemplar-Generalization model came out ahead.\nEven the research above drawing distinctions between the explanatory performance of instance-based and prototype-based models report experimental conditions where the two architectures perform similarly. We can conclude that either the considered research conditions or the model specifications themselves also sidestep any of their more substantive differences. Two assumptions enforced in both the prototype- and instance-based frameworks compared here as well as in corresponding datasets were that list item were effectively representationally orthogonal, and encountered just once or twice before retrieval. Furthermore, contextual states as characterized by CMR differ a consistent amount from item to item during study in a traditional list learning experiment. The assumptions together may prevent a distinction from emerging between highly common and highly idiosyncratic item or contextual features under traditional research conditions as emphasized in architectural comparisons drawn by Jamieson et al. (2018). Similarly, the uniform similarity structure of list items studied and recalled across evaluated datasets here potentially sidesteps issues raised by Nosofsky and Zaki (2002) with prototype-based models.\nHigher rates of item repetition or enforced distortions of contextual variation (such as by dividing an encoding phase into distinct trials or sessions) might be enough to more clearly distinguish architecture performance. Simulations of high rates of item repetitions reported in Figure 8.1 identify one potentially relevant difference between InstanceCMR and PrototypeCMR – an exponential rate of increase of recall rates for repeated items in the former, but not the latter – but the distinction seems independent from contrasts drawn between the architectures drawn by other researchers such as Jamieson et al. (2018) and Nosofsky and Zaki (2002). By contrast, research conditions where items are repeated rarely in some contexts but frequently in others or nonorthogonal item features influence and are factored into model performance would further explore the relevance of prior exploration of prototype- and instance-based architectures to our understanding of free recall and similar tasks. At the same time, our current results establish that architectural distinctions relevant in other tasks domains may be not particularly critical for accounting for performance across the more traditional research conditions explored here.\nWhile these results suggest some level of equivalance between instance-based and prototype-based models with respect to accounting for free recall performance, their generality is as limited to the simple architectures evaluated as to the datasets explored. More complex or just different models that might be classed in one of these categories may not exhibit the same patterns. For example, the examinations here only consider a constrained conceptualization of instance-based models styled after the MINERVA 2 simulation model of human memory (Hintzman 1984). Lehman and Malmberg (2013) produced a dual store model of performance on various recall tasks and can be classed as an instance-based model despite excluding some traditional features of models inspired by Hintzman (1984), such as reliance on a single trace store and application of a nonlinear response scaling mechanism during retrieval. Its main assumption is that a limited-capacity buffer tracks both information about items and about associations between items and between items and their encoding context; at the same time, it supposes that a secondary, unlimited-capacity store is, with some probability, populated with traces from this buffer. For the free recall task, the model integrates concepts from retrieved context theory, including initiation of recall based on the content of a context cue. With these mechanisms, the model is able to account for serial position and temporal contiguity effects using novel mechanisms not directly instantiated in the variants of CMR explored here. Similarities in predictions offered by the different models indicate that they include analogous features, but important explanatory differences may just as well exist between them and MINERVA-based instance models under certain research conditions as exist between prototype-based and instance-based models in others. Deeper clarification of the distinctions and homologies between different models characterizing performance on memory tasks such as free recall is critical for driving further modeling innovation.\n\n\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nKetz, Nicholas, Srinimisha G Morkonda, and Randall C O’Reilly. 2013. “Theta Coordinated Error-Driven Learning in the Hippocampus.” PLoS Comput Biol 9 (6): e1003067.\n\n\nLehman, Melissa, and Kenneth J Malmberg. 2013. “A Buffer Model of Memory Encoding and Temporal Correlations in Retrieval.” Psychological Review 120 (1): 155.\n\n\nLogan, Gordon D. 2021. “Serial Order in Perception, Memory, and Action.” Psychological Review 128 (1): 1.\n\n\nNewell, Allen. 1973. “You Can’t Play 20 Questions with Nature and Win: Projective Comments on the Papers of This Symposium.”\n\n\nNosofsky, Robert M, and Safa R Zaki. 2002. “Exemplar and Prototype Models Revisited: Response Strategies, Selective Attention, and Stimulus Generalization.” Journal of Experimental Psychology: Learning, Memory, and Cognition 28 (5): 924.\n\n\nSchapiro, Anna C, Nicholas B Turk-Browne, Matthew M Botvinick, and Kenneth A Norman. 2017. “Complementary Learning Systems Within the Hippocampus: A Neural Network Modelling Approach to Reconciling Episodic Memory with Statistical Learning.” Philosophical Transactions of the Royal Society B: Biological Sciences 372 (1711): 20160049.\n\n\nStanton, Roger D, Robert M Nosofsky, and Safa R Zaki. 2002. “Comparisons Between Exemplar Similarity and Mixed Prototype Models Using a Linearly Separable Category Structure.” Memory & Cognition 30 (6): 934–44."
  },
  {
    "objectID": "01_Classic_CMR.html#the-prototype-based-account-of-context-maintenance-and-retrieval",
    "href": "01_Classic_CMR.html#the-prototype-based-account-of-context-maintenance-and-retrieval",
    "title": "3  default_exp models",
    "section": "3.1 The Prototype-Based Account of Context Maintenance and Retrieval",
    "text": "Retrieved context theories explain memory search in terms of interactions between between two representations across experience: one of temporal context (a context layer, \\(C\\)) and another of features of studied items (an item layer, \\(F\\)). While this paper introduces an instance-based account of these interactions, we here specify a variant of the original prototype-based context maintenance and retrieval (CMR) model (Polyn, Norman, and Kahana 2009) to support comparison against this account. The instance-based model we emphasize tracks the history of interactions between context and item features by storing a discrete record of each experience in memory for later inspection. In contrast, PrototypeCMR maintains a simplified neural network whose connection weights accumulate a center of tendency representation reflecting context and item interactions across experience.\n\nCode\n# export\n\nimport numpy as np\nfrom numba import float64, int32, boolean\nfrom numba.experimental import jitclass\nfrom compmemlearn.familiarity import familiarity_weighting\n\ncmr_spec = [\n    ('item_count', int32), \n    ('encoding_drift_rate', float64),\n    ('delay_drift_rate', float64),\n    ('start_drift_rate', float64),\n    ('recall_drift_rate', float64),\n    ('shared_support', float64),\n    ('item_support', float64),\n    ('learning_rate', float64),\n    ('primacy_scale', float64),\n    ('primacy_decay', float64),\n    ('stop_probability_scale', float64),\n    ('stop_probability_growth', float64),\n    ('choice_sensitivity', float64),\n    ('context', float64[::1]),\n    ('start_context_input', float64[::1]),\n    ('delay_context_input', float64[::1]),\n    ('preretrieval_context', float64[::1]),\n    ('recall', int32[::1]),\n    ('retrieving', boolean),\n    ('recall_total', int32),\n    ('primacy_weighting', float64[::1]),\n    ('probabilities', float64[::1]),\n    ('mfc', float64[:,::1]),\n    ('mcf', float64[:,::1]),\n    ('encoding_index', int32),\n    ('items', float64[:,::1]),\n    ('drift_familiarity_scale', float64),\n    ('mfc_familiarity_scale', float64),\n    ('mcf_familiarity_scale', float64),\n    ('sampling_rule', int32)\n]\n\n\nCode\n# export\n@jitclass(cmr_spec)\nclass Classic_CMR:\n\n    def __init__(self, item_count, presentation_count, parameters):\n\n        # store initial parameters\n        self.item_count = item_count\n        self.encoding_drift_rate = parameters['encoding_drift_rate']\n        self.delay_drift_rate = parameters['delay_drift_rate']\n        self.start_drift_rate = parameters['start_drift_rate']\n        self.recall_drift_rate = parameters['recall_drift_rate']\n        self.shared_support = parameters['shared_support']\n        self.item_support = parameters['item_support']\n        self.learning_rate = parameters['learning_rate']\n        self.primacy_scale = parameters['primacy_scale']\n        self.primacy_decay = parameters['primacy_decay']\n        self.stop_probability_scale = parameters['stop_probability_scale']\n        self.stop_probability_growth = parameters['stop_probability_growth']\n        self.choice_sensitivity = parameters['choice_sensitivity']\n        self.drift_familiarity_scale = parameters['drift_familiarity_scale']\n        self.mfc_familiarity_scale = parameters['mfc_familiarity_scale']\n        self.mcf_familiarity_scale = parameters['mcf_familiarity_scale']\n        self.sampling_rule = parameters['sampling_rule']\n        \n        # at the start of the list context is initialized with a state \n        # orthogonal to the pre-experimental context\n        # associated with the set of items\n        self.context = np.zeros(item_count + 2)\n        self.context[0] = 1\n        self.preretrieval_context = self.context\n        self.recall = np.zeros(item_count, int32) # recalls has at most `item_count` entries\n        self.retrieving = False\n        self.recall_total = 0\n\n        # predefine primacy weighting vectors\n        self.primacy_weighting = parameters['primacy_scale'] * np.exp(\n            -parameters['primacy_decay'] * np.arange(presentation_count)) + 1\n\n        # preallocate for outcome_probabilities\n        self.probabilities = np.zeros((item_count + 1))\n\n        # predefine contextual input vectors relevant for delay_drift_rate and start_drift_rate parameters\n        self.start_context_input = np.zeros((self.item_count+2))\n        self.start_context_input[0] = 1\n        self.delay_context_input = np.zeros((self.item_count+2))\n        self.delay_context_input[-1] = 1\n\n        # The two layers communicate with one another through two sets of \n        # associative connections represented by matrices Mfc and Mcf. \n        # Pre-experimental Mfc is 1-learning_rate and pre-experimental Mcf is\n        # item_support for i=j. For i!=j, Mcf is shared_support.\n        self.mfc = np.eye(item_count, item_count+2, 1) * (1-self.learning_rate)\n        self.mcf = np.ones((item_count, item_count)) * self.shared_support\n        for i in range(item_count):\n            self.mcf[i, i] = self.item_support\n        self.mcf =  np.vstack((np.zeros((1, item_count)), self.mcf, np.zeros((1, item_count))))\n        self.encoding_index = 0\n        self.items = np.eye(item_count, item_count)\n\n    def experience(self, experiences):\n        \n        for i in range(len(experiences)):\n            \n            mfc_familiarity = 1\n            mcf_familiarity = 1\n            if np.any(self.context[1:-1] > 0) and ((self.mfc_familiarity_scale != 0) or (self.mcf_familiarity_scale != 0)):\n                \n                similarity = np.dot(self.context[1:-1], experiences[i]) / (\n                    np.sqrt(np.dot(self.context[1:-1], self.context[1:-1])) * np.sqrt(\n                        np.dot(experiences[i],experiences[i])))\n            \n                mfc_familiarity = familiarity_weighting(self.mfc_familiarity_scale, similarity)\n                mcf_familiarity = familiarity_weighting(self.mcf_familiarity_scale, similarity)\n\n            self.update_context(self.encoding_drift_rate, experiences[i])\n            self.mfc += mfc_familiarity * self.learning_rate * np.outer(self.context, experiences[i]).T\n            self.mcf += mcf_familiarity * self.primacy_weighting[self.encoding_index] * np.outer(\n                self.context, experiences[i])\n            self.encoding_index += 1\n\n    def update_context(self, drift_rate, experience):\n\n        # first pre-experimental or initial context is retrieved\n        familiarity = 1\n        if len(experience) == len(self.mfc):\n            \n            if np.any(self.context[1:-1] > 0) and self.drift_familiarity_scale != 0:\n                \n                similarity = np.dot(self.context[1:-1], experience) / (\n                     np.sqrt(np.dot(self.context[1:-1], self.context[1:-1])) * np.sqrt(\n                          np.dot(experience,experience)))\n                familiarity = familiarity_weighting(self.drift_familiarity_scale, similarity)\n\n            context_input = np.dot(experience, self.mfc)\n            context_input = np.power(context_input, familiarity)\n\n            # if the context is not pre-experimental, the context is retrieved\n            context_input = context_input / np.sqrt(np.sum(np.square(context_input))) # norm to length 1\n            \n        else:\n            context_input = experience\n  \n        # updated context is sum of context and input, modulated by rho to have len 1 and some drift_rate\n        rho = np.sqrt(1 + np.square(min(drift_rate, 1.0)) * (\n            np.square(self.context * context_input) - 1)) - (\n                min(drift_rate, 1.0) * (self.context * context_input))\n        self.context = (rho * self.context) + (min(drift_rate, 1.0) * context_input)\n\n    def activations(self, probe, use_mfc=False):\n\n        if use_mfc:\n            return np.dot(probe, self.mfc) + 10e-7\n        else:\n            return np.dot(probe, self.mcf) + 10e-7\n\n    def outcome_probabilities(self):\n\n        self.probabilities[0] = min(self.stop_probability_scale * np.exp(\n            self.recall_total * self.stop_probability_growth), 1.0 - (\n                 (self.item_count-self.recall_total) * 10e-7))\n        self.probabilities[1:] = 10e-7\n\n        if self.probabilities[0] < (1.0 - ((self.item_count-self.recall_total) * 10e-7)):\n\n            # measure the activation for each item\n            activation = self.activations(self.context)\n\n            # already recalled items have zero activation\n            activation[self.recall[:self.recall_total]] = 0\n\n            if np.sum(activation) > 0:\n\n                # power sampling rule vs modified exponential sampling rule\n                if self.sampling_rule == 0:\n                    activation = np.power(activation, self.choice_sensitivity)\n                else:\n                    pre_activation = (2 * activation)/ self.choice_sensitivity\n                    activation = np.exp(pre_activation - np.max(pre_activation))\n                \n                # normalized result downweighted by stop prob is probability of choosing each item\n                self.probabilities[1:] = (1-self.probabilities[0]) * activation / np.sum(activation)\n            \n        return self.probabilities\n\n    def free_recall(self, steps=None):\n\n        # some amount of the pre-list context is reinstated before initiating recall\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        # number of items to retrieve is # of items left to recall if steps is unspecified\n        if steps is None:\n            steps = self.item_count - self.recall_total\n        steps = self.recall_total + steps\n        \n        # at each recall attempt\n        while self.recall_total < steps:\n\n            # the current state of context is used as a retrieval cue to attempt recall of a studied item\n            # compute outcome probabilities and make choice based on distribution\n            outcome_probabilities = self.outcome_probabilities()\n            if np.any(outcome_probabilities[1:]):\n                choice = np.sum(np.cumsum(outcome_probabilities) < np.random.rand(), dtype=int32)\n            else:\n                choice = 0\n\n            # resolve and maybe store outcome\n            # we stop recall if no choice is made (0)\n            if choice == 0:\n                self.retrieving = False\n                self.context = self.preretrieval_context\n                break\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.items[choice - 1])\n        return self.recall[:self.recall_total]\n\n    def force_recall(self, choice=None):\n\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        if choice is None:\n            pass\n        elif choice > 0:\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.items[choice - 1])\n        else:\n            self.retrieving = False\n            self.context = self.preretrieval_context\n        return self.recall[:self.recall_total]\n\n\nParameters and structures specifying CMR\n\n\n\n\n\n\n\n\nStructure Type\nSymbol\nName\nDescription\n\n\n\n\nArchitecture\n\n\n\n\n\n\n\\(C\\)\ntemporal context\nA recency-weighted average of encoded items\n\n\n\n\\(F\\)\nitem features\nCurrent pattern of item feature unit activations\n\n\n\n\\(M^{FC}\\)\n\nencoded feature-to-context associations\n\n\n\n\\(M^{CF}\\)\n\nencoded context-to-feature associations\n\n\nContext Updating\n\n\n\n\n\n\n\\({\\beta}_{enc}\\)\nencoding drift rate\nRate of context drift during item encoding\n\n\n\n\\({\\beta}_{start}\\)\nstart drift rate\nAmount of start-list context retrieved at start of recall\n\n\n\n\\({\\beta}_{rec}\\)\nrecall drift rate\nRate of context drift during recall\n\n\nAssociative Structure\n\n\n\n\n\n\n\\({\\alpha}\\)\nshared support\nAmount of support items initially have for one another\n\n\n\n\\({\\delta}\\)\nitem support\nInitial pre-experimental contextual self-associations\n\n\n\n\\({\\gamma}\\)\nlearning rate\nAmount of experimental context retrieved by a recalled item\n\n\n\n\\({\\phi}_{s}\\)\nprimacy scale\nScaling of primacy gradient on trace activations\n\n\n\n\\({\\phi}_{d}\\)\nprimacy decay\nRate of decay of primacy gradient\n\n\nRetrieval Dynamics\n\n\n\n\n\n\n\\({\\tau}\\)\nchoice sensitivity\nExponential weighting of similarity-driven activation\n\n\n\n\\({\\theta}_{s}\\)\nstop probability scale\nScaling of the stop probability over output position\n\n\n\n\\({\\theta}_{r}\\)\nstop probability growth\nRate of increase in stop probability over output position\n\n\n\n\n3.1.1 Initial State\nAssociative connections built within prototypeCMR are represented by matrices \\(M^{FC}\\) and \\(M^{CF}\\).\nTo summarize pre-experimental associations built between relevant item features and possible contextual states, we initialize \\(M^{FC}\\) according to:\n\\[\nM^{FC}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n          0 \\text{, if } i \\neq j\n   \\end{alignedat} \\end{cases}\n \\qquad(3.1)\\]\nThis connects each unit on \\(F\\) to a unique unit on \\(C\\). Used this way, \\(\\gamma\\) controls the relative contribution of pre-experimentally acquired associations to the course of retrieval compared to experimentally acquired associations. Correspondingly, context-to-feature associations tracked by \\(M^{CF}\\) are set according to:\n\\[\nM^{CF}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\delta \\text{, if } i=j \\\\\\\n          \\alpha \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(3.2)\\]\nLike \\(\\gamma\\) with respect to \\(M^{FC}\\), the \\(\\delta\\) parameter controls the contribution of pre-experimental context-to-feature associations relative to experimentally acquired ones. Since context-to-feature associations organizes the competition of items for retrieval, the \\(\\alpha\\) parameter specifies a uniform baseline extent to which items support one another in that competition.\nContext is initialized with a state orthogonal to any of those pre-experimentally associated with an relevant item feature. Feature representations corresponding to items are also assumed to be orthonormal with respect to one another such that each unit on \\(F\\) corresponds to one item.\n\n\n3.1.2 Encoding Phase\nWhenever an item \\(i\\) is presented for study, its corresponding feature representation \\(f_i\\) is activated on \\(F\\) and its contextual associations encoded into \\(M^{FC}\\) are retrieved, altering the current state of context \\(C\\).\nThe input to context is determined by:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.3)\\]\nand normalized to have length 1. Context is updated based on this input according to:\n\\[ \nc_i = \\rho_ic_{i-1} + \\beta_{enc} c_{i}^{IN}\n \\qquad(3.4)\\]\nwith \\(\\beta\\) (for encoding we use \\(\\beta_{enc}\\)) shaping the rate of contextual drift with each new experience, and \\(\\rho\\) enforces the length of \\(c_i\\) to 1 according to:\n\\[ \n\\rho_i = \\sqrt{1 + \\beta^2\\left[\\left(c_{i-1} \\cdot c^{IN}_i\\right)^2 - 1\\right]} - \\beta\\left(c_{i-1} \\cdot\nc^{IN}_i\\right)\n \\qquad(3.5)\\]\nAssociations between each \\(c_i\\) and \\(f_i\\) are built through Hebbian learning:\n\\[\n\\Delta M^{FC}_{exp} = \\gamma c_i f^{'}_i\n \\qquad(3.6)\\]\nand\n\\[\n\\Delta M^{CF}_{exp} = \\phi_i f_i c^{'}_i\n \\qquad(3.7)\\]\nwhere \\(\\phi_i\\) enforces a primacy effect, scales the amount of learning based on the serial position of the studied item according to\n\\[ \n\\phi_i = \\phi_se^{-\\phi_d(i-1)} + 1\n \\qquad(3.8)\\]\nThis function decays over time, such that \\(\\phi_{s}\\) modulates the strength of primacy while \\(\\phi_{d}\\) modulates the rate of decay.\nThis extended Hebbian learning process characterizes how PrototypeCMR performs abstraction. When each item is encoded with a particular temporal context, representations are updated to aggregate a prototypical summary of the item’s temporal contextual associations in \\(M^{FC}\\) and vice versa in \\(M^{CF}\\).\n\n\n3.1.3 Retrieval Phase\nTo help the model account for the primacy effect, we assume that between the encoding and retrieval phase of a task, the content of \\(C\\) has drifted some amoung back toward its pre-experimental state and set the state of context at the start of retrieval according to following, with \\(\\rho\\) calculated as specified above:\n\\[ \nc_{start} = \\rho_{N+1}c_N + \\beta_{start}c_0\n \\qquad(3.9)\\]\nAt each recall attempt, the current state of context is used as a cue to attempt retrieval of some studied item. An activation \\(a\\) is solicited for each item according to:\n\\[ \na = M^{CF}c\n \\qquad(3.10)\\]\nEach item gets a minimum activation of \\(10^{-7}\\). To determine the probability of a given recall event, we first calculate the probability of stopping recall - returning no item and ending memory search. This probability varies as a function of output position \\(j\\):\n\\[\nP(stop, j) = \\theta_se^{j\\theta_r}\n \\qquad(3.11)\\]\nIn this way, \\(\\theta_s\\) and \\(\\theta_r\\) control the scaling and rate of increase of this exponential function. Given that recall is not stopped, the probability \\(P(i)\\) of recalling a given item depends mainly on its activation strength according\n\\[\nP(i) = (1-P(stop))\\frac{a^{\\tau}_i}{\\sum_{k}^{N}a^{\\tau}_k}\n \\qquad(3.12)\\]\n\\(\\tau\\) here shapes the contrast between well-supported and poorly supported items: exponentiating a large activation and a small activation by a large value of \\(\\tau\\) widens the difference between those activations, making recall of the most activated item even more likely. Small values of \\(\\tau\\) can alternatively driven recall likelihoods of differentially activated items toward one another.\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.13)\\]\nContext is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\n\n\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129."
  },
  {
    "objectID": "06_Item_Repetitions.html",
    "href": "06_Item_Repetitions.html",
    "title": "8  Repetition Effects",
    "section": "",
    "text": "While previous analyses evince that our prototype-based and instance-based implementations of CMR equivalently account for free recall performance when each unique item is presented just once during study, there is reason to suspect that the models might diverge when it comes to accounting for the effect of item repetition on later free recall.\nPrevious work (Siegel and Kahana 2014) has related CMR to two broad accounts of how item repetition influences memory and in particular drives the spacing effect, a monotonic relationship between recall probability and the size of the lag between item repetitions in a study list. Under the contextual-variability account (Anderson and Bower 1972), each time an item is studied, it’s associated in memory with the current state of a slowly drifting contextual representation. Depending on how spaced apart two presentations of an item might be, the contextual states they are associated with might either be very similar or very distinct. Later, participants use the current state of their contextual representation to probe their memories and retrieve items during free recall. When an item has been associated with diverse contextual states, it can correspondingly be retrieved using diverse possible cues. In this way, the improvements in recall we gain from spacing presentations of an item are explained in terms of variation in the range of possible cues that can trigger recall of that item. A study-phase retrieval account of the spacing effect alternatively emphasizes the consequences of studying a successively presented item. According to the account, when this happens we retrieve memories of the repeated item’s earlier occurrences and their associated contexts. When this happens, it’s proposed that retrieved information is memorally associated with information corresponding to the current presentation context.\nAnalyses of our instance-based implementation of CMR so far suggest it realizes these mechanisms similarly to the original prototype-based CMR. A potentially more relevant distinction between the models might instead turn on differences in how records of past experience are integrated for retrieval. InstanceCMR, like MINERVA 2, has the option to apply its nonlinear activation scaling parameter \\(\\tau\\) to activations of individual traces - that is, before integration into a unitary vector tracking retrieval support. However, CMR does not access trace activations and applies \\(\\tau\\) to the integrated echo representation result.\nThis distinction between instance-based and prototype-based architectures has been marshalled to explain model differences in other research contexts (e.g., Jamieson et al. 2018). In this context, however, the different between applying \\(\\tau\\) to trace activations or echo content is between enforcing quasi-linear or quasi-exponential effect of item repetition on subsequent recall probability. Suppose a constant sensitivity parameter \\(\\tau\\) and that two distinct experiences each contributed a support of \\(c\\) for a given feature unit in the current recall. Under trace-based sensitivity scaling, the retrieval support for that feature unit would be \\(c^{\\tau} + c^{\\tau}\\). But under echo-based sensitivity scaling, support would be \\({(c + c)}^{\\tau}\\), a much larger quantity.\nAnother way to illustrate this architectural difference is by simulation. We can have our prototype-based and each variant of our instance-based implementation of CMR simulate a traditional list-learning experiment with study of 20 unique items in order. Then, we can simulate repeated study of an arbitrary item in that list and measure the effect on the probability of retrieving that item for each successive repetition given a static retrieval cue. Figure 8.1 plots the result of of this simulation over 1000 experiments for 50 item repetitions using PrototypeCMR and InstanceCMRand model parameters fitted using data from Murdock and Okada (1970) and corresponds with our predictions. Model fitting over a different dataset might obviate these observed differences; however these simulations raise the possibility that with increasing item repetitions, prototype-based and instance-based implementations of CMR might support different predictions about the influence of item repetition on later recall probability, motivating further investigation.\n\ncode – 1) load simulation dependencies and set general parameters\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numba.typed import Dict\nfrom numba.core import types\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nexperiment_count = 1000\nitem_count = 20\npresentation_count = 70\n\n\n\ncode – 2) using PrototypeCMR, simulate repeated encoding of arbitrary item after normal trial 50 times and plot variation in recall probability given a static cue\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\ncmr_fit_values = np.array([5.79524319e-01, 4.07083020e-03, 7.24717634e-01, 7.47425733e-01,\n       1.00000000e+00, 9.58358158e-02, 9.55947397e+00, 8.71434638e+01,\n       3.13827247e-02, 3.36754300e-01, 9.25336064e+00, 9.95710836e-01])\n\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(cmr_free_parameters):\n    parameters[name] = cmr_fit_values[index]\n    \nparameters['sampling_rule'] = 0\nparameters['mfc_familiarity_scale'] = 0\nparameters['mcf_familiarity_scale'] = 0\nparameters['drift_familiarity_scale'] = 0\n\nresults = np.zeros((experiment_count, 1+presentation_count-item_count))\n\nfor experiment in range(experiment_count):\n    # arbitrary item and contextual cue\n    repeated_item = 0\n    \n    # initialize model\n    model = Classic_CMR(item_count, presentation_count, parameters)\n    cue = model.context\n    model.experience(model.items)\n    results[experiment, 0] = np.nan \n    \n     # track outcome probability of selected item as it is repeatedly encoded\n    for i in range(presentation_count - item_count):\n        \n        model.experience(model.items[repeated_item:repeated_item+1])\n        \n        if i == 0:\n            cue = model.context\n        \n        pre_cue_context = model.context.copy()\n        model.context = cue\n        results[experiment, i+1] = model.outcome_probabilities()[repeated_item+1]\n        model.context = pre_cue_context\n        \nplt.plot(np.mean(results, axis=0))\nplt.xlabel('Number of Successive Repetitions After 20-Item Trial')\nplt.ylabel('Recall Probability for Repeated Item')\nplt.savefig('cmr_repeffect.pdf', bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\ncode – 2) using InstanceCMR, simulate repeated encoding of arbitrary item after normal trial 50 times and plot variation in recall probability given a static cue\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nicmr_fit_values = np.array([7.25274023e-01, 5.49552946e-03, 7.76637231e-01, 6.81235304e-03,\n       1.00000000e+00, 2.88780665e-01, 6.21718894e+00, 2.83467864e+01,\n       2.40239395e-02, 2.61909193e-01, 1.63472597e+00, 9.45953503e-01])\n\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(icmr_free_parameters):\n    parameters[name] = icmr_fit_values[index]\n    \nparameters['sampling_rule'] = 0\nparameters['choice_sensitivity'] = 1\nparameters['feature_sensitivity'] = 1\n\nresults = np.zeros((experiment_count, 1+presentation_count-item_count))\n\nfor experiment in range(experiment_count):\n    # arbitrary item and contextual cue\n    repeated_item = 0\n    \n    # initialize model\n    model = Instance_CMR(item_count, presentation_count, parameters)\n    cue = model.context\n    model.experience(model.items)\n    results[experiment, 0] = np.nan \n    \n     # track outcome probability of selected item as it is repeatedly encoded\n    for i in range(presentation_count - item_count):\n        \n        model.experience(model.items[repeated_item:repeated_item+1])\n        \n        if i == 0:\n            cue = model.context\n        \n        #cue = model.context # for when i want context to be the cue\n        pre_cue_context = model.context.copy()\n        model.context = cue\n        results[experiment, i+1] = model.outcome_probabilities()[repeated_item+1]\n        model.context = pre_cue_context\n        \nplt.plot(np.mean(results, axis=0))\nplt.xlabel('Number of Successive Repetitions After 20-Item Trial')\nplt.ylabel('Recall Probability for Repeated Item')\nplt.savefig('icmr_repeffect.pdf', bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PrototypeCMR\n\n\n\n\n\n\n\n(b) InstanceCMR\n\n\n\n\nFigure 8.1: Course of effect of successive item repetitions on recall probability, by model, using parameters fitted over Murdock and Okada (1970) dataset and a static contextual cue after simulation of a pure 20-item list.\n\n\nThough initial simulations suggest a way to distinguish between instance- and prototype-based accounts of context maintenance and retrieval, free recall datasets with high amounts of repetition to the extent simulated in the above example do not yet exist. However, to support an initial comparison of how models account for item repetition effects, we use data associated with Siegel and Kahana (2014). Within the dataset, 35 subjects performed delayed free recall of 48 lists over four sessions. Except for deliberate item repetitions, words were unique within each session and did not repeat in successive sessions. The semantic relatedness of words was also controlled below a value of .55 according to WAS (Steyvers, Shiffrin, and Nelson 2005). Across trials, lists were structured in four different ways:\n\nIn control lists, all items were only presented once.\nIn pure massed lists, items were presented twice, always in succession (e.g. 1, 1, 2, 2)\nIn pure spaced lists, items were presented twice with spacing of repetitions from 1 to 8 positions, with each spacing amount equiprobable.\nFinally, mixed lists feature once-presented, massed, and spaced items, with each spacing amount equiprobable\n\n\ncode – 1) load dependencies and data\nfrom compmemlearn.fitting import lohnas_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport matplotlib.pyplot as plt\nfrom numba.core import types\nfrom numba import njit\nimport seaborn as sns\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nAs with previous analyses, each model variant was fit once for each participant to identify the parameter configuration maximizing the likelihood of observed recall sequences given the considered model, considering all conditions of the dataset. The distribution of data log-likelihoods given each fitted model and participant are plotted in Figure 8.2, with median values for each model variant highlighted. Similarly to previous analyses, these value distributions were found largely similar. The distribution of log-likelihood scores between participants for the PrototypeCMR and InstanceCMR model variants only marginally differ, suggesting that all considered model variants can predict recall sequences even when item repetitions occur during study with similar degrees of success.\n\n\ncode – 2) fit PrototypeCMR participant-by-participant\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nindividual_cmr_results = []\n\nfor subject in np.unique(subjects):\n    \n    print(subject)\n\n    selection = subjects == subject\n\n    cost_function = lohnas_objective_function(\n        trials[selection], \n        presentations[selection],\n        init_cmr,\n        {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n        cmr_free_parameters)\n\n    individual_cmr_results.append(differential_evolution(\n        cost_function, cmr_bounds, disp=False))\n    print(individual_cmr_results[-1].fun)\n\n\n\n\ncode – 2) fit InstanceCMR participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nindividual_icmr_results = []\n\nfor subject in np.unique(subjects):\n    \n    print(subject)\n\n    selection = subjects == subject\n\n    cost_function = lohnas_objective_function(\n        trials[selection], \n        presentations[selection],\n        init_icmr,\n        {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n        icmr_free_parameters)\n\n    individual_icmr_results.append(differential_evolution(\n        cost_function, icmr_bounds, disp=False))\n    print(individual_icmr_results[-1].fun)\n\n\n\n\ncode – 3) plot distribution of log-likelihoods across individual subjects and render summary statistics as table\nplt.style.use('default')\n\nindividual_fits = [result.fun for result in individual_icmr_results] + [result.fun for result in individual_cmr_results]\nlabels = ['InstanceCMR'] * len(individual_icmr_results) + ['PrototypeCMR'] * len(individual_cmr_results)\nindividual_df = pd.DataFrame(individual_fits, index=labels, columns=['Fit']).reset_index()\nindividual_df.columns = ['Model', 'Fit']\n\nsns.set(style=\"darkgrid\")\n\ng = sns.catplot(x='Model', y='Fit', data=individual_df, kind='violin', inner='stick')\nsns.swarmplot(x=\"Model\", y=\"Fit\", color=\"k\", size=3, data=individual_df, ax=g.ax)\ng.ax.set_ylabel('Individual-Level Fitted Model Log-Likelihoods');\nplt.savefig('individual_lohnas2014.pdf', bbox_inches=\"tight\")\nplt.show()\n\nsummary_table = pd.DataFrame(group.describe().rename(columns={'Fit':name}).squeeze()\n            for name, group in individual_df.groupby('Model')).T.to_markdown()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstanceCMR\nPrototypeCMR\n\n\n\n\ncount\n35\n35\n\n\nmean\n1676.3\n1668.35\n\n\nstd\n434.819\n429.581\n\n\nmin\n914.375\n856.293\n\n\n25%\n1367.75\n1378.27\n\n\n50%\n1686.97\n1698.98\n\n\n75%\n1955.37\n1957.65\n\n\nmax\n2752.36\n2751.88\n\n\n\n\n\nFigure 8.2: Log-likelihood score distributions for each subject under each considered model (Siegel and Kahana 2014)\n\n\nWhile follow-up analysis of summary statistics in previous analyses focused on benchmark phenomena such as serial position effects, inclusion of item repetitions in study designs complicates associated visualizations. Instead, we focused comparison on summary statistics measuring classical item repetition effects. In Figure 8.3, we measure how effectively our prototype- and instance-based CMR implementations account for the spacing effect. Main model variants were fit to the mixed list (fourth) condition of the entire dataset across subjects to optimize the likelihood of observed recall sequences. Then with each configured model, study phases of each trial in the mixed condition of the dataset were simulated and then followed with simulation of free recall. We then plot for both the behavioral data and simulated datasets, the rate at which items were recalled, binned based on the number of intervening items between repetitions. On the one hand, we observe that both models poorly account for the pattern of recall rates observed as a function of presentation spacing in the mixed condition of the Siegel and Kahana (2014) dataset, exaggerating the mnemonic benefit of item repetition in general while understating the mnemonic effect of increased spacing between repetitions. On the other hand, recapitulating all previous analyses, we again found that both our prototype-based and main instance-based implementations of CMR predicted similar patterns of effects of repetition spacing on later item recall.\n\n\ncode – 1) fit CMR to entire dataset\nselection = list_types == 4\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\ncmr_result = differential_evolution(cost_function, cmr_bounds, disp=True)\n\n\ncode – 2) fit InstanceCMR to entire dataset rather than participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nselection = list_types == 4\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\n\n\ncode – 3) with PrototypeCMR, compose simulated recall_probability_by_lag from overall fitting results\nfrom compmemlearn.analyses import sim_recall_probability_by_lag\n\n\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(cmr_free_parameters):\n    parameters[name] = cmr_result.x[index]\n    \nparameters['sampling_rule'] = 0\nparameters['mfc_familiarity_scale'] = 0\nparameters['mcf_familiarity_scale'] = 0\nparameters['drift_familiarity_scale'] = 0\n\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nselection = list_types == 4\nresult = sim_recall_probability_by_lag(presentations[selection], 10, init_cmr, parameters)\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\n\nax = sns.barplot(x=['N/A', '0', '1-2', '3-5', '6-8'], \n                 y=binned)\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\nplt.show()\n\n\ncode – 4) with InstanceCMR, compose simulated recall_probability_by_lag from overall fitting results\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(icmr_free_parameters):\n    parameters[name] = icmr_result.x[index]\n    \nparameters['sampling_rule'] = 0\nparameters['choice_sensitivity'] = 1\nparameters['feature_sensitivity'] = 1\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nresult = sim_recall_probability_by_lag(presentations[selection], 10, init_icmr, parameters)\n\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\nax = sns.barplot(x=['N/A', '0', '1-2', '3-5', '6-8'], \n                 y=binned)\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\nplt.show()\n\n\n\n\n\n\n\n\n(a) PrototypeCMR\n\n\n\n\n\n\n\n(b) InstanceCMR\n\n\n\n\n\n\n\n(c) Data\n\n\n\n\nFigure 8.3: Comparison of predicted recall probability as function of item repetition spacing between each model and the observed data (Siegel and Kahana 2014).\n\n\n\n\n\n\nAnderson, John R, and Gordon H Bower. 1972. “Recognition and Retrieval Processes in Free Recall.” Psychological Review 79 (2): 97.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMurdock, Bennet B, and Ronald Okada. 1970. “Interresponse Times in Single-Trial Free Recall.” Journal of Experimental Psychology 86 (2): 263.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nSteyvers, Mark, Richard M Shiffrin, and Douglas L Nelson. 2005. “Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory.”"
  },
  {
    "objectID": "00_Introduction.html#research-approach",
    "href": "00_Introduction.html#research-approach",
    "title": "2  Instance and Prototype Accounts of Abstraction",
    "section": "2.2 Research Approach",
    "text": "In this paper, I show that the mechanisms proposed by the influential Context Maintanence and Retrieval (CMR) model of memory search (Morton and Polyn 2016) can be realized within either a prototypical or instance-based model architecture without substantially impacting performance across various experimental conditions. This instance-based CMR (InstanceCMR) extends the established MINERVA 2 multiple traces model (Hintzman 1984, 1986, 1988) to support context-based memory search and simulate performance on the free recall task. I fit InstanceCMR and its original prototype-based counterpart (prototypeCMR) to the sequences of individual responses made by participants in three distinct free recall task datasets.I find that the models account for retrieval performance with similar effectiveness despite architectural differences, including over data manipulating the lengths of study lists between trials and other data manipulating the number of times particular items are studied within trials.\nAnalyses of the two specifications for CMR suggest that these outcomes can be largely explained by the model’s assumption that feature representations corresponding to studied items in free recall experiments are orthogonal — activation of each unit on an item feature layer corresponds to one item. This ensures that context-to-feature associations built via experience of one item do not overlap with associations built through experience of some other distinct item. Correspondingly, the influence of relevant experiences on the content of abstractive representations retrieved via these associations can be selectively enhanced while simultaneously suppressing the influence of less relevant experiences, without any interference. This capacity to nonlinearly modulate the influence of selected learning episodes on recall based on the content of a probe approximates trace-based activation functions realized within instance-based models, sidestepping issues reported about prototype-based memory models in other literatures.\n\n\n\n\nChurch, Kenneth Ward. 2017. “Word2Vec.” Natural Language Engineering 23 (1): 155–62.\n\n\nDumais, Susan T. 2004. “Latent Semantic Analysis.” Annual Review of Information Science and Technology 38 (1): 188–230.\n\n\nGolomb, Julie D, Jonathan E Peelle, Kelly M Addis, Michael J Kahana, and Arthur Wingfield. 2008. “Effects of Adult Aging on Utilization of Temporal and Semantic Associations During Free and Serial Recall.” Memory & Cognition 36 (5): 947–56.\n\n\nHealey, M Karl, and Michael J Kahana. 2016. “A Four-Component Model of Age-Related Memory Change.” Psychological Review 123 (1): 23.\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\n———. 1986. “\"Schema Abstraction\" in a Multiple-Trace Memory Model.” Psychological Review 93 (4): 411.\n\n\n———. 1988. “Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model.” Psychological Review 95 (4): 528.\n\n\nHoward, Marc W, and Michael J Kahana. 2002. “A Distributed Representation of Temporal Context.” Journal of Mathematical Psychology 46 (3): 269–99.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nKahana, Michael J. 1996. “Associative Retrieval Processes in Free Recall.” Memory & Cognition 24 (1): 103–9.\n\n\n———. 2020. “Computational Models of Memory Search.” Annual Review of Psychology 71: 107–38.\n\n\nKragel, James E, Neal W Morton, and Sean M Polyn. 2015. “Neural Activity in the Medial Temporal Lobe Reveals the Fidelity of Mental Time Travel.” Journal of Neuroscience 35 (7): 2914–26.\n\n\nLehman, Melissa, and Kenneth J Malmberg. 2013. “A Buffer Model of Memory Encoding and Temporal Correlations in Retrieval.” Psychological Review 120 (1): 155.\n\n\nLogan, Gordon D. 2021. “Serial Order in Perception, Memory, and Action.” Psychological Review 128 (1): 1.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40.\n\n\nMurdock Jr, Bennet B. 1962. “The Serial Position Effect of Free Recall.” Journal of Experimental Psychology 64 (5): 482.\n\n\nNosofsky, Robert M, and Safa R Zaki. 2002. “Exemplar and Prototype Models Revisited: Response Strategies, Selective Attention, and Stimulus Generalization.” Journal of Experimental Psychology: Learning, Memory, and Cognition 28 (5): 924.\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129.\n\n\nPostman, Leo. 1971. “Organization and Interference.” Psychological Review 78 (4): 290.\n\n\nPuff, C Richard. 1979. Memory Organization and Structure. Academic Press.\n\n\nSchwartz, Greg, Marc W Howard, Bing Jing, and Michael J Kahana. 2005. “Shadows of the Past: Temporal Retrieval Effects in Recognition Memory.” Psychological Science 16 (11): 898–904.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nStanton, Roger D, Robert M Nosofsky, and Safa R Zaki. 2002. “Comparisons Between Exemplar Similarity and Mixed Prototype Models Using a Linearly Separable Category Structure.” Memory & Cognition 30 (6): 934–44.\n\n\nTalmi, Deborah, Lynn J Lohnas, and Nathaniel D Daw. 2019. “A Retrieved Context Model of the Emotional Modulation of Memory.” Psychological Review 126 (4): 455.\n\n\nWachter, Jessica A, and Michael Jacob Kahana. 2019. “A Retrieved-Context Theory of Financial Decisions.” National Bureau of Economic Research.\n\n\nYee, Eiling. 2019. “Abstraction and Concepts: When, How, Where, What and Why?” Taylor & Francis."
  },
  {
    "objectID": "02_Instance_CMR.html#model-architecture",
    "href": "02_Instance_CMR.html#model-architecture",
    "title": "4  Context Maintenance and Retrieval within an Instance-Based Architecture",
    "section": "4.1 Model Architecture",
    "text": "Prototypical CMR stores associations between item feature representations (represented a pattern of weights in an item layer \\(F\\)) and temporal context (represented in a contextual layer \\(C\\)) by integrating prototypical mappings between the representations via Hebbian learning over the course of encoding. In contrast, InstanceCMR tracks the history of interactions between context and item features by storing a discrete record of each experience, even repeated ones, as separate traces within in a memory store for later inspection. Memory for each experience is encoded as a separate row in an \\(m\\) by \\(n\\) memory matrix \\(M\\) where rows correspond to memory traces and columns correspond to features. Each trace representing a pairing \\(i\\) of a presented item’s features \\(f_i\\) and the temporal context of its presentation \\(c_i\\) is encoded as a concatenated vector:\n\\[\nM_i = (f_i, c_i)\n \\qquad(4.3)\\]"
  },
  {
    "objectID": "02_Instance_CMR.html#initial-state",
    "href": "02_Instance_CMR.html#initial-state",
    "title": "4  Context Maintenance and Retrieval within an Instance-Based Architecture",
    "section": "4.2 Initial State",
    "text": "Structuring \\(M\\) as a stack of concatenated item and contextual feature vectors \\((f_i, c_i)\\) makes it possible to define pre-experimental associations between items and contextual states similarly to the pattern by which PrototypeCMR’s pre-experimental associations are specified in equations 3.1 and 3.2. To set pre-experimental associations, a trace is encoded into memory \\(M\\) for each relevant item. Each entry \\(j\\) for each item feature component of pre-experimental memory traces trace \\(f_{pre}(i)\\) is set according to\n\\[\nf_{pre(i, j)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n          0 \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(4.4)\\]\nThis has the effect of relating each unit on \\(F\\) to a unique unit on \\(C\\) during retrieval. As within prototypical CMR, the \\(\\gamma\\) parameter controls the strength of these pre-experimental associations relative to experimental associations.\nSimilarly to control pre-experimental context-to-item associations, the content of each entry \\(j\\) for the contextual component of each pre-experimental trace \\(c_{pre(i,j)}\\) is set by:\n\\[\nc_{pre(i,j)} = \\begin{cases} \\begin{alignedat}{2} \\delta \\text{, if } i=j \\\\\\\n          \\alpha \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(4.5)\\]\nHere, \\(\\delta\\) works similarly to \\(\\gamma\\) to connect indices on \\(C\\) to the corresponding index on \\(F\\) during retrieval from a partial or mixed cue. The \\(\\alpha\\) parameter additionally allows all the items to support one another in the recall competition in a uniform manner.\nBefore list-learning, context \\(C\\) is initialized with a state orthogonal to the pre-experimental context associated with the set of items via the extra index that the representation vector has relative to items’ feature vectors. Following the convention established for prototypical specifications of CMR, item features are further assumed to be orthonormal with respect to one another such that each unique unit on \\(F\\) corresponds to one item."
  },
  {
    "objectID": "02_Instance_CMR.html#encoding-phase",
    "href": "02_Instance_CMR.html#encoding-phase",
    "title": "4  Context Maintenance and Retrieval within an Instance-Based Architecture",
    "section": "4.1 Encoding Phase",
    "text": "In a broad sense, the initial steps of item encoding within InstanceCMR proceed similarly to the process in PrototypeCMR. Just as with PrototypeCMR, when an item \\(i\\) is presented during the study period, its corresponding feature representation \\(f_i\\) is activated on \\(F\\) and its contextual associations encoded into \\(M^{FC}\\) are retrieved by presenting \\(f_i\\) as a probe to memory. InstanceCMR, however, performs retrieval by applying an extension of the basic two-step echo \\(E\\) mechanism outlined in equations 4.1 and 4.2.\nThe extension of the original mechanism differentiates between item- and context-based retrieval. When probes include item feature information (\\(p_f \\neq 0\\)), activation for traces encoded during the experiment are modulated by \\(\\gamma\\) to control the contribution of experimentally-accumulated associations to retrieved representations relative to pre-experimental associations:\n\\[\n(, c^{IN}) = E(f_i, 0) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\gamma} \\times a(f_i, 0)_j \\times M_{jk}\n \\qquad(4.6)\\]\nThe contextual features of the retrieved echo determine contextual input; this retrieved pre-experimental context is normalized to have length 1. Upon retrieval of \\(c^{IN}\\), the current state of context is updated the same way as it is under the prototype-based framework, applying equations 3.4 and 3.5 to drift \\(c\\) toward \\(c^{IN}\\) and enforce its length to 1, respectively.\nAfter context is updated, the current item \\(f_i\\) and the current state of context \\(c_i\\) become associated in memory \\(M\\) by storing a concatenation of the two vectors as a new trace \\((f_i, c_i)\\). This mechanism reserves abstraction over learning episodes for cue-based retrieval rather than at the point of encoding as in PrototypeCMR.\n\n4.1.1 Retrieval Phase\nFollowing the lead of the classic prototype-based implementation of CMR, before retrieval InstanceCMR reinstates some pre-list context according to 3.9. Similarly, at each recall attempt \\(i\\), we calculate the probability of stopping recall (where no item is recalled and search is terminated) based on output position according to 3.11.\nTo determine the probability of recalling an item given that recall does not terminate, first the current state of context is applied as a retrieval cue to retrieve an item feature presentation \\(f_{rec}\\), again applying a modification of the echo-based retrieval mechanism characteristic of instance-based models that modulates trace activations before aggregation into an echo representation:\n\\[\n(f_{rec},) = E(0, c_i) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\phi}_j \\times a(0, c_i)_j \\times M_{jk}\n \\qquad(4.7)\\]\nwhere \\({\\phi}_i\\) scales the amount of learning, simulating increased attention to initial items in a list that has been proposed to explain the primacy effect. \\({\\phi}_i\\) depends on the serial position \\(i\\) of the studied item the same as it does in PrototypeCMR (equation 3.8), with the free parameters \\({\\phi}_s\\) and \\({\\phi}_d\\) respectively controlling the magnitude and decay of the corresponding learning-rate gradient.\nSince item feature representations are presumed to be orthogonal for the purposes of the model, the content of \\(f_{rec}\\) can be interpreted as a measure of the relative support in memory for retrieval of each item \\(i\\), setting the probability distribution of item recalls \\(P(i)\\) to\n\\[\nP(i) = (1-P(stop))\\frac{f_{rec}}{\\sum_{k}^{N}f_{rec}}\n \\qquad(4.8)\\]\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to Eq. 4.6. Context is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\nAn important difference between equation 4.8 and that applied in our specification of PrototypeCMR to compute \\(P(i)\\) (equation 3.12) is that \\(\\tau\\) is not applied as an exponent to retrieval supports to shape the contrast between well-supported and poorly supported items. Instead, instance-based models apply this transformation to discrete trace activations before aggregation of an echo representation. This difference still achieves the effect of ultimately either widening or shrinking the difference between item supports driving retrieval, but is not trivial. Its consequences are explored in later sections.\n\n\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40."
  },
  {
    "objectID": "02_Instance_CMR.html#retrieval-phase",
    "href": "02_Instance_CMR.html#retrieval-phase",
    "title": "4  Context Maintenance and Retrieval within an Instance-Based Architecture",
    "section": "4.4 Retrieval Phase",
    "text": "Following the lead of the classic prototype-based implementation of CMR, before retrieval InstanceCMR reinstates some pre-list context according to 3.9. Similarly, at each recall attempt \\(i\\), we calculate the probability of stopping recall (where no item is recalled and search is terminated) based on output position according to 3.11.\nTo determine the probability of recalling an item given that recall does not terminate, first the current state of context is applied as a retrieval cue to retrieve an item feature presentation \\(f_{rec}\\), again applying a modification of the echo-based retrieval mechanism characteristic of instance-based models that modulates trace activations before aggregation into an echo representation:\n\\[\n(f_{rec},) = E(0, c_i) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\phi}_j \\times a(0, c_i)_j \\times M_{jk}\n \\qquad(4.7)\\]\nwhere \\({\\phi}_i\\) scales the amount of learning, simulating increased attention to initial items in a list that has been proposed to explain the primacy effect. \\({\\phi}_i\\) depends on the serial position \\(i\\) of the studied item the same as it does in PrototypeCMR (equation 3.8), with the free parameters \\({\\phi}_s\\) and \\({\\phi}_d\\) respectively controlling the magnitude and decay of the corresponding learning-rate gradient.\nSince item feature representations are presumed to be orthogonal for the purposes of the model, the content of \\(f_{rec}\\) can be interpreted as a measure of the relative support in memory for retrieval of each item \\(i\\), setting the probability distribution of item recalls \\(P(i)\\) to\n\\[\nP(i) = (1-P(stop))\\frac{f_{rec}}{\\sum_{k}^{N}f_{rec}}\n \\qquad(4.8)\\]\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to Eq. 4.6. Context is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\nAn important difference between equation 4.8 and that applied in our specification of PrototypeCMR to compute \\(P(i)\\) (equation 3.12) is that \\(\\tau\\) is not applied as an exponent to retrieval supports to shape the contrast between well-supported and poorly supported items. Instead, instance-based models apply this transformation to discrete trace activations before aggregation of an echo representation. This difference still achieves the effect of ultimately either widening or shrinking the difference between item supports driving retrieval, but is not trivial. Its consequences are explored in later sections.\n\n\n\n\nHintzman, Douglas L. 1984. “MINERVA 2: A Simulation Model of Human Memory.” Behavior Research Methods, Instruments, & Computers 16 (2): 96–101.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMorton, Neal W, and Sean M Polyn. 2016. “A Predictive Framework for Evaluating Models of Semantic Organization in Free Recall.” Journal of Memory and Language 86: 119–40."
  },
  {
    "objectID": "03_Methods.html#likelihood-based-model-comparison",
    "href": "03_Methods.html#likelihood-based-model-comparison",
    "title": "5  Analysis Approach",
    "section": "5.1 Likelihood-based model comparison",
    "text": "To evaluate how effectively each model accounts for the responses in our datasets, we applied a likelihood-based model comparison technique introduced by Kragel, Morton, and Polyn (2015) that assesses model variants based on how accurately they can predict the specific sequence in which items are recalled. According to this method, repeated items and intrusions (responses naming items not presented in the list) are included from participants’ recall sequences. Given an arbitrary parameter configuration and a sequences of recalls to predict, a model simulates encoding of each item presented in the corresponding study list in its respective order. Then, beginning with the first item the participant recalled in the trial, the probability assigned by the model to the recall event is recorded. Next, the model simulates retrieval of that item, and given its updated state is used to similarly predict the next event in the recall sequence - either retrieval of another item, or termination of recall - and so on until retrieval terminates. The probability that the model assigns to each event in the recall sequence conditional on previous trial events are thus all recorded. These recorded probabilities are then log-transformed and summed to obtain the log-likelihood of the entire sequence. Across an entire dataset containing multiple trials, sequence log-likelihoods can be summed to obtain a log-likelihood of the entire dataset given the model and its parameters. Higher log-likelihoods assigned to datasets by a model correspond to better effectiveness accounting for those datasets."
  },
  {
    "objectID": "03_Methods.html#parameter-optimization",
    "href": "03_Methods.html#parameter-optimization",
    "title": "5  Analysis Approach",
    "section": "5.2 Parameter Optimization",
    "text": "To find the parameter configuration for each model that maximizes its predicted likelihood of observed data, we applied the optimization technique called differential evolution (Storn and Price 1997) as implemented in the Python library scipy. Differential evolution maintains a population of possible parameter configurations; at each update, the algorithm mutates each population member by stochastically mixing them with other members of the population. If the new configuration of a member is an improvement over its previous configuration, then it becomes part of the updated population. Otherwise, the new parameter configuration is discarded. Through repetition of this process, gradually driving the population toward configurations that maximize the log-likelihood of the observed data assigned by the considered model. This maximal log-likelihood and its corresponding parameter configurations form the basis of comparison between models.\nWhen exploring how effectively the model accounts for qualitative benchmark phenomena in free recall performance such as the temporal contiguity and serial position effects, we optimized parameter configurations and evaluated performance across all subjects in the considered dataset, except where otherwise noted. For direct comparison of the log-likelihoods of recall sequences, however, we search for optimal parameters and perform comparison at the subject level, considering distributions of log-likelihood values calculated between subjects when contrasting model versions."
  },
  {
    "objectID": "03_Methods.html#summary-statistics",
    "href": "03_Methods.html#summary-statistics",
    "title": "5  Analysis Approach",
    "section": "5.3 Summary Statistics",
    "text": "In each comparison, we use and visualize a set of summary statistics to characterize the the recall performance of both participants and of each considered model version. To make calculation of these summary statistics with respect to a model possible, we first had to simulate recall sequences using each model. We simulated 1000 unique trials for each unique study list in a considered dataset. For each trial, we simulated encoding of each list item into memory. Next, we simulated free recall according to model specifications outlined above, proceeding stochastically in ecah trial based on the probability distribution computed for each recall attempt until termination. Summary statistics characterizing a model were computed across all relevant simulations.\nOur main analyses focus on the three consistent regularities across experiments reviewed above that have received especial emphasis in accounts of performance on the free recall task. To examine the extent to which datasets and model versions realize the serial position effect, we measured and visualized for each study (serial) position in study lists the rate at which items were retrieved across recall sequences. Relative retrieval rates for early-presented items reflect the magnitude of any primacy effect, while those for items in more terminal study positions measure the recency effect. To measure items retrieved at the initiation of recall across trials, we similarly measured and visualized for each serial position in study lists the rate at which items were retrieved first across each recall sequence.\nWe were similarly interested in the extent to which temporal contiguity where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment – was exhibited across recall sequences in our considered datasets and models. To quantify this pattern, we followed the tradition of applying lag-based condition response probability (lag-CRP) analyses. Here, “lag” refers to the number of positions between two item presentations in a study list. Lag-CRP analyses measure the probability of making a recall transition of a particular positive or negative lag, conditional on transition to recall at that lag being possible. Under high temporal contiguity, recall transitions are largely to items with low lag from the last retrieved item and more rarely to items with high lag. Examining conditional response probabilities as a function of lag thus helps characterize the temporal organization of recall across trials.\n\n\n\n\nKragel, James E, Neal W Morton, and Sean M Polyn. 2015. “Neural Activity in the Medial Temporal Lobe Reveals the Fidelity of Mental Time Travel.” Journal of Neuroscience 35 (7): 2914–26.\n\n\nStorn, Rainer, and Kenneth Price. 1997. “Differential Evolution–a Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.” Journal of Global Optimization 11 (4): 341–59."
  },
  {
    "objectID": "06_Item_Repetitions.html#simulation-of-lohnas-kahana-2014",
    "href": "06_Item_Repetitions.html#simulation-of-lohnas-kahana-2014",
    "title": "8  Repetition Effects",
    "section": "8.1 Simulation of Lohnas & Kahana (2014)",
    "text": "Though the simulations above suggest a way to distinguish between instance- and prototype-based accounts of context maintenance and retrieval, free recall datasets with high amounts of repetition to the extent simulated in the above example do not yet exist. However, to support an initial comparison of how models account for item repetition effects, we use data associated with Siegel and Kahana (2014). Within the dataset, 35 subjects performed delayed free recall of 48 lists over four sessions. Except for deliberate item repetitions, words were unique within each session and did not repeat in successive sessions. The semantic relatedness of words was also controlled below a value of .55 according to WAS (Steyvers, Shiffrin, and Nelson 2005). Across trials, lists were structured in four different ways:\n\nIn control lists, all items were only presented once.\nIn pure massed lists, items were presented twice, always in succession (e.g. 1, 1, 2, 2)\nIn pure spaced lists, items were presented twice with spacing of repetitions from 1 to 8 positions, with each spacing amount equiprobable.\nFinally, mixed lists feature once-presented, massed, and spaced items, with each spacing amount equiprobable\n\n\ncode – 1) load dependencies and data\nfrom compmemlearn.fitting import lohnas_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_data\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport matplotlib.pyplot as plt\nfrom numba.core import types\nfrom numba import njit\nimport seaborn as sns\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nAs with previous analyses, each model variant was fit once for each participant to identify the parameter configuration maximizing the likelihood of observed recall sequences given the considered model, considering all conditions of the dataset. The distribution of data log-likelihoods given each fitted model and participant are plotted in Figure 8.2, with median values for each model variant highlighted. Similarly to previous analyses, these value distributions were found largely similar. The distribution of log-likelihood scores between participants for the PrototypeCMR and InstanceCMR model variants only marginally differ, suggesting that all considered model variants can predict recall sequences even when item repetitions occur during study with similar degrees of success.\n\n\ncode – 2) fit PrototypeCMR participant-by-participant\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nindividual_cmr_results = []\n\nfor subject in np.unique(subjects):\n    \n    print(subject)\n\n    selection = subjects == subject\n\n    cost_function = lohnas_objective_function(\n        trials[selection], \n        presentations[selection],\n        init_cmr,\n        {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n        cmr_free_parameters)\n\n    individual_cmr_results.append(differential_evolution(\n        cost_function, cmr_bounds, disp=False))\n    print(individual_cmr_results[-1].fun)\n\n\n\n\ncode – 2) fit InstanceCMR participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nindividual_icmr_results = []\n\nfor subject in np.unique(subjects):\n    \n    print(subject)\n\n    selection = subjects == subject\n\n    cost_function = lohnas_objective_function(\n        trials[selection], \n        presentations[selection],\n        init_icmr,\n        {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n        icmr_free_parameters)\n\n    individual_icmr_results.append(differential_evolution(\n        cost_function, icmr_bounds, disp=False))\n    print(individual_icmr_results[-1].fun)\n\n\n\n\ncode – 3) plot distribution of log-likelihoods across individual subjects and render summary statistics as table\nplt.style.use('default')\n\nindividual_fits = [result.fun for result in individual_icmr_results] + [result.fun for result in individual_cmr_results]\nlabels = ['InstanceCMR'] * len(individual_icmr_results) + ['PrototypeCMR'] * len(individual_cmr_results)\nindividual_df = pd.DataFrame(individual_fits, index=labels, columns=['Fit']).reset_index()\nindividual_df.columns = ['Model', 'Fit']\n\nsns.set(style=\"darkgrid\")\n\ng = sns.catplot(x='Model', y='Fit', data=individual_df, kind='violin', inner='stick')\nsns.swarmplot(x=\"Model\", y=\"Fit\", color=\"k\", size=3, data=individual_df, ax=g.ax)\ng.ax.set_ylabel('Individual-Level Fitted Model Log-Likelihoods');\nplt.savefig('individual_lohnas2014.pdf', bbox_inches=\"tight\")\nplt.show()\n\nsummary_table = pd.DataFrame(group.describe().rename(columns={'Fit':name}).squeeze()\n            for name, group in individual_df.groupby('Model')).T.to_markdown()\n\n\n\n\n\n\nCode\nfrom IPython.display import display, Markdown\n\ndisplay(Markdown(\"\"\"\n::: {{#fig-lohnasfits layout-ncol=2 layout-valign=\"center\"}}\n\n![](individual_lohnas2014.pdf)\n\n{}\n\nDistribution of log-likelihood scores of recall sequences exhibited by each subject under each considered model across list-lengths [@siegel2014retrieved]\n:::\n\"\"\".format(summary_table)))\n\n\n\n\n\n\n\n\n\n\n\n\nInstanceCMR\nPrototypeCMR\n\n\n\n\ncount\n35\n35\n\n\nmean\n1676.3\n1668.35\n\n\nstd\n434.819\n429.581\n\n\nmin\n914.375\n856.293\n\n\n25%\n1367.75\n1378.27\n\n\n50%\n1686.97\n1698.98\n\n\n75%\n1955.37\n1957.65\n\n\nmax\n2752.36\n2751.88\n\n\n\n\n\nFigure 8.2: Distribution of log-likelihood scores of recall sequences exhibited by each subject under each considered model across list-lengths (Siegel and Kahana 2014)\n\n\nWhile follow-up analysis of summary statistics in previous analyses focused on benchmark phenomena such as serial position effects, inclusion of item repetitions in study designs complicates associated visualizations. Instead, we focused comparison on summary statistics measuring classical item repetition effects. In Figure 8.3, we measure how effectively our prototype- and instance-based CMR implementations account for the spacing effect. Main model variants were fit to the mixed list (fourth) condition of the entire dataset across subjects to optimize the likelihood of observed recall sequences. Then with each configured model, study phases of each trial in the mixed condition of the dataset were simulated and then followed with simulation of free recall. We then plot for both the behavioral data and simulated datasets, the rate at which items were recalled, binned based on the number of intervening items between repetitions. On the one hand, we observe that both models poorly account for the pattern of recall rates observed as a function of presentation spacing in the mixed condition of the Siegel and Kahana (2014) dataset, exaggerating the mnemonic benefit of item repetition in general while understating the mnemonic effect of increased spacing between repetitions. On the other hand, recapitulating all previous analyses, we again found that both our prototype-based and main instance-based implementations of CMR predicted similar patterns of effects of repetition spacing on later item recall.\n\n\ncode – 1) fit CMR to entire dataset\nselection = list_types == 4\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\ncmr_result = differential_evolution(cost_function, cmr_bounds, disp=True)\n\n\ncode – 2) fit InstanceCMR to entire dataset rather than participant-by-participant\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n#    'feature_sensitivity',\n    'context_sensitivity',\n    'delay_drift_rate',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nselection = list_types == 4\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\n\n\ncode – 3) with PrototypeCMR, compose simulated recall_probability_by_lag from overall fitting results\nfrom compmemlearn.analyses import sim_recall_probability_by_lag\n\n\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(cmr_free_parameters):\n    parameters[name] = cmr_result.x[index]\n    \nparameters['sampling_rule'] = 0\nparameters['mfc_familiarity_scale'] = 0\nparameters['mcf_familiarity_scale'] = 0\nparameters['drift_familiarity_scale'] = 0\n\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nselection = list_types == 4\nresult = sim_recall_probability_by_lag(presentations[selection], 10, init_cmr, parameters)\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\n\nax = sns.barplot(x=['N/A', '0', '1-2', '3-5', '6-8'], \n                 y=binned)\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\nplt.show()\n\n\ncode – 4) with InstanceCMR, compose simulated recall_probability_by_lag from overall fitting results\nparameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor index, name in enumerate(icmr_free_parameters):\n    parameters[name] = icmr_result.x[index]\n    \nparameters['sampling_rule'] = 0\nparameters['choice_sensitivity'] = 1\nparameters['feature_sensitivity'] = 1\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\nresult = sim_recall_probability_by_lag(presentations[selection], 10, init_icmr, parameters)\n\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\nax = sns.barplot(x=['N/A', '0', '1-2', '3-5', '6-8'], \n                 y=binned)\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\nplt.show()\n\n\n\n\n\n\n\n\n(a) PrototypeCMR\n\n\n\n\n\n\n\n(b) InstanceCMR\n\n\n\n\n\n\n\n\n\n(c) Data\n\n\n\n\nFigure 8.3: Comparison of predicted recall probability as function of item repetition spacing between each model and the observed data (Siegel and Kahana 2014).\n\n\n\n\n\n\nAnderson, John R, and Gordon H Bower. 1972. “Recognition and Retrieval Processes in Free Recall.” Psychological Review 79 (2): 97.\n\n\nJamieson, Randall K, Johnathan E Avery, Brendan T Johns, and Michael N Jones. 2018. “An Instance Theory of Semantic Memory.” Computational Brain & Behavior 1 (2): 119–36.\n\n\nMurdock, Bennet B, and Ronald Okada. 1970. “Interresponse Times in Single-Trial Free Recall.” Journal of Experimental Psychology 86 (2): 263.\n\n\nSiegel, Lynn L, and Michael J Kahana. 2014. “A Retrieved Context Account of Spacing and Repetition Effects in Free Recall.” Journal of Experimental Psychology: Learning, Memory, and Cognition 40 (3): 755.\n\n\nSteyvers, Mark, Richard M Shiffrin, and Douglas L Nelson. 2005. “Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory.”"
  },
  {
    "objectID": "00_Introduction.html#models-of-free-recall-are-traditionally-prototype-based",
    "href": "00_Introduction.html#models-of-free-recall-are-traditionally-prototype-based",
    "title": "2  Instance and Prototype Accounts of Abstraction",
    "section": "2.1 Models of Free Recall are Traditionally Prototype-Based",
    "text": "While instance-based models have organized formal work in a variety of research subfields, models of memory search primarily focused on accounting for performance on the free recall task largely countervail this pattern. In the free recall task paradigm, research participants are presented a sequence of items — usually a word list — to memorize during a study phase. Later, after a delay or perhaps some distraction task, participants are prompted to recall as many items from the list as possible, in whatever order they come to mind. Since participants largely organize the course of retrieval themselves in the response phase of a free recall task, work by researchers to characterize the organization of responses measured under the paradigm (Postman 1971; Puff 1979) have provided important constraints on accounts of the representations and mechanisms underlying search through memory to retrieve information.\nIn particular, three consistent regularities across experiments have received especial emphasis in accounts of performance on the free recall task (Kahana 2020). The serial position effect identifies a nonlinear, U-shaped relationship between the position of an item within a study list — its serial position — and its probability of retrieval after encoding (Murdock Jr 1962). Researchers typically distinguish between the enhanced retrieval probabilities for early and terminal items; the advantage for the initially presented items is called the primacy effect, while the normally larger advantage for the few presented items is called the recency effect.\nA similar but distinct pattern constraining accounts of memory search is found in analyses relating an item’s serial position with the probability that it will be recalled first in the retrieval phase of experiments. Pivotally, in list-based free recall tasks, participants tend to initiate recall with the most recently studied items from the list; however, in a serial recall task where participants are instructed to recall items in the order in which they were presented rather than freely, participants tend to successfully recall the earliest presented items first (for example in Golomb et al. 2008). This difference implies that while participants maintain and can access memories of item positions to perform a serial recall task, memory search and retrieval is organized by other features of experience.\nPrimacy and recency effects demonstrate that the temporal structure of the list affects the memorability of the items within it. This temporal structure can also be seen in the organization of responses throughout the response sequence, not just for initial and terminal items or recall positions. Free recall task data exhibits a pattern called temporal contiguity where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment. To quantify this pattern, researchers measure across trials the conditional probability of retrieving items given increasing inter-item lags between the serial positions of considered items and the serial position of the item last recalled. These lag-based condition response probability (lag-CRP) analyses find that subjects reliably tend to make transitions between temporally contiguous items (that is, items presented near one another) during free recall. Furthermore, they exhibit a forward bias, recalling contiguous items presented after the last recalled item more frequently than items presented before (Kahana 1996).\nTo account for these phenomena, the formal literature has largely converged on retrieved context theories of memory search (for example, Howard and Kahana 2002; Polyn, Norman, and Kahana 2009; Morton and Polyn 2016). Generally, according to these theories, as items are encoded into a memory system, an internal representational of temporal context is also maintained that dynamically updates itself to reflect a weighted summary of recent experience. As each item is studied, a Hebbian learning mechanism associates the item’s features to the current state of the context representation. Once associated, item features can cue retrieval of associated contextual features, and vice versa. When the retrieval phase comes, the current contextual representation can drive memory search by activating a blend of associated item features. This prompts a retrieval competition in which a particular item is selected and retrieved. Correspondingly, retrieving an item reactivates its associated contextual features, updating context before the next recall attempt. The retrieved context supports the neighbors of the just-recalled item, which gives rise to temporal organization.\nWith these basic mechanisms, retrieved-context models have been used to explain many phenomena, including serial and temporal organizational effects in list-learning tasks (Polyn, Norman, and Kahana 2009; Siegel and Kahana 2014; Schwartz et al. 2005), and broader domains such as financial decision making (Wachter and Kahana 2019), emotion regulation (Talmi, Lohnas, and Daw 2019), and neural signal dynamics within the medial temporal lobe (Kragel, Morton, and Polyn 2015). Further model development has integrated retrieved context accounts of memory search with theories of semantic knowledge (Morton and Polyn 2016) and changes related to healthy aging (Healey and Kahana 2016).\nThe framework used to implement most retrieved context models of memory search acts like a prototype model. These models typically encode memories associating contextual states and item features by updating connection weights within a simplified neural network. Through Hebbian learning, where co-activation of item and contextual features increase weights associating those features, the network accumulates a collapsed average representation reflecting the history of context and item interactions across experience. During retrieval, the network can be probed with a contextual cue to retrieve an item feature representation (or vice versa) based on a linear function of the cue’s content and stored context-to-item weights.\nIn contrast, an instance-based alternative would track this history by storing a discrete record of each experience with its unique temporal context in memory to perform abstraction over only at the point of retrieval. Previous instance-based accounts of performance on various tasks have emphasized a role of some sort of temporal contextual representation in organizing performance. Indeed, the original presentation of MINERVA 2, the first major instance-based memory modeling architecture, included a representation of list context as a feature in stored memory instances to model source-specific frequency judgments from memory (Hintzman 1984). (Lehman and Malmberg 2013) proposed an instance-based buffer model that accounts for patterns like recency and the position position effect in terms of storage and retrieval of traces containing information about item and contextual co-occurrences. Most recently, Logan (2021) introduced the Context Retrieval and Updating (CRU) model, which extends retrieved context theories’ conceptualization of context as a recency-weighted history of previously presented items to account for performance on whole report, serial recall, and copy typing tasks. Nonetheless, it remains unclear whether differences reported in related memory literatures between the performance of prototype- and instance-based memory models might similarly distinguish models of memory search."
  },
  {
    "objectID": "01_Classic_CMR.html#initial-state",
    "href": "01_Classic_CMR.html#initial-state",
    "title": "3  The Prototype-Based Account of Context Maintenance and Retrieval",
    "section": "3.1 Initial State",
    "text": "Associative connections built within prototypeCMR are represented by matrices \\(M^{FC}\\) and \\(M^{CF}\\).\nTo summarize pre-experimental associations built between relevant item features and possible contextual states, we initialize \\(M^{FC}\\) according to:\n\\[\nM^{FC}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n          0 \\text{, if } i \\neq j\n   \\end{alignedat} \\end{cases}\n \\qquad(3.1)\\]\nThis connects each unit on \\(F\\) to a unique unit on \\(C\\). Used this way, \\(\\gamma\\) controls the relative contribution of pre-experimentally acquired associations to the course of retrieval compared to experimentally acquired associations. Correspondingly, context-to-feature associations tracked by \\(M^{CF}\\) are set according to:\n\\[\nM^{CF}_{pre(ij)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\delta \\text{, if } i=j \\\\\\\n          \\alpha \\text{, if } i \\neq j\n       \\end{alignedat} \\end{cases}\n \\qquad(3.2)\\]\nLike \\(\\gamma\\) with respect to \\(M^{FC}\\), the \\(\\delta\\) parameter controls the contribution of pre-experimental context-to-feature associations relative to experimentally acquired ones. Since context-to-feature associations organizes the competition of items for retrieval, the \\(\\alpha\\) parameter specifies a uniform baseline extent to which items support one another in that competition.\nContext is initialized with a state orthogonal to any of those pre-experimentally associated with an relevant item feature. Feature representations corresponding to items are also assumed to be orthonormal with respect to one another such that each unit on \\(F\\) corresponds to one item."
  },
  {
    "objectID": "01_Classic_CMR.html#encoding-phase",
    "href": "01_Classic_CMR.html#encoding-phase",
    "title": "3  The Prototype-Based Account of Context Maintenance and Retrieval",
    "section": "3.2 Encoding Phase",
    "text": "Whenever an item \\(i\\) is presented for study, its corresponding feature representation \\(f_i\\) is activated on \\(F\\) and its contextual associations encoded into \\(M^{FC}\\) are retrieved, altering the current state of context \\(C\\).\nThe input to context is determined by:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.3)\\]\nand normalized to have length 1. Context is updated based on this input according to:\n\\[ \nc_i = \\rho_ic_{i-1} + \\beta_{enc} c_{i}^{IN}\n \\qquad(3.4)\\]\nwith \\(\\beta\\) (for encoding we use \\(\\beta_{enc}\\)) shaping the rate of contextual drift with each new experience, and \\(\\rho\\) enforces the length of \\(c_i\\) to 1 according to:\n\\[ \n\\rho_i = \\sqrt{1 + \\beta^2\\left[\\left(c_{i-1} \\cdot c^{IN}_i\\right)^2 - 1\\right]} - \\beta\\left(c_{i-1} \\cdot\nc^{IN}_i\\right)\n \\qquad(3.5)\\]\nAssociations between each \\(c_i\\) and \\(f_i\\) are built through Hebbian learning:\n\\[\n\\Delta M^{FC}_{exp} = \\gamma c_i f^{'}_i\n \\qquad(3.6)\\]\nand\n\\[\n\\Delta M^{CF}_{exp} = \\phi_i f_i c^{'}_i\n \\qquad(3.7)\\]\nwhere \\(\\phi_i\\) enforces a primacy effect, scales the amount of learning based on the serial position of the studied item according to\n\\[ \n\\phi_i = \\phi_se^{-\\phi_d(i-1)} + 1\n \\qquad(3.8)\\]\nThis function decays over time, such that \\(\\phi_{s}\\) modulates the strength of primacy while \\(\\phi_{d}\\) modulates the rate of decay.\nThis extended Hebbian learning process characterizes how PrototypeCMR performs abstraction. When each item is encoded with a particular temporal context, representations are updated to aggregate a prototypical summary of the item’s temporal contextual associations in \\(M^{FC}\\) and vice versa in \\(M^{CF}\\)."
  },
  {
    "objectID": "01_Classic_CMR.html#retrieval-phase",
    "href": "01_Classic_CMR.html#retrieval-phase",
    "title": "3  The Prototype-Based Account of Context Maintenance and Retrieval",
    "section": "3.3 Retrieval Phase",
    "text": "To help the model account for the primacy effect, we assume that between the encoding and retrieval phase of a task, the content of \\(C\\) has drifted some amoung back toward its pre-experimental state and set the state of context at the start of retrieval according to following, with \\(\\rho\\) calculated as specified above:\n\\[ \nc_{start} = \\rho_{N+1}c_N + \\beta_{start}c_0\n \\qquad(3.9)\\]\nAt each recall attempt, the current state of context is used as a cue to attempt retrieval of some studied item. An activation \\(a\\) is solicited for each item according to:\n\\[ \na = M^{CF}c\n \\qquad(3.10)\\]\nEach item gets a minimum activation of \\(10^{-7}\\). To determine the probability of a given recall event, we first calculate the probability of stopping recall - returning no item and ending memory search. This probability varies as a function of output position \\(j\\):\n\\[\nP(stop, j) = \\theta_se^{j\\theta_r}\n \\qquad(3.11)\\]\nIn this way, \\(\\theta_s\\) and \\(\\theta_r\\) control the scaling and rate of increase of this exponential function. Given that recall is not stopped, the probability \\(P(i)\\) of recalling a given item depends mainly on its activation strength according\n\\[\nP(i) = (1-P(stop))\\frac{a^{\\tau}_i}{\\sum_{k}^{N}a^{\\tau}_k}\n \\qquad(3.12)\\]\n\\(\\tau\\) here shapes the contrast between well-supported and poorly supported items: exponentiating a large activation and a small activation by a large value of \\(\\tau\\) widens the difference between those activations, making recall of the most activated item even more likely. Small values of \\(\\tau\\) can alternatively driven recall likelihoods of differentially activated items toward one another.\nIf an item is recalled, then that item is reactivated on \\(F\\), and its contextual associations retrieved for integration into context again according to:\n\\[\nc^{IN}_{i} = M^{FC}f_{i}\n \\qquad(3.13)\\]\nContext is updated again based on this input (using \\(\\beta_{rec}\\) instead of \\(\\beta_{enc}\\)) and used to cue a successive recall attempt. This process continues until recall stops.\n\n\n\n\nPolyn, Sean M, Kenneth A Norman, and Michael J Kahana. 2009. “A Context Maintenance and Retrieval Model of Organizational Processes in Free Recall.” Psychological Review 116 (1): 129."
  }
]