{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funded-burner",
   "metadata": {},
   "source": [
    "# Parameter Bounds\n",
    "We have to know the model's parameter bounds in order for the model to fit successfully. These notes report\n",
    "what we've found. For model fitting we can start with the bounds array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "lb = np.finfo(float).eps\n",
    "ub = 1-np.finfo(float).eps\n",
    "\n",
    "bounds = [\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, 100),\n",
    "    (lb, 100),\n",
    "    (lb, ub),\n",
    "    (lb, 10),\n",
    "    (lb, 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-counter",
   "metadata": {},
   "source": [
    "where `lb` and `ub` specify default lower and upper bounds, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-lobby",
   "metadata": {},
   "source": [
    "### Drift Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-smell",
   "metadata": {},
   "source": [
    "In past explorations, we found that `drift_rate` parameters cannot exceed values of `1.0` since the `rho`\n",
    "quantity that enforces the length of `context` to 1.0 is `nan`. In practice anyway, `drift_rate` seems to\n",
    "more or less define the extent to which `context_input` defines `context` going forward. At at a `drift_rate`\n",
    "of `1.0`, `context` becomes identical to `context_input`, and at zero, `context` is static. So we should only\n",
    "support parameter values in that range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-webster",
   "metadata": {},
   "source": [
    "#### Potential Problems\n",
    "`rho` doesn't seem to reliably enforce `context` to a length of 1. Instead, `context` slowly grows in length\n",
    "over the course of the experiment. I have noticed this before, but so far it seems unaddressed. This is\n",
    "probably a result of `context_input` having too high a magnitude? No; I normalize `context_input`. The\n",
    "explanation has to lie in how I compute `rho`. My guess is that it's a matter of number precision; nothing I\n",
    "can do, and unlikely to impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-turkey",
   "metadata": {},
   "source": [
    "### Shared Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-burning",
   "metadata": {},
   "source": [
    "This is supposed to encode uniform support items initially have for one another in recall competition. It's a\n",
    "very odd parameter, and I'm considering holding it static to some low value. \n",
    "\n",
    "In the original CMR, the `shared_support` parameter set the nondiagonal values of pre-experimental Mcf. This\n",
    "meant that if an item were arbitrarily presented as a probe to the model, there would be a minimum amount of\n",
    "support for recall of all items, though especially the presented item itself. I already have something ensure\n",
    "recall rates are always above zero for every item for any cue, but why doesn't `shared_support` handle that?\n",
    "Why do I need both?\n",
    "\n",
    "This alone should set a theoretical bound of 1.0 for this parameter since I otherwise never let items support\n",
    "themselves more than that. In theory, too, I imagine I'd also normalize every instance vector to have a\n",
    "length of 1 despite this parameter setting. I'm going to require `shared_support` to be above 0 but below 1.0\n",
    "and work from there, excluding the minimum positivity constant in activations. \n",
    "\n",
    "Shared support can be zero and item support can be zero, but both can't be zero at the same time - probably\n",
    "because it results in zero vectors sometimes? Behavior when one is nonzero and other is `lb` is identical to\n",
    "when one is nonzero and other is zero though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-works",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-drunk",
   "metadata": {},
   "source": [
    "This is the gamma parameter of the other model. It would set the initial diagonal of $M_{fc}$ (always zero\n",
    "otherwise), as well as weight the impact of new experiences on $M_{cf}$'s structure (specifically the\n",
    "importance of new associations between experience features and experience context). The bigger the gamma, the\n",
    "weaker the pre-experimental associations and the stronger the experimental associations. An assertion error\n",
    "happens at $1.0$, presumably because I'm setting the feature half of my instance vectors to $0$ if I do that.\n",
    "At $1-lb$ (lower bound), I have a left-lopsided CRP, which is theory-inconsistent. At $0$, the exact opposite\n",
    "occurs. And there's no bug for setting it that low either. I'll still have a lower bound though.\n",
    "\n",
    "I assume we shouldn't have negative numbers in our instance vectors, anyway, so we'll ceiling the parameter\n",
    "at $1.0-lb$ (so diagonals of Mfc are always $>0$). Similarly, they should not exceed $1.0$, so we'll do the\n",
    "same in the other direction too and floor at zero.\n",
    "\n",
    "Shared support can be zero and item support can be zero, but both can't be zero at the same time - probably\n",
    "because it results in zero vectors sometimes? Behavior when one is nonzero and other is `lb` is identical to\n",
    "when one is nonzero and other is zero though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-glucose",
   "metadata": {},
   "source": [
    "### Item Support Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-baptist",
   "metadata": {},
   "source": [
    "Corresponds to parameter deciding initial strength of the diagonal of Mcf. Thus must range between 0 and 1.\n",
    "When the parameter is near zero, CRPs are symmetrical - but by `lb` it gets flat. And my CRPs themselves seem\n",
    "to have a peak amount of right-weighted CRPs. Can I go above 1? If I do, the CRPs get left-symmetrical. There\n",
    "seems to be no bound to the height, but we're controlling things. \n",
    "\n",
    "Shared support can be zero and item support can be zero, but both can't be zero at the same time - probably\n",
    "because it results in zero vectors sometimes? Behavior when one is nonzero and other is `lb` is identical to\n",
    "when one is nonzero and other is zero though. Just enforce that parameters never set an array entry out of\n",
    "the value 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-attention",
   "metadata": {},
   "source": [
    "### Primacy Scale and Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-twist",
   "metadata": {},
   "source": [
    "Scaling of primacy gradient in learning rate on Mcf. The higher it is, the bigger the difference in impact on\n",
    "memory for early versus later encoded items, multiplying an exponential function. Increasing it increases\n",
    "recall rate for early items. And rate of decay from peak primacy in learning rate on Mcf. The higher it is,\n",
    "the faster the decay of primacy over successive items.\n",
    "\n",
    "Seems unbounded above 0, and with stop_probability parameters codetermine recall rates. Primacy scale\n",
    "certainly strongly controls PFR for first item (stronger support for first item). Primacy decay increases PFR\n",
    "for first item as it increases (less competition from other early items). Together they strongly control how\n",
    "much competition early items get with more recently experienced items. If I really do unbound, there will be\n",
    "fitting issues. But how far do I let them go? Any principled peak?\n",
    "\n",
    "Standard practice is apparently to pick an arbitrary upper bound and increase it if model fitting tends to\n",
    "prefer that bound as a configuration of the parameter. This result suggests that the optimal configuration\n",
    "may exceed the bound, justifying an increase. Setting the upper bound too high, however, can result in\n",
    "exploration of an unnecessarily long parameter space, increasing fitting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "primacy_scale = 14\n",
    "primacy_decay = 2\n",
    "\n",
    "primacy_scale * np.exp(-primacy_decay * np.arange(40)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-terrorist",
   "metadata": {},
   "source": [
    "### Stop probability scale and growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-programmer",
   "metadata": {},
   "source": [
    "When growth is 0, then stop probability is constant for each recall item. And when scale is 0, it's zero. So\n",
    "I should have scale above 0 and probably peaked at 1. Growth is more ambiguous. Certainly floor it at 0. But\n",
    "ceiling? Even with a scale of lb and growth of 2, stop happens long before item 20. Again, we experiment at a\n",
    "probably too high ceiling of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_probability_scale = lb\n",
    "stop_probability_growth = 4\n",
    "recall_total = np.arange(20)\n",
    "stop_probability_scale * np.exp(recall_total * stop_probability_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-secretary",
   "metadata": {},
   "source": [
    "### Choice Sensitivity\n",
    "Activations get exponentiated by this number. Result makes high activations might higher than low\n",
    "activations. Also unbounded, so we'll peak at 10 and empirically increase it, too."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
