{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stable-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp model_fitting\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-packaging",
   "metadata": {},
   "source": [
    "# InstanceCMR Repetitions Benchmark\n",
    "> Pipeline for parameter fitting updated to respect the possibility of item repetition during encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-fisher",
   "metadata": {},
   "source": [
    "## Configuring the Parameter Search\n",
    "This time, we specify a log_likelihood function that accepts a presentations parameter indicating when an item was presented for encoding. This adds some runtime to the cost function, since a unique model is instantiated for each trial given that encoding patterns can vary between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "covered-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from instance_cmr.models import InstanceCMR\n",
    "\n",
    "@njit(fastmath=True, nogil=True)\n",
    "def icmr_rep_likelihood(\n",
    "        trials, presentations, list_types, list_length, encoding_drift_rate, start_drift_rate, \n",
    "        recall_drift_rate, shared_support, item_support, learning_rate, \n",
    "        primacy_scale, primacy_decay, stop_probability_scale, \n",
    "        stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity):\n",
    "    \"\"\"\n",
    "    Generalized cost function for fitting the InstanceCMR model optimized \n",
    "    using the numba library.\n",
    "    \n",
    "    Output scales inversely with the likelihood that the model and specified \n",
    "    parameters would generate the specified trials. For model fitting, is \n",
    "    usually wrapped in another function that fixes and frees parameters for \n",
    "    optimization.\n",
    "\n",
    "    **Arguments**:\n",
    "    - data_to_fit: typed list of int64-arrays where rows identify a unique \n",
    "        trial of responses and columns corresponds to a unique recall index.  \n",
    "    - A configuration for each parameter of `InstanceCMR` as delineated in \n",
    "        `Formal Specification`.\n",
    "\n",
    "    **Returns** the negative sum of log-likelihoods across specified trials \n",
    "    conditional on the specified parameters and the mechanisms of InstanceCMR.\n",
    "    \"\"\"\n",
    "\n",
    "    likelihood = np.ones((len(trials), list_length))\n",
    "    \n",
    "    # we can use the same model for list types 1 and 2\n",
    "    stable_models = [InstanceCMR(\n",
    "            list_length, list_length, encoding_drift_rate, start_drift_rate, \n",
    "            recall_drift_rate, shared_support, item_support, learning_rate, \n",
    "            primacy_scale, primacy_decay, stop_probability_scale, \n",
    "            stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity), \n",
    "                    InstanceCMR(\n",
    "            int(list_length/2), list_length, encoding_drift_rate, start_drift_rate, \n",
    "            recall_drift_rate, shared_support, item_support, learning_rate, \n",
    "            primacy_scale, primacy_decay, stop_probability_scale, \n",
    "            stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity)]\n",
    "    stable_models[0].experience(np.eye(list_length, list_length + 1, 1))\n",
    "    stable_models[1].experience(np.eye(int(list_length/2), int(list_length/2) + 1, 1)[np.repeat(np.arange(int(list_length/2)), 2)])\n",
    "\n",
    "    for trial_index in range(len(trials)):\n",
    "\n",
    "        item_count = np.max(presentations[trial_index])+1\n",
    "        \n",
    "        if list_types[trial_index] > 2:\n",
    "            model = InstanceCMR(\n",
    "                item_count, list_length, encoding_drift_rate, start_drift_rate, \n",
    "                recall_drift_rate, shared_support, item_support, learning_rate, \n",
    "                primacy_scale, primacy_decay, stop_probability_scale, \n",
    "                stop_probability_growth, choice_sensitivity, context_sensitivity, feature_sensitivity)\n",
    "\n",
    "            model.experience(np.eye(item_count, item_count + 1, 1)[presentations[trial_index]])\n",
    "        else:\n",
    "            model = stable_models[list_types[trial_index]-1]\n",
    "            \n",
    "        trial = trials[trial_index]\n",
    "\n",
    "        model.force_recall()\n",
    "        for recall_index in range(len(trial) + 1):\n",
    "\n",
    "            # identify index of item recalled; if zero then recall is over\n",
    "            if recall_index == len(trial) and len(trial) < item_count:\n",
    "                recall = 0\n",
    "            elif trial[recall_index] == 0:\n",
    "                recall = 0\n",
    "            else:\n",
    "                recall = presentations[trial_index][trial[recall_index]-1] + 1\n",
    "\n",
    "            # store probability of and simulate recalling item with this index\n",
    "            activation_cue = np.hstack(\n",
    "                    (np.zeros(model.item_count + 1), model.context))\n",
    "            likelihood[trial_index, recall_index] = \\\n",
    "                model.outcome_probabilities(activation_cue)[recall]\n",
    "\n",
    "            if recall == 0:\n",
    "                break\n",
    "            model.force_recall(recall)\n",
    "\n",
    "        # reset model to its pre-retrieval (but post-encoding) state\n",
    "        model.force_recall(0)\n",
    "\n",
    "    return -np.sum(np.log(likelihood))\n",
    "\n",
    "def icmr_rep_objective_function(data_to_fit, presentations, list_types, list_length, fixed_parameters, free_parameters):\n",
    "    \"\"\"\n",
    "    Generates and returns an objective function for input to support search \n",
    "    through parameter space for ICMR model fit using an optimization function.\n",
    "\n",
    "    Arguments:  \n",
    "    - fixed_parameters: dictionary mapping parameter names to values they'll \n",
    "        be fixed to during search, overloaded by free_parameters if overlap  \n",
    "    - free_parameters: list of strings naming parameters for fit during search  \n",
    "    - data_to_fit: array where rows identify a unique trial of responses and \n",
    "        columns corresponds to a unique recall index\n",
    "\n",
    "    Returns a function that accepts a vector x specifying arbitrary values for \n",
    "    free parameters and returns evaluation of icmr_likelihood using the model \n",
    "    class, all parameters, and provided data.\n",
    "    \"\"\"\n",
    "    return lambda x: icmr_rep_likelihood(data_to_fit, presentations, list_types, list_length, **{**fixed_parameters, **{\n",
    "        free_parameters[i]:x[i] for i in range(len(x))}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-album",
   "metadata": {},
   "source": [
    "## Single-Subject Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-review",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>list</th>\n",
       "      <th>item</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>study</th>\n",
       "      <th>recall</th>\n",
       "      <th>repeat</th>\n",
       "      <th>intrusion</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  list  item  input  output  study  recall  repeat  intrusion  \\\n",
       "0        1     1     0      1     1.0   True    True       0      False   \n",
       "1        1     1     1      2     2.0   True    True       0      False   \n",
       "2        1     1     2      3     3.0   True    True       0      False   \n",
       "3        1     1     3      4     4.0   True    True       0      False   \n",
       "4        1     1     4      5     5.0   True    True       0      False   \n",
       "\n",
       "   condition  \n",
       "0          4  \n",
       "1          4  \n",
       "2          4  \n",
       "3          4  \n",
       "4          4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from instance_cmr.datasets import *\n",
    "\n",
    "trials, events, list_length, presentations, list_types, rep_data, subjects = prepare_repdata(\n",
    "    '../../data/repFR.mat')\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worse-external",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3293.0806120089983"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = np.finfo(float).eps\n",
    "hand_fit_parameters = {\n",
    "    'encoding_drift_rate': .8,\n",
    "    'start_drift_rate': .7,\n",
    "    'recall_drift_rate': .8,\n",
    "    'shared_support': 0.01,\n",
    "    'item_support': 1.0,\n",
    "    'learning_rate': .3,\n",
    "    'primacy_scale': 1,\n",
    "    'primacy_decay': 1,\n",
    "    'stop_probability_scale': 0.01,\n",
    "    'stop_probability_growth': 0.3,\n",
    "    'choice_sensitivity': 2,\n",
    "    'context_sensitivity': 1,\n",
    "    'feature_sensitivity': 1\n",
    "}\n",
    "\n",
    "icmr_rep_likelihood(trials[:48], presentations[:48], list_types[:48], list_length, **hand_fit_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dietary-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 ms ± 645 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "icmr_rep_likelihood(trials[:48], presentations[:48], list_types[:48], list_length, **hand_fit_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-external",
   "metadata": {},
   "source": [
    "```\n",
    "17 ms ± 60.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "```\n",
    "\n",
    "Now we perform the single subject fitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= 3017.03\n",
      "differential_evolution step 2: f(x)= 3017.03\n",
      "differential_evolution step 3: f(x)= 3017.03\n",
      "differential_evolution step 4: f(x)= 3017.03\n",
      "differential_evolution step 5: f(x)= 2684.91\n",
      "differential_evolution step 6: f(x)= 2684.91\n",
      "differential_evolution step 7: f(x)= 2618.1\n",
      "differential_evolution step 8: f(x)= 2618.1\n",
      "differential_evolution step 9: f(x)= 2618.1\n",
      "differential_evolution step 10: f(x)= 2600.4\n",
      "differential_evolution step 11: f(x)= 2595.87\n",
      "differential_evolution step 12: f(x)= 2591.16\n",
      "differential_evolution step 13: f(x)= 2573.26\n",
      "differential_evolution step 14: f(x)= 2573.26\n",
      "differential_evolution step 15: f(x)= 2573.26\n",
      "differential_evolution step 16: f(x)= 2573.26\n",
      "differential_evolution step 17: f(x)= 2545.82\n",
      "differential_evolution step 18: f(x)= 2545.82\n",
      "differential_evolution step 19: f(x)= 2545.82\n",
      "differential_evolution step 20: f(x)= 2519.43\n",
      "differential_evolution step 21: f(x)= 2406.36\n",
      "differential_evolution step 22: f(x)= 2379.63\n",
      "differential_evolution step 23: f(x)= 2379.63\n",
      "differential_evolution step 24: f(x)= 2379.63\n",
      "differential_evolution step 25: f(x)= 2379.63\n",
      "differential_evolution step 26: f(x)= 2379.63\n",
      "differential_evolution step 27: f(x)= 2379.63\n",
      "differential_evolution step 28: f(x)= 2379.63\n",
      "differential_evolution step 29: f(x)= 2379.63\n",
      "differential_evolution step 30: f(x)= 2379.63\n",
      "differential_evolution step 31: f(x)= 2216.03\n",
      "differential_evolution step 32: f(x)= 2216.03\n",
      "differential_evolution step 33: f(x)= 2216.03\n",
      "differential_evolution step 34: f(x)= 2216.03\n",
      "differential_evolution step 35: f(x)= 2190.65\n",
      "differential_evolution step 36: f(x)= 2186.01\n",
      "differential_evolution step 37: f(x)= 2186.01\n",
      "differential_evolution step 38: f(x)= 1997.42\n",
      "differential_evolution step 39: f(x)= 1997.42\n",
      "differential_evolution step 40: f(x)= 1997.42\n",
      "differential_evolution step 41: f(x)= 1997.42\n",
      "differential_evolution step 42: f(x)= 1974.78\n",
      "differential_evolution step 43: f(x)= 1974.78\n",
      "differential_evolution step 44: f(x)= 1974.78\n",
      "differential_evolution step 45: f(x)= 1974.78\n",
      "differential_evolution step 46: f(x)= 1974.78\n",
      "differential_evolution step 47: f(x)= 1948.22\n",
      "differential_evolution step 48: f(x)= 1948.22\n",
      "differential_evolution step 49: f(x)= 1948.22\n",
      "differential_evolution step 50: f(x)= 1948.22\n",
      "differential_evolution step 51: f(x)= 1948.22\n",
      "differential_evolution step 52: f(x)= 1943.92\n",
      "differential_evolution step 53: f(x)= 1943.92\n",
      "differential_evolution step 54: f(x)= 1936.99\n",
      "differential_evolution step 55: f(x)= 1936.99\n",
      "differential_evolution step 56: f(x)= 1925.51\n",
      "differential_evolution step 57: f(x)= 1925.51\n",
      "differential_evolution step 58: f(x)= 1925.51\n",
      "differential_evolution step 59: f(x)= 1925.51\n",
      "differential_evolution step 60: f(x)= 1925.51\n",
      "differential_evolution step 61: f(x)= 1925.51\n",
      "differential_evolution step 62: f(x)= 1923.19\n",
      "differential_evolution step 63: f(x)= 1923.19\n",
      "differential_evolution step 64: f(x)= 1923.19\n",
      "differential_evolution step 65: f(x)= 1921.18\n",
      "differential_evolution step 66: f(x)= 1908.74\n",
      "differential_evolution step 67: f(x)= 1908.74\n",
      "differential_evolution step 68: f(x)= 1908.74\n",
      "differential_evolution step 69: f(x)= 1908.74\n",
      "differential_evolution step 70: f(x)= 1908.74\n",
      "differential_evolution step 71: f(x)= 1908.74\n",
      "differential_evolution step 72: f(x)= 1906.13\n",
      "     fun: 1884.7134857606184\n",
      "     jac: array([ 0.30804585,  0.16957529,  0.25688678, -0.86538421, -1.7370894 ,\n",
      "       -0.10948042, -0.59756076, -0.14570105, -1.99906935, -0.06070877,\n",
      "        0.47696176])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 14841\n",
      "     nit: 72\n",
      " success: True\n",
      "       x: array([7.87073626e-01, 8.72320799e-01, 9.42650668e-01, 9.91137390e-03,\n",
      "       1.00000000e+00, 4.07940705e-01, 1.95434591e+00, 2.85652244e-01,\n",
      "       1.54146162e-03, 2.56544498e-01, 1.91710151e+00])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "import numpy as np\n",
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'context_sensitivity']\n",
    "\n",
    "lb = np.finfo(float).eps\n",
    "ub = 1-np.finfo(float).eps\n",
    "\n",
    "bounds = [\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, 100),\n",
    "    (lb, 100),\n",
    "    (lb, ub),\n",
    "    (lb, 10),\n",
    "    (lb, 10)\n",
    "]\n",
    "\n",
    "# cost function to be minimized\n",
    "# ours scales inversely with the probability that the data could have been \n",
    "# generated using the specified parameters and our model\n",
    "cost_function = icmr_rep_objective_function(\n",
    "    trials[:48], presentations[:48], list_types[:48], list_length, {'choice_sensitivity': 1, 'feature_sensitivity': 1}, free_parameters)\n",
    "\n",
    "result = differential_evolution(cost_function, bounds, disp=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-delight",
   "metadata": {},
   "source": [
    "For the first subject in our murd_trials data structure, the function runs pretty slowly compared to my primary baseline code and returns an output with the following attributes:\n",
    "\n",
    "```\n",
    "     fun: 1896.0488460602892\n",
    "     jac: array([-1.37909932e+02, -6.49760292e+01,  3.80207664e+01,  3.89726438e+01,\n",
    "       -1.08398125e+00,  4.96122311e+01, -7.76151406e+00,  7.50098939e+00,\n",
    "        1.87378446e+03,  5.77529363e+01, -6.19131701e+00])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 7656\n",
    "     nit: 43\n",
    " success: True\n",
    "       x: array([7.38333200e-01, 9.11533659e-01, 9.45503715e-01, 1.59987819e-01,\n",
    "       5.26298280e-01, 3.21809937e-01, 8.64043185e-01, 1.10011958e+00,\n",
    "       1.24608414e-03, 2.67508370e-01, 2.93468878e+00])\n",
    "```\n",
    "\n",
    "The `x` attribute of the result object contains the best parameter configuration found, while the `fun` attribute represents the overall cost of the configuration as computed with our specified cost function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
