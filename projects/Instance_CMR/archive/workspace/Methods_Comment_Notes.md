# Methods Section Comment Notes

**sean.polyn**: 
> Our simulation analyses were designed to determine whether instance-based and prototype-based instantiations of CMR can similarly account for behavioral performance in the free recall task. This includes key benchmark phenomena such as the temporal contiguity and serial position effects, as well as for the overall sequence of responses generated by participants. We used a likelihood-based model comparison technique introduced by \citet{kragel2015neural} that assesses model variants based on how accurately they can predict the specific sequence in which items were recalled. For each model, we related this technique with an optimization technique called differential evolution \citep{storn1997differential} to search for the parameter configuration that maximize the likelihood of the considered data. Likelihoods assigned to datasets by models and their respective optimized parameters in turn support comparison of their effectiveness accounting for the recall sequences exhibited by participants in the data. Visualization of datasets compared to those of simulation outputs given these models with these parameters similarly help compare how well models realize temporal contiguity and serial position effects.
I think you might be able to just integrate this opening paragraph into the subsections below, not sure it is adding value, text is partially redundant

Oh god he's right. 

**You**: How much/which details is appropriate here, I wonder? At minimum I guess I should elaborate about which parameters I use for the differential evolution function and what they mean (e.g. the best1bin strategy's mutation algorithm?)
**sean.polyn**: If there are specific parameters that you adjusted in DE I think you can get away with just saying the name and values of those parameters without having to describe what they do

Okay, I'll do that. Though I mainly used default parameters.

You: I could also add account of AIC and AIC weights here; I already have code for generating them and know the formula. But so far we've found pretty little added value in using them. But reviewers will probably want them?
**sean.polyn**: If we present AIC weights in the results, we should at least mention here that we use them for model comparison. But the text here could simply refer the reader to other references that explain them in more detail, like that Wagenmakers and Farrell 2004 paper.

Okay, good to know reference. Suggests that I should maybe include information here about item repetitions, etc? 

**sean.polyn**: You don't necessarily have to justify each analysis here in the methods. I usually just save the methods for when I'm using a specialized analysis that might be hard for the reader to figure out otherwise.  Like SPC doesn't need justification or explanation.
Basically the analysis should be justified when it appears in the results, with a callback to the methods section if there are important details regarding how it was implemented.

Hmm, I get what he's saying, but I don't know if I agree here. Maybe ask him to elaborate on this point.

## Resolutions
Edits suggested here can probably all be implemented in a single pomodoro; they're all pretty small-scale.