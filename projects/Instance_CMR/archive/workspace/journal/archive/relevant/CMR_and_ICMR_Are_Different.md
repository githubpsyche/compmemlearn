Now swung toward thinking that our version of ICMR and CMR can't be one-to-one equivalent - in principle - because CMR separates F->C and C->F associations in a way I can't really imagine is achievable in anything that looks very much like something of the instance model tradition.

## The Point
We can conceptualize ICMR as an associative network with a set of trace units (we'll label them T) working as a sort of hidden layer mediating the relationship between context units and item units. When F gets activated, trace units get activated which in turn have their own set of connection weights determining context unit activity. If we think of retrieving F->C associations in CMR as relating a 2d Mfc array with active F units, then in ICMR we can say it's a two step process: first relate a 2d Mft array to F to generate trace activations T, and then relate a 2d Mtc array to C to generate a pattern of context unit activations. 

The big consequence of not having a separate Mfc and Mfc memory, though, is that when we do the converse, when we try to retrieve C->F associations, we're using the same two implied structures. If ICMR uses Mft and then Mtc to activate context from F, then context uses those in reverse. Mct and Mtc are the same matrix. Mtf and Mft are the same matrix.

This doesn't mean our ICMR's latent Mfc and Mcf are symmetrical, as so long as they don't always co-activate, activating corresponding c_i and f_i units activates different trace vectors. But it does suggest that for any given configuration of parameters, ICMR's latent Mfc and Mcf (that is, the pattern of contextual input activated when arbitrary F unit is activated) will be more _similar_ than CMR's explicit Mcf and Mfc in some important sense. 

The case that made aware of the consequences of this relationship is the result of updating context after experiencing the first item in a list. At the start of the list, ICMR's memory structure is basically a concatenation of CMR's pre-experimental Mfc and Mcf. So if we focus on just the feature unit activated when encoding list item 0, CMR's Mfc is 1-gamma for one unit in the relevant vector and 0 otherwise. CMf's Mcf however is delta for one unit in the relevant vector and alpha otherwise. Like ICMR, only one vector in memory is relevant when ICMR encodes its first item, and we can say corresponding things about the distribution of its F->T and C->T associations reflected in that vector. But since ICMR uses F->T->C associations to derive contextual input from activated F and its C->T associations are non-zero for all elements, first trial contextual input affects _all_ units in C by some small amount. No such dynamic arises in CMR - updated context after the first trial only changes two units in C.

And of course, whether we're talking about CMR or ICMR, the state of context after experiencing a new item determines how memory is updated. The entire course of encoding and then retrieval is shaped by this dynamic!

## What Now?
Should we accept this dynamic or try to formulate a version of ICMR that avoids it? The crudest way to avoid it is to specify a version of CMR that just operates exactly the same as the original, except _also_ tracking a representation of each experience along with its usual structures. This CMR+I model can exclusively wield those traces during retrieval, activating and selecting between memory traces instead of items and then updating context based on what's retrieved. This effectively makes Mcf causally inert, and would allow us to test in a more constrained way whether traces instead of prototypical representations are retrieved during memory search. 

However, if we want a model that looks like MINERVA 2 to work as our Mcf and Mfc, I think we're stuck with something in our current implementation's general neighborhood. Even if we were to use separate instance-based formulations of Mcf and Mfc, I have struggled to imagine a simple way to make them work just like CMR's structures.

Despite these issues, we still pretty decidedly already have an instance-based version of CMR on our hands. The model seems able to flexibly specify human-like SPCs, CRPs, and PFRs, and I haven't found a dataset yet that returns worse costs for ICMR than my implementation of classic CMR. At the same time, the model is also undeniably of the same mold as MINERVA 2. The mechanism specified by Hintzman in 1982 to build and to probe memory from arbitrary cues is the same mechanism we use to update and probe memory, whether when deciding how to update context after every new experience or retrieval, when forming new associations between F and C, and when deciding item retrieval. Any instance-based model that encodes item sequences can be adapted to support these mechanisms too by similarly appending and operating on corresponding contextual representations for each trace, something we can infer mostly from feature information in relevant trace vectors and the structure of the encoding.

Still, if the behavior of the model can't be characterized as simply as "the same Mfc and Mcf are formed latently during retrieval via a weighted sum of coactivated memory traces", then it's a little harder to use ICMR to set up examination of a really specific research question like "do humans use instance-based or averaged-over representations of feature-to-context associations to perform retrieval". In our version of ICMR, a little more is going on than that. The CMR+I specification I describe above might be better suited for asking that question in a controlled way. On the other hand, our version of ICMR probably _is_ what you get if you try to suppose that people use instance-based representations to do both context maintenance and retrieval.

Maybe CMR+I is actually useful for building up to ICMR, an increment to evince (or discount) the use of instance-based representations for memory search. A corresponding model that uses instance-based context maintainence but prototype-based retrieval is possible, too, I suppose. It can be specified by similarly building ICMR's M and CMR's Mfc and Mcf simultaneously. However, dynamics in ICMR's M will determine the state of context when retrieval starts, while CMR's structures will determine item retrieval probabilities that themselves help determine the cue for next recall. But ICMR can already be flexibly modified to use an averaged-over representation to cue successive recalls, so the model would be less interesting.

## What Will We Compare ICMR Against?
Since we can't show that ICMR and CMR correspond in principle, we will instead have to do formal model comparison to show that ICMR is around at least as good as CMR on traditional list memory datasets, using the same number of parameters. If we don't find that, that's a tougher pill to swallow. We either change the model or have to write a paper saying that our particular implementation of ICMR doesn't work. Let Sean choose the datasets.

But in traditional list-memory experiments where each item occurs just once, ICMR isn't necessarily interesting by itself. It's not very different at all from CMR, which I've spent a long time showing. ICMR's charm aside from how flexibly it can be cued is supposed to be the way it maintains representations of all experiences. 

ITS uses those extra representations to enable access to subordinate senses of words.  "This prototype representation conflates multiple meanings and senses of words into a center of tendency, often losing the subordinate senses of a word in favor of more frequent ones." How does that work? What really is the difference between abstraction-at-learning and abstraction-at-retrieval models of word meaning when it comes to accessing subordinate word senses? And ITS doesn't use trace selection to support these features; so why do we?

ITS uses selective cueing to pick out subordinate representations of individual words. The product of activations from using a word with those for a cue of its subordinate meaning selectively activates traces that use both while still somewhat activating traces that use one or the other. ICMR can be similarly cued to selectively activate particular traces - it'd would just be feature and context representations that form the cue. But how would that fit into CMR's specification? A similar compound cue during retrieval?

When retrieval happens, the echo contains feature and contextual information. The feature representation will be a blend based on traces' corresponding contextual features. 

## Other Quotes

This one seems to outline how connectionist networks can be conceptualized as abstraction-at-learning models.
> The newest additions to the DSM family are predictive neural embedding models, which use a connectionist architecture and error-driven backpropagation to learn a distributed vector pattern for a word’s meaning. The current frontrunner is the Word2Vec model of Mikolov et al. (2013). Word2Vec is a three-layer connectionist network with localist input and output layers (i.e., with one node for each word in the vocabulary), fully connected via a hidden layer of 300 nodes. Each context, a target output word is predicted by using the other co-occurring words as input. The error signal is backpropagated through the network to increase the probability of the network predicting the correct output word given the same input words in future epochs. After many learning epochs, the network settles, and the matrix of hidden-to-output connections is exported as a semantic representation. In this matrix, words are similar if they are predicted by similar contexts. Hence, Word2Vec produces a similar outcome to both LSA and BEAGLE by collapsing a word’s episodic contexts into a single reduced representation of meaning. All three models accomplish the same task of producing a prototype for a word’s meaning, albeit by different learning mechanisms. To be clear, Word2Vec is a multilayer network; hence, it can predict the same output word (e.g., bark) given very different input patterns (e.g., bark in the tree sense versus bark in the dog sense). But the final representation for a word is the complete pattern of weights—the prototype. Two words are typically compared via the cosine of their respective vectors in all models, and so a homonym such as bark will be pulled between its two senses just as it is in LSA or BEAGLE.

## Add to To-Do List
Plot how parameter modification affects shape of graph. Overlap of result of modifying parameter.

Use a more recent dataset than Murdoch - maybe from lab? Pierce2016, experiment 3. (exp 1 is good too?) of 2016 paper
Not the Sederberg one - had some quirks. Though could show model handles distraction okay.

Look into semantics. Can't get a sharp recency effect - either SPC, PFR, CRP - when feature vectors are nonorthogonal. Can use Glove or whatever to demonstrate.

Capture the sharp effect w/o overpredicting semantic information. Predicting both temporal and semantic organization simultaneously is the problem.

## 
