# March 3, 2021

## What's Up?
Because of my misconception that ICMR and CMR are performatively equivalent, emphasis lately on the passageFR project, and ambiguity about the right direction to take evaluation of the models, we are a few weeks behind on the planned timeline for ICMR. Let's clarify how we'll get back on the right track.

## Notes From Past Journal Entries

### Clean_Optimized_ICMR
This was when I first applied jitclass to organize the numba-optimized version of ICMR, and probably reflects my most direct intuitions about performance and whatnot. Importantly, with an optimized compute_likelihood and InstanceCMR, I obtained runtimes of 11.2 +/- 224 microseconds per loop. I need to check whether I still have that performance now, and potentially whether it can be improved on even further.

### CMR and ICMR are Different
I think my analysis focuses on how ICMR has dependencies between its latent Mcf and Mfc that don't exist between CMR's. 

The idea was that, for any given configuration of parameters, ICMR's latent Mfc and Mcf (that is, the pattern of contextual input activated when arbitrary F unit is activated) will be more _similar_ than CMR's explicit Mcf and Mfc in some important sense. 

What are the consequences of that? Still somewhat ambiguous. But will have to be explained in a final paper.

Another thing the entry brings up is the possibility of analyzing an intermediate CMR+I model that allows for instance-based retrieval but otherwise maintains CMR's mechanisms and structures: the only difference is that a specific memory instance is retrieved rather than the whole shebang. 

CMR+I is probably false, and similarly ICMR might also be false if retrieval occurred for traces instead of over echoes. But it could still be a worthwhile comparison.

Finally, I start to wonder if there is anything interesting about ICMR or not: a principled theoretical distinction between CMR or ICMR that would have behavioral consequences instead of just, say, slowing down model training. 

## Relation to Jamieson_2018?

### Nonlinear Activation of Stored Instances
I think I've figured out the big reason CMR needs item feature representations to be orthogonal. It seems to be closely related to why ITS handles polysemous words better than DSMs.

As Jamieson et al (2018) reiterate a lot, an integration-at-retrieval model can produce nonlinear activation of stored instances, while integration-at-encoding models (like CMR) cannot. CMR seems to sidestep this problem, though, with two commitments:

1. **It makes feature unit activations corresponding to each item orthogonal**. The result is that each index of the activation vector generated during retrieval corresponds directly to support for a particular item/experience/memory trace. 
2. **When transforming activations into probabilities, it applies a sensitivity parameter that nonlinearly scales the contrast between well-supported and poorly supported items.** The result is that CMR effectively also produces nonlinear activation of stored instances. 

While item feature representations are orthogonal, this sensitivity scaling step has the same consequence whether you perform it before or after doing your sum of trace vectors. So you can do abstraction-at-encoding while still enjoying the dividends of activating traces nonlinearly based on cue similarity.

As you break down the one-to-one correspondence between each feature unit and and some specific item/experience, the correspondence between outcomes of nonlinear scaling before and after abstraction also breakdown. Taking the cube of the first entry of your activation vector in CMR is no longer equivalent to scaling the activation of a particular trace based on cue similarity. 

Instead, learning can be distorted in this scaling step as feature information in experiences only weakly associated with the current contextual cue will be enhanced exponentially while original weightings encoded based on trace similarity only apply linearly.

So, summing up. I said this was closely related to the ITS/DSM distinction, but it's not quite the same critique. In the Jamieson 2018 paper, nonlinear activation of traces in the context of DSMs is considered impossible. In CMR, nonlinear activation is achieved via softmax, but does not weight support effectively if feature units don't identify items (that is, if item representations aren't orthogonal). By performing nonlinear activation over traces instead of over the collapsed activation vector, ICMR can have non-orthogonal item representations _without_ any breakdown in learning. If my reasoning is correct.

### Handling Repetitions
The difference between applying a nonlinear transformation before and after integration is a difference between predicting an exponential scaling of retrieval support when an item gets repeated and predicting a linear increase, between (1+2+3+...)^3 and (1)^3 + (2)^3 + (3)^3 + .... Because of the way context evolves, we can't test that prediction directly, but it's something that will color less pure cases of item repetition as well.

<!--
Here, "meaning" corresponds to the company a word keeps, and a model of semantic memory is thus evaluable in terms of how capably it can retrieve contextual information about any cue. In that sense, the tasks of episodic and semantic memory are equally concerned with context. Successful semantic memory depends on tracking contextual features that occur consistently across experiences of an arbitrary item. Episodic memory is concerned rather with tracking item features associated with an arbitrary configuration of contextual features. In the context of ITS, this distinction isn't real except to the extent that episodic memory generally entails retrieval of item information specific enough to characterize a single memory trace.
-->

## So What Now?
We have a well-motivated purpose now, and a decent timeline. I want all core analyses done by mid March - the seventeenth. That probably means pushing back PCIT and slow-rolling passageFR.

This week, I'm limited to optimizing the model, configuring relevant datasets, and performing initial simulations. If I'm lucky I can start outlining the paper, but it's not the top priority; first I need results that will themselves clarify the direction of the paper.

Next week, I need fits to real data sets.

The following week, we cope with the ramifications.

Okay doing tomorrow. Tonight then, narrative stuff plus course stuff.

```python

```
