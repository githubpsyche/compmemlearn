{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "productive-rapid",
   "metadata": {},
   "source": [
    "# Progress Week 4/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-exploration",
   "metadata": {},
   "source": [
    "We're now clearly less than a month away from my seminar presentation and committee meeting, so it's important to start finishing up sections of my talk and paper. Along with making steady progress on my remaining analyses, I also need to hurry out a draft of these important materials for further editting, potentially even making that draft my top priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-mailman",
   "metadata": {},
   "source": [
    "## Talk/Paper Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-charger",
   "metadata": {},
   "source": [
    "### Tentative Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-utilization",
   "metadata": {},
   "source": [
    "0. **Introduction, extending the abstract.** Basic conflict: instance-based models have grown increasingly influential in other areas of memory science, but when it comes to modeling free recall, the high profile models such as CMR and its variants are generally connectionist prototype-based models. In this talk, I present an instance-based implementation of CMR and use it as a site to explore the consequences of applying an instance-based model architecture to account for memory search.\n",
    "1. **Explain the free recall paradigm.** What is it? Why do people do it? What are the core phenomena to be accounted for (SPC, CRP, PFR).\n",
    "2. **Explain the context maintenance and retrieval model**. We'll use this as leverage to introduce our own model. Make sure it's clear how it works mechanistically, and also explain how model fitting works and what its consequences are.\n",
    "3. **Introduce InstanceCMR**. Motivate it. Outline its mechanisms. Convince listeners of its broad equivalence with CMR, theoretically and then empirically. \n",
    "4. **Distinguish InstanceCMR from CMR**. Focus on nonlinear activation of traces as a defining feature of InstanceCMR that CMR can't mimic when presentations aren't representationally orthogonal. Specify experimental conditions where this distinction might result in divergent predictions about behavior. \n",
    "5. **Item Repetitions**. A shared mechanism in CMR and ICMR is application of a sensitivity parameter to nonlinearly determine the contrast between well-supported and poorly supported responses during recall. Critically, however, instance-based models apply this transformation to trace activations while CMR and other hypothetical prototype models that lack persistent memory trace representations can only enforce selective retrieval in this way on retrieved representations that integrate across representations. This distinction results in distinct predictions about the consequences of repeatedly encoding items for later retrieval. Here I would explain the distinction conceptually, then by simulation, then present the Lohnas dataset, then explore the consequences for recall.\n",
    "6. **Non-orthogonal contextual representations**. I'm not sure yet if this is a thing.\n",
    "7. **Non-orthogonal item representations**. If I have time. When \"echo\" feature units don't identify unique items, CMR's retrieval process breaks down in a way that InstanceCMR can avoid.\n",
    "8. **Integrating with other models**. I'd love to have one example (e.g. ITS) of how ICMR can be integrated with other instance-based theories to model phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-raleigh",
   "metadata": {},
   "source": [
    "### Is this enough for a full paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-election",
   "metadata": {},
   "source": [
    "Suppose I don't finish the latter two analyses. Is this enough for a full paper? Is this enough for a full talk? I'm dissatisfied with the idea of only tackling repetitions, however deeply. But I also don't know if I have the time to generate much else.\n",
    "\n",
    "Let's compare to those two Jones' papers. Introduction was a bit more than two pages. A bit more than two pages for model specification (with a big figure). Simulation analysis were 3 pages, had 3 unique analysis results (two figures). 6 pages for empirical analyses, including 5 sections/sets of analyses and 5 figures. Then a general discussion a bit under two pages. 15 pages, 8 figures.\n",
    "\n",
    "What have I got? Suppose I keep the introduction at the same size (2 pages) and that a group of SPCs/PFRs/CRPs are one analysis. I have two models to specify and visualize (2 figures, 4 pages). I'll likely also devote a whole section (2 pages) and set of figures (e.g. M to Mcf) to convincing readers of equivalence by simulation (1 figure). Then empirical simulations (3 pages): fits to single list length dataset (1/2 figures) as well as fits to a different dataset with varying list lengths (1/2 figures). Then a section introducing repetitions as a dissociating condition with simulations to back up (1 figure, 2 pages). Then another couple pages for empirical simulations and fits (1/2 figure, 2 pages). \n",
    "\n",
    "It's similar in terms of content, but the multi-list length thing feels like padding. I should try to elaborate further on the nonlinear activation of traces thing or on the model flexibility thing. On the other hand, there are plenty of patterns in the Lohnas paper that I can reproduce. Idk idk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-details",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-photograph",
   "metadata": {},
   "source": [
    "Overleaf lets me edit and clone within the repository. Marp, etc, enable easy version control for document-based presentations. I'll try to make my notebooks as paper-like as possible and regularly export content to slide decks and papers within latex. To start (i.e. before my meeting this week), I'll add my initial outline to the repository and my presentation and my paper draft and fill in with the pieces I already have ready from my notebook.\n",
    "\n",
    "`index.ipynb` should mainly have the Abstract and a repository-specific tour of where everything is. But perhaps before my reference or features section I can include what would be the content of my paper introduction. Which is what? How long can I make an introduction before describing MINERVA 2 or CMR in detail? I can explain the task and the main phenomena driving accounts of the task. Then I can provide an overview of how retrieved context models account for these phenomena and then echo Jamieson et al (2018)'s critique of prototype-based models and sermon on the merits of architecture unification. All this motivates development and analysis of instance_cmr, a model I also summarize in this section.\n",
    "\n",
    "Then in my model specification section, I start by introducing CMR and then MINERVA 2, then explaining how ICMR integrates both. Three different visualizations, at minimum.\n",
    "\n",
    "Then in the next section, I focus on proving both models can account for free recall similarly well. Present mechanistic comparison and then empirical one on two datasets, with some chi-squared test. This means concretizing my model comparison notebook as I've been planning for a while.\n",
    "\n",
    "The following sections can focus on the argument that nonlinear activation of traces distinguishes instance-based models from prototype models, not just in theory but practically: in terms of the predictions they support. First I drive these predictions with a theoretical analysis. Then by analyzing model performance with respect to the Siegel dataset, and perhaps others, if I have time. \n",
    "\n",
    "Notebook Sequence:\n",
    "- `index.ipynb`. Have to add deeper background information, including a review of main task and core phenomena and overview of both the retrieved context account thereof and the architectural debate I hope to weigh in surrounding it.\n",
    "- `CMR.ipynb`. Have to expand into a full specification. Potentially pull together multiple versions. And connect more clearly to background/introduction. Need a figure explaining how it works, too.\n",
    "- `Classic Instance Model.ipynb`. Same as above.\n",
    "- `InstanceCMR.ipynb`. Same as above. Need to much more thoroughly relate it to above two models.\n",
    "- `Model_Comparison.ipynb`. Need to build into cohesive analysis of how models performance largely equivalently on the classic free recall paradigm. Take existing fitting and add quantitative analysis, better figures.\n",
    "- `Repetitions.ipynb`. Yup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-assault",
   "metadata": {},
   "source": [
    "## What's Left From Equivalence Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-belfast",
   "metadata": {},
   "source": [
    "- **Add fitting with another dataset**. One of the Morton and Polyn, 2016, datasets).\n",
    "- **Quantitative fit comparison**. AIC can do the work; MortPolyn2016 models how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-assignment",
   "metadata": {},
   "source": [
    "## What's Left For Repetition Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-brooklyn",
   "metadata": {},
   "source": [
    "- Rewrite Fitting Function\n",
    "- Rewrite Visualization Function\n",
    "- Generate Fits\n",
    "- Follow-Up Analyses on Fitted Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-diesel",
   "metadata": {},
   "source": [
    "## How Do I Want to Rewrite Repetitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-helping",
   "metadata": {},
   "source": [
    "I want different versions of instance_cmr (and perhaps CMR) that give me more precise control over the model mechanisms under consideration. I need to be able to rule out that the differences I might be observing in my fits aren't the result of, say, eliminating use of the sensitivity parameter when updating content instead of the more limited result I'm actually interested in.\n",
    "\n",
    "I also want fitting to be faster. I have some reforms in mind that might address that concern.\n",
    "\n",
    "I also want to rewrite my use of `visualize_fit` such that unique sets of graphs are generated for each kind of trial? Is that possible? Is that worthwhile? I'm not sure, I'm not sure.\n",
    "\n",
    "I think that more broadly I wish I had different versions of ICMR and CMR specialized for different purposes. CMR and ICMR are now in a state that I can't recommend it to other people without a lot of preparatory background. That's lame and a situation I've always wanted to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-conditions",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-orange",
   "metadata": {},
   "source": [
    "For somewhere around a century and a half now, much ado has focused on accounting for how people encode and retrieve sequences of discrete items - word lists and whatnot. Hermann Ebbinghaus, quite a major late 19th century pioneer in the field, \n",
    "\n",
    "In the free recall version of this task, participants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-philip",
   "metadata": {},
   "source": [
    "In computational memory science, much ado has focused on accounting for how people encode and retrieve sequences of discrete items - word lists and whatnot. We can look at least as far back as the late 1800s at the work of Hermann Ebbinghaus for examples of this research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "miniature-buddy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\beta_b\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\beta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-coordinator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
