{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp homonym_experiment\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-amino",
   "metadata": {},
   "source": [
    "# Reproducing the Classic Instance-Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-titanium",
   "metadata": {},
   "source": [
    "We start with the classic instance-based model. To get acquainted with the MINERVA 2\n",
    "multiple-trace model of episodic memory, I implemented the model in the Python programming language. I also\n",
    "reproduced a recent simulation (Jamieson, Avery, Johns, & Jones, 2018) studying the model's capacity to\n",
    "represent homonyms from a artificial language. The reasoning was that if my simulation obtained the same\n",
    "outcomes as those reported in the paper, I could feel confident that my model itself faithfully reproduced\n",
    "the one described in the paper. This would work as a firm foundation for more novel work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-excess",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export models\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class ExemplarModel:\n",
    "    \"\"\"\n",
    "    The basic exemplar model of memory as originated by Hintzman (1984, 1986, 1988) in MINERVA 2.\n",
    "\n",
    "    In the model, every `experience` is represented as a vector - an ordered list of feature values along many\n",
    "    dimensions. A record of each experience - called a `trace` is stored as a new, separate row in a m x n\n",
    "    `memory` matrix where rows correspond to memory traces and columns correspond to feature dimensions.\n",
    "\n",
    "    To retrieve information from memory, a feature vector can be presented as a `probe`. The probe activates\n",
    "    all traces in memory in parallel. Each trace's `activation` is a cubed function of its `similarity` to the\n",
    "    probe. The sum of these traces weighted by their activation represents an `echo` summarizing the memory\n",
    "    system's response to the probe. The content and intensity of this echo can serve downstream behavior such\n",
    "    as recognition or word sense disambiguation or (hopefully) free recall. For example, to compare memory\n",
    "    representations associated with two probes, the model can compute the resemblance (cosine similarity)\n",
    "    between the echoes associated with probes A and B.\n",
    "\n",
    "    Attributes:\n",
    "    - memory: array where rows correspond to accumulated memory traces and columns correspond to feature dims\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiences=None):\n",
    "        \"\"\"Inits exemplar model with initial set of experiences in memory (if any).\"\"\"\n",
    "        self.memory = None\n",
    "        if experiences:\n",
    "            self.experience(experiences)\n",
    "\n",
    "    def experience(self, experiences):\n",
    "        \"\"\"Adds new experience(s) to model memory, represented as new row(s) in the model's memory array.\"\"\"\n",
    "        self.memory = np.vstack((self.memory, np.array(experiences))) if self.memory else np.array(experiences)\n",
    "\n",
    "    def probe(self, probe):\n",
    "        \"\"\"\n",
    "        Presents a cue to memory system, fetching echo reflecting its pattern of activation across traces.\n",
    "        The probe activates all traces in memory in parallel. Each trace's `activation` is a cubed function of\n",
    "        its `similarity` to the probe. The sum of these traces weighted by their activation is an `echo`\n",
    "        summarizing the memory system's response to the probe.\n",
    "        \"\"\"\n",
    "\n",
    "        # computes and cubes similarity value to find activation for each trace in memory\n",
    "        activation = np.power(\n",
    "            np.sum(self.memory * probe, axis=1) / (norm(self.memory, axis=1) * norm(probe)), 3)\n",
    "\n",
    "        # multiply each trace by its associated activation and take a column-wise sum to retrieve echo\n",
    "        echo = np.sum((self.memory.T * activation).T, axis=0)\n",
    "        return echo\n",
    "\n",
    "    def compare_probes(self, first_probe, second_probe):\n",
    "        \"\"\"Compute the resemblance (cosine similarity) between the echoes associated with probes A and B.\"\"\"\n",
    "        echoes = self.probe(first_probe), self.probe(second_probe)\n",
    "        return np.sum(echoes[0] * echoes[1]) / (norm(echoes[0]) * norm(echoes[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(ExemplarModel)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(ExemplarModel.experience)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(ExemplarModel.probe)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(ExemplarModel.compare_probes)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-coalition",
   "metadata": {},
   "source": [
    "## Homonym Experiment\n",
    "We then applied this implementation of the classic model to reproduce some results from **Experiment 1:\n",
    "Homonyms in an Artificial Language** of Jamieson et al (2018). The paper MINERVA 2 as a model of semantic\n",
    "memory called ITS (Instance Theory of Semantics Memory), and considers whether it can track contextual\n",
    "differences in word sense. For example, in some contexts \"break\" can have a similar meaning to the word\n",
    "\"report\" (e.g. \"break the news\") while in other contexts \"break\" can have a similar meaning to the word\n",
    "\"smash\" (e.g. \"broke the toy\"). ITS records a neutral record of word use in the corpus and develops a\n",
    "representation of word meaning on-the-fly by parallel and probe-driven retrieval. This enables it to create\n",
    "unique representations of the meaning of even same-spelled words that occur in different kinds of contexts. \n",
    "\n",
    "We start by building an artificial language corpus enforcing meaningful statistical regularities for our\n",
    "model to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def artificial_language_corpus():\n",
    "    \"\"\"\n",
    "    Generate a corpus of experiences to serve examination of ExemplarModel\n",
    "\n",
    "    `words` are represented as a unique vector where each dimension takes a randomly sampled value from a\n",
    "    normal distribution with mean zero and variance 1/n. Experiences are encoded as the sum of the word vectors\n",
    "    occurring in a given context.\n",
    "\n",
    "    A simple artificial language is constructed to generate a corpus of experiences (specified in Table 1 of\n",
    "    paper), consisting of 12 words sorted between 7 lexical categories and 3 sentence frames grammatically\n",
    "    specifying how triplets of words from different categories can be associated within a verbal context.\n",
    "\n",
    "    To explore whether an exemplar model can predict human judgements even in the case of homonyms - words with\n",
    "    the same spelling/pronunciation but different meanings - 20,000 grammatical sentences are sampled from the\n",
    "    artificial language and encoded as experiences for an ExemplarModel instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # random vectors for each word\n",
    "    word_list = ['man', 'woman', 'car', 'truck', 'plate', 'glass', 'story',\n",
    "                'news', 'stop', 'smash', 'report', 'break']\n",
    "    word_vectors = {word: np.random.normal(0, np.sqrt(1 / 20000), 20000) for word in word_list}\n",
    "\n",
    "    # categories of lexical items\n",
    "    lexical_items = {\n",
    "        'NOUN_HUMAN': ['man', 'woman'],\n",
    "        'NOUN_VEHICLE': ['car', 'truck'],\n",
    "        'NOUN_DINNERWARE': ['plate', 'glass'],\n",
    "        'NOUN_NEWS': ['story', 'news'],\n",
    "        'VERB_VEHICLE': ['stop', 'break'],\n",
    "        'VERB_DINNERWARE': ['smash', 'break'],\n",
    "        'VERB_NEWS': ['report', 'break'],\n",
    "    }\n",
    "\n",
    "    # sample sentences and experiences\n",
    "    sentences, experiences = [], []\n",
    "    frames = np.random.choice([1, 2, 3], 20000)\n",
    "    for i in range(20000):\n",
    "        if frames[i] == 1:\n",
    "            sentence = [np.random.choice(lexical_items['NOUN_HUMAN']),\n",
    "                        np.random.choice(lexical_items['VERB_VEHICLE']),\n",
    "                        np.random.choice(lexical_items['NOUN_VEHICLE'])]\n",
    "        elif frames[i] == 2:\n",
    "            sentence = [np.random.choice(lexical_items['NOUN_HUMAN']),\n",
    "                        np.random.choice(lexical_items['VERB_DINNERWARE']),\n",
    "                        np.random.choice(lexical_items['NOUN_DINNERWARE'])]\n",
    "        else:\n",
    "            sentence = [np.random.choice(lexical_items['NOUN_HUMAN']),\n",
    "                        np.random.choice(lexical_items['VERB_NEWS']),\n",
    "                        np.random.choice(lexical_items['NOUN_NEWS'])]\n",
    "\n",
    "        sentences.append(sentence)\n",
    "        experiences.append(np.sum([word_vectors[word] for word in sentence], axis=0))\n",
    "\n",
    "    return {'word_list': word_list, 'word_vectors': word_vectors, 'lexical_items': lexical_items,\n",
    "            'sentences': sentences, 'experiences': experiences}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-french",
   "metadata": {},
   "source": [
    "Using a model encoding this corpus, we confirm that words that occur in similar contexts have similar\n",
    "meanings by comparing their echoes against their co-occurrence frequencies in a contextual similarity\n",
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def contextual_similarity_experiment(model):\n",
    "    \"\"\"\n",
    "    Generates similarity matrix visualization testing if words in similar contexts have similar echoes.\n",
    "\n",
    "    We compute and visualize a pairwise similarity matrix comparing echoes associated with each unique word to\n",
    "    one another. Items in the same lexical categories (e.g. 'story' and 'news') occur in similar contexts and\n",
    "    so should have similar echoes. Items in opposing lexical categories (e.g. 'story' and 'smash') should be\n",
    "    found dissimilar. Items that occur equally often in every context ('break') should be somewhere in the\n",
    "    middle.\n",
    "    \"\"\"\n",
    "\n",
    "    # initiate model with corpus of word contexts as experiences\n",
    "    corpus = artificial_language_corpus()\n",
    "    model = model(corpus['experiences'])\n",
    "\n",
    "    # compute pairwise similarities for each word in list\n",
    "    similarities = np.full((len(corpus['word_list']), len(corpus['word_list'])), np.nan)\n",
    "    for x in tqdm(range(len(corpus['word_list']))):\n",
    "        for y in range(len(corpus['word_list'])):\n",
    "            if x == y:\n",
    "                continue\n",
    "            word, other_word = corpus['word_list'][x], corpus['word_list'][y]\n",
    "            similarities[x, y] = model.compare_probes(\n",
    "                corpus['word_vectors'][word], corpus['word_vectors'][other_word])\n",
    "    sns.heatmap(similarities, xticklabels=corpus['word_list'], yticklabels=corpus['word_list'],\n",
    "                annot=True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "    plt.savefig('figures/contextual_similarity_experiment.png')\n",
    "    return corpus, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-introduction",
   "metadata": {},
   "source": [
    "### Result\n",
    "With these functions, we apply the simulation to our implementation of the classic exemplar model. If the\n",
    "simulation has worked out, the monogamous words from the vehicle topic are clustered together (i.e., stop,\n",
    "car, truck), the monogamous words from the dinnerware topic are clustered together (i.e., plate, glass,\n",
    "smash), the monogamous words from the news topic are clustered together (i.e., story, news, report), and the\n",
    "promiscuous nouns (i.e., man, woman) are not so clustered with one another but highly clustered with other\n",
    "terms while the promiscuous verb (break) is highly associated with every noun but not with other verbs.\n",
    "\n",
    "These would correspond with the outcomes reported in the literature, and confirm that we can treat this\n",
    "implementation of the MINERVA 2 model as a basis for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "corpus, similarities = contextual_similarity_experiment(ExemplarModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-windsor",
   "metadata": {},
   "source": [
    "![](figures/contextual_similarity_experiment.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
