{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifty-typing",
   "metadata": {},
   "source": [
    "# Item Repetitions\n",
    "> Is there a meaningful difference between how CMR and ICMR track the effect of repeated item presentations on later retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-johnson",
   "metadata": {},
   "source": [
    "There seems to be substantive differences in the way CMR and ICMR apply a sensitivity parameter to nonlinearly increase the gap between highly supported and less supported recall outcomes. InstanceCMR, like MINERVA 2, has the option to apply its sensitivity scaling operation (i.e. an exponent to each value) to activations of individual traces - that is, before integration into a unitary vector tracking retrieval support. CMR, on the other hand, has no access to any trace activations and thus applies its sensitity scaling to the integrated result. \n",
    "\n",
    "The latter operation is hypothesized to more strongly differentiate strongly from weakly supported items than the former. Suppose a constant sensitivity parameter $\\tau$ and that two distinct experiences each contributed a support of $c$ for a given feature unit in the current recall. Under trace-based sensitivity scaling, the retrieval support for that feature unit would be $c^{\\tau} + c^{\\tau}$. But under echo-based sensitity scaling, support would be ${(c + c)}^{\\tau}$, a much larger quantity. \n",
    "\n",
    "When feature representations associated with each presentation encoded into models are all orthogonal, this distinction doesn't emerge. Since each trace corresponds to activation of a unique feature unit, no equivalent of the $c^{\\tau} + c^{\\tau}$ operation from the above example ever emerges. But experiments where items are presented repeatedly offer an opportunity to explore this distinction. It could be that parameter fitting avoids any of the main consequences for this mechanism difference, much as InstanceCMR and CMR learn similar retrieval dynamics despite implementing distinct associative networks with distinct learning algorithms. And alternatively it could be that more subtle manipulations, such as the use of partially overlapping rather than identical-but-otherwise-orthogonal feature representations, could explore this mechanistic distinction more thoughtfully. We'll see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-matthew",
   "metadata": {},
   "source": [
    "## Preliminary Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-development",
   "metadata": {},
   "source": [
    "One way to initiate analysis of how the models might differentially handle item repetitions is to measure how the probability of recalling an item given an arbitrary contextual cue changes as the item is repeatedly encoded. \n",
    "\n",
    "\n",
    "We expect it to increase linearly (by $x^{\\tau}$) with respect to `InstanceCMR`, but exponentially within `CMR`. If this reasoning survives scrutiny, that's a big deal.\n",
    "\n",
    "Parameters, contextual cue, and repeated item shouldn't matter here. We'll try to set them to values that aren't distracting and vary them to ensure our simulation results aren't contingent on any particular configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
