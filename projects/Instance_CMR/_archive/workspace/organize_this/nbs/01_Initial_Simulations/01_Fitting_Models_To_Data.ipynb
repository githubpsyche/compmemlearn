{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp model_analysis\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-timothy",
   "metadata": {},
   "source": [
    "# Fitting Models to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-liver",
   "metadata": {},
   "source": [
    "A key step for evaluating any cognitive model is to find and measure model fit to real behavioral datasets.\n",
    "This process requires a few new sections of code. These include:\n",
    "\n",
    "1. A `prepare_data` function that loads data from a provided path and formats it to support both efficient\n",
    "   model fitting and data visualization.\n",
    "2. A `likelihood` function that computes the likelihood of the data given a model and a specified parameter\n",
    "   configuration.\n",
    "3. A `likelihood_search` function that searches for the parameter configuration of a specified model that\n",
    "   makes the provided data the most likely.\n",
    "4. A `visualize_fit` function that visually compares a selected dataset with a model fitted to it using\n",
    "   figures such as the serial position and conditional recall probability curve.\n",
    "5. We may also need to try optimizing `models.InstanceCMR` to help speed up fitting, which can be a\n",
    "   pretty time-consuming process!\n",
    "\n",
    "We will use this code to fit our updated model to data sourced for a classical 1962 study reporting the\n",
    "serial position effect of free recall, cited below. To confirm that our discovered parameter configuration\n",
    "really works, we'll visualize the resulting fit. We'll also add some tests confirming the robustness of the\n",
    "fitting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-young",
   "metadata": {},
   "source": [
    "> Murdock, B. B., Jr. (1962). The serial position effect of free recall. Journal of Experimental Psychology,\n",
    "> 64(5), 482-488. https://doi.org/10.1037/h0045106  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-liechtenstein",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-barrier",
   "metadata": {},
   "source": [
    "To decide on how we'll make a function for data preparation, we'll consider the dataset under current interest at\n",
    "`data/MurdData_clean.mat` and hope that other relevant datasets will have a similar structure. The code is thus\n",
    "tentative, since using other datasets or using the current dataset for other purposes may require updating or\n",
    "expanding this code.\n",
    "\n",
    "It has three `LL` structures that each seem to correspond to a different data set with different list lengths.  Inside\n",
    "each structure is:\n",
    "- `recalls` with 1200 rows and 50 columns. Each row presumably represents a subject, and each column seems to\n",
    "  correspond to a recall position, with -1 coded for intrusions. `MurdData_clean.mat` probably doesn't have these\n",
    "  intrusions coded at all.\n",
    "- `listlength` is an integer indicating how long the studied list is.\n",
    "- `subject` is a 1200x1 vector coding the identities of each subject for each row. Each subject seems to get 80 rows a\n",
    "  piece. He really got that much data for each subject? \n",
    "- `session` similarly codes the index of the session under consideration, and it's always 1 in this case.\n",
    "- `presitemnumbers` probably codes the number associated with each item. Is just its presentation index.\n",
    "\n",
    "At first, we will only work with on `LL` structure at a time.\n",
    "\n",
    "We need a few structures extracted from a selected dataset.\n",
    "1. A `trials` array, where each row identifies a unique trial of responses and each call corresponds to a unique\n",
    "   recall index. Entries are 0 where no item is recalled, -1 where an inapplicable item is recalled, while other\n",
    "   entries are 1-indexed based on the order in which they were presented in the list.\n",
    "2. A \"long' format table representing recall data as specified in the [`psifr`\n",
    "   documentation](https://psifr.readthedocs.io/en/latest/guide/import.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from psifr import fr\n",
    "\n",
    "def prepare_murddata(path, dataset_index):\n",
    "    \"\"\"\n",
    "    Prepares data formatted like `data/MurdData_clean.mat` for fitting.\n",
    "\n",
    "    Loads data from `path` with same format as `data/MurdData_clean.mat` and returns a selected dataset as an array of\n",
    "    unique recall trials and a dataframe of unique study and recall events organized according to `psifr`\n",
    "    specifications.\n",
    "\n",
    "    **Arguments**:  \n",
    "    - path: source of data file  \n",
    "    - dataset_index: index of the dataset to be extracted from the file\n",
    "\n",
    "    **Returns**:\n",
    "    - trials: int64-array where rows identify a unique trial of responses and columns corresponds to a unique recall\n",
    "        index.  \n",
    "    - merged: as a long format table where each row describes one study or recall event.  \n",
    "    - list_length: length of lists studied in the considered dataset\n",
    "    \"\"\"\n",
    "    # load all the data\n",
    "    matfile = sio.loadmat(path, squeeze_me=True)\n",
    "    murd_data = [matfile['data'].item()[0][i].item() for i in range(3)]\n",
    "    \n",
    "    # encode dataset into psifr format\n",
    "    trials, list_length, subjects = murd_data[dataset_index][:3]\n",
    "    trials = trials.astype('int64')\n",
    "    \n",
    "    data = []\n",
    "    for trial_index, trial in enumerate(trials):\n",
    "\n",
    "        # every time the subject changes, reset list_index\n",
    "        if not data or data[-1][0] != subjects[trial_index]:\n",
    "            list_index = 0\n",
    "        list_index += 1\n",
    "\n",
    "        # add study events\n",
    "        for i in range(list_length):\n",
    "            data += [[subjects[trial_index], \n",
    "                      list_index, 'study', i+1, i+1]]\n",
    "\n",
    "        # add recall events\n",
    "        for recall_index, recall_event in enumerate(trial):\n",
    "            if recall_event != 0:\n",
    "                data += [[subjects[trial_index], list_index, \n",
    "                          'recall', recall_index+1, recall_event]]\n",
    "\n",
    "    data = pd.DataFrame(data, columns=[\n",
    "        'subject', 'list', 'trial_type', 'position', 'item'])\n",
    "    merged = fr.merge_free_recall(data)\n",
    "    return trials, merged, list_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(prepare_murddata, title_level=3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-manhattan",
   "metadata": {},
   "source": [
    "We can generate a quick preview of some datasets using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "murd_trials, murd_events, murd_length = prepare_murddata('../data/MurdData_clean.mat', 0)\n",
    "\n",
    "murd_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-jumping",
   "metadata": {},
   "source": [
    "|    |   subject |   list |   item |   input |   output | study   | recall   |   repeat | intrusion   |\n",
    "|---:|----------:|-------:|-------:|--------:|---------:|:--------|:---------|---------:|:------------|\n",
    "|  0 |         1 |      1 |      1 |       1 |        5 | True    | True     |        0 | False       |\n",
    "|  1 |         1 |      1 |      2 |       2 |        7 | True    | True     |        0 | False       |\n",
    "|  2 |         1 |      1 |      3 |       3 |      nan | True    | False    |        0 | False       |\n",
    "|  3 |         1 |      1 |      4 |       4 |      nan | True    | False    |        0 | False       |\n",
    "|  4 |         1 |      1 |      5 |       5 |      nan | True    | False    |        0 | False       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-season",
   "metadata": {},
   "source": [
    "## Configuring the Parameter Search\n",
    "To fit the model to some dataset, we select a cost function that scales against the likelihood that the model\n",
    "with a specified parameter configuration could have generated the specified dataset. Here we'll use those built just for InstanceCMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from instance_cmr.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(icmr_likelihood, title_level=3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-silly",
   "metadata": {},
   "source": [
    "We return a sum of negative log likelihoods because our fitting functions search for parameters that minimize\n",
    "error functions and log-likelihoods are negative. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lb = np.finfo(float).eps\n",
    "hand_fit_parameters = {\n",
    "    'item_count': murd_length,\n",
    "    'encoding_drift_rate': .8,\n",
    "    'start_drift_rate': .7,\n",
    "    'recall_drift_rate': .8,\n",
    "    'shared_support': 0.01,\n",
    "    'item_support': 1.0,\n",
    "    'learning_rate': .3,\n",
    "    'primacy_scale': 1,\n",
    "    'primacy_decay': 1,\n",
    "    'stop_probability_scale': 0.01,\n",
    "    'stop_probability_growth': 0.3,\n",
    "    'choice_sensitivity': 2\n",
    "}\n",
    "icmr_likelihood(murd_trials[:60], **hand_fit_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-identifier",
   "metadata": {},
   "source": [
    "returns `1154.6050515722645`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-broad",
   "metadata": {},
   "source": [
    "When it comes to model fitting, we will normally wrap this loss function within another that fixes function\n",
    "parameters we aren't seeking a fit for - for example, our data structure `trials`. In general, our true\n",
    "objective function should fill free parameters of `icmr_likelihood` using values in a single `x` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(icmr_objective_function, title_level=3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-runner",
   "metadata": {},
   "source": [
    "We can generate and apply the function to compute loss for different configurations of just\n",
    "`encoding_drift_rate` with code like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = icmr_objective_function(murd_trials[:60], hand_fit_parameters, ['encoding_drift_rate'], )\n",
    "\n",
    "cost_function([.8]), cost_function([.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-decision",
   "metadata": {},
   "source": [
    "Which returns (again) `1154.6050515722645`, and `1338.7420002341266`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-rachel",
   "metadata": {},
   "source": [
    "From here, the function searching the parameter space to find model configurations that fit as well to data\n",
    "as possible is mostly written for us. We use the `differential_evolution` function from `scipy.optimize`. The\n",
    "entire function specification can be found in [the corresponding\n",
    "docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html), but\n",
    "its main requirements are a cost function to be minimized and a `bounds` array specifying in each row a (min,\n",
    "max) pair constraining search over each parameter. With the cost function generated above, we can find the\n",
    "best fitting value of `encoding_drift_rate` to `murd_trials[:60]` for our `InstanceCMR` class very\n",
    "efficiently after specifying a bounds array that constrains search between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "result = differential_evolution(cost_function, [(np.finfo(float).eps, 1-np.finfo(float).eps)], disp=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-excess",
   "metadata": {},
   "source": [
    "This function returns an output with the following attributes:\n",
    "\n",
    "```\n",
    "     fun: 1153.963638767822\n",
    "     jac: array([0.])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 57\n",
    "     nit: 2\n",
    " success: True\n",
    "       x: array([0.81981498])\n",
    "```\n",
    "\n",
    "The `x` attribute of the result object contains the best parameter configuration found, while the `fun`\n",
    "attribute represents the overall cost of the configuration as computed with our specified cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-literature",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can visually compare the behavior of the model with these parameters against the data it's fitted to with a new\n",
    "`visualize_fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_fit(model_class, parameters, data, data_query=None, experiment_count=1000, savefig=False):\n",
    "    \"\"\"\n",
    "    Apply organizational analyses to visually compare the behavior of the model with these parameters against\n",
    "    specified dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate simulation data from model\n",
    "    model = model_class(**parameters)\n",
    "    try:\n",
    "        model.experience(np.eye(model.item_count, model.item_count + 1, 1))\n",
    "    except ValueError:\n",
    "        model.experience(np.eye(model.item_count, model.item_count))\n",
    "    sim = []\n",
    "    for experiment in range(experiment_count):\n",
    "        sim += [[experiment, 0, 'study', i + 1, i] for i in range(model.item_count)]\n",
    "    for experiment in range(experiment_count):\n",
    "        sim += [[experiment, 0, 'recall', i + 1, o] for i, o in enumerate(model.free_recall())]\n",
    "    sim = pd.DataFrame(sim, columns=['subject', 'list', 'trial_type', 'position', 'item'])\n",
    "    sim_data = fr.merge_free_recall(sim)\n",
    "    \n",
    "    # generate simulation-based spc, pnr, lag_crp\n",
    "    sim_spc = fr.spc(sim_data).reset_index()\n",
    "    sim_pfr = fr.pnr(sim_data).query('output <= 1') .reset_index()\n",
    "    sim_lag_crp = fr.lag_crp(sim_data).reset_index()\n",
    "    \n",
    "    # generate data-based spc, pnr, lag_crp\n",
    "    data_spc = fr.spc(data).query(data_query).reset_index()\n",
    "    data_pfr = fr.pnr(data).query('output <= 1').query(data_query).reset_index()\n",
    "    data_lag_crp = fr.lag_crp(data).query(data_query).reset_index()\n",
    "    \n",
    "    # combine representations\n",
    "    data_spc['Source'] = 'Data'\n",
    "    sim_spc['Source'] = model_class.__name__\n",
    "    combined_spc = pd.concat([data_spc, sim_spc], axis=0)\n",
    "    \n",
    "    data_pfr['Source'] = 'Data'\n",
    "    sim_pfr['Source'] = model_class.__name__\n",
    "    combined_pfr = pd.concat([data_pfr, sim_pfr], axis=0)\n",
    "    \n",
    "    data_lag_crp['Source'] = 'Data'\n",
    "    sim_lag_crp['Source'] = model_class.__name__\n",
    "    combined_lag_crp = pd.concat([data_lag_crp, sim_lag_crp], axis=0)\n",
    "    \n",
    "    # generate plots of result\n",
    "    # spc\n",
    "    g = sns.FacetGrid(dropna=False, data=combined_spc)\n",
    "    g.map_dataframe(sns.lineplot, x='input', y='recall', hue='Source')\n",
    "    g.set_xlabels('Serial position')\n",
    "    g.set_ylabels('Recall probability')\n",
    "    plt.title('P(Recall) by Serial Position Curve')\n",
    "    g.add_legend()\n",
    "    g.set(ylim=(0, 1))\n",
    "    if savefig:\n",
    "        plt.savefig('figures/{}_fit_spc.jpeg'.format(model_class.__name__), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    #pdf\n",
    "    h = sns.FacetGrid(dropna=False, data=combined_pfr)\n",
    "    h.map_dataframe(sns.lineplot, x='input', y='prob', hue='Source')\n",
    "    h.set_xlabels('Serial position')\n",
    "    h.set_ylabels('Probability of First Recall')\n",
    "    plt.title('P(First Recall) by Serial Position')\n",
    "    h.add_legend()\n",
    "    h.set(ylim=(0, 1))\n",
    "    if savefig:\n",
    "        plt.savefig('figures/{}_fit_pfr.jpeg'.format(model_class.__name__), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # lag crp\n",
    "    max_lag = 5\n",
    "    filt_neg = f'{-max_lag} <= lag < 0'\n",
    "    filt_pos = f'0 < lag <= {max_lag}'\n",
    "    i = sns.FacetGrid(dropna=False, data=combined_lag_crp)\n",
    "    i.map_dataframe(\n",
    "        lambda data, **kws: sns.lineplot(data=data.query(filt_neg),\n",
    "                                         x='lag', y='prob', hue='Source', **kws))\n",
    "    i.map_dataframe(\n",
    "        lambda data, **kws: sns.lineplot(data=data.query(filt_pos),\n",
    "                                         x='lag', y='prob', hue='Source', **kws))\n",
    "    i.set_xlabels('Lag')\n",
    "    i.set_ylabels('Recall Probability')\n",
    "    plt.title('Recall Probability by Item Lag')\n",
    "    i.add_legend()\n",
    "    i.set(ylim=(0, 1))\n",
    "    if savefig:\n",
    "        plt.savefig('figures/{}_fit_crp.jpeg'.format(model_class.__name__), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_doc(visualize_fit, title_level=3)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-treasurer",
   "metadata": {},
   "source": [
    "With the function we can plot the model with the resulting parameter configuration against the actual data in\n",
    "one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_fit(InstanceCMR, {**hand_fit_parameters, **{'encoding_drift_rate': result.x[0]}}, murd_events, 'subject == 1', experiment_count=1000, savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-anger",
   "metadata": {},
   "source": [
    "Which results in figures like these:\n",
    "\n",
    "![](figures/fit_spc.jpeg)\n",
    "\n",
    "![](figures/fit_pfr.jpeg)\n",
    "\n",
    "![](figures/fit_crp.jpeg)\n",
    "\n",
    "These fits can be enhanced by freeing all model parameter for optimization, but the process takes too long to\n",
    "occur in the context of this notebook."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
