{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recorded-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp model_fitting\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-gasoline",
   "metadata": {},
   "source": [
    "# InstanceCMR - Single List Length\n",
    "Now that relevant dependencies are specified and testing, we'll jump right into fitting the model to larger portions of the dataset. This time, we'll do the entire 20-item list length subset of the Murdock (1962) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-ukraine",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "french-blackberry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>list</th>\n",
       "      <th>item</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>study</th>\n",
       "      <th>recall</th>\n",
       "      <th>repeat</th>\n",
       "      <th>intrusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  list  item  input  output  study  recall  repeat  intrusion\n",
       "0        1     1     1      1     5.0   True    True       0      False\n",
       "1        1     1     2      2     7.0   True    True       0      False\n",
       "2        1     1     3      3     NaN   True   False       0      False\n",
       "3        1     1     4      4     NaN   True   False       0      False\n",
       "4        1     1     5      5     NaN   True   False       0      False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from instance_cmr.datasets import prepare_murddata\n",
    "from instance_cmr.model_fitting import icmr_murd_likelihood\n",
    "from instance_cmr.model_fitting import icmr_murd_objective_function\n",
    "from instance_cmr.model_fitting import visualize_fit\n",
    "from instance_cmr.models import InstanceCMR\n",
    "\n",
    "murd_trials0, murd_events0, murd_length0 = prepare_murddata(\n",
    "    '../../data/MurdData_clean.mat', 0)\n",
    "\n",
    "murd_events0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-arlington",
   "metadata": {},
   "source": [
    "## Free Echo-Based Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'choice_sensitivity']\n",
    "\n",
    "lb = np.finfo(float).eps\n",
    "ub = 1-np.finfo(float).eps\n",
    "\n",
    "bounds = [\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, 100),\n",
    "    (lb, 100),\n",
    "    (lb, ub),\n",
    "    (lb, 10),\n",
    "    (lb, 10)\n",
    "]\n",
    "\n",
    "# cost function to be minimized\n",
    "# ours scales inversely with the probability that the data could have been \n",
    "# generated using the specified parameters and our model\n",
    "cost_function = icmr_murd_objective_function(\n",
    "    List([murd_trials0]), \n",
    "    {'item_counts': List([murd_length0]), 'context_sensitivity': 1, 'feature_sensitivity': 1}, \n",
    "    free_parameters)\n",
    "\n",
    "result = differential_evolution(cost_function, bounds, disp=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-harmony",
   "metadata": {},
   "source": [
    "## Results\n",
    "```\n",
    "     fun: 21079.43935271875\n",
    "     jac: array([  0.07748895,   3.22615961,  -0.71813701,  -0.68321242,\n",
    "        -1.79716152,   0.49112714,  -0.78653095,   0.        ,\n",
    "       -32.81020326, -16.64593584,  -0.38271537])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 12081\n",
    "     nit: 60\n",
    " success: True\n",
    "       x: array([7.35396287e-01, 1.87003498e-02, 9.02468034e-01, 4.02430122e-03,\n",
    "       1.00000000e+00, 1.10964824e-01, 1.01153545e+01, 7.00452937e+01,\n",
    "       3.45770082e-02, 2.57315752e-01, 1.27029929e+00])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'item_count':murd_length0,\n",
    "    'presentation_count': murd_length0,\n",
    "    'context_sensitivity': 1,\n",
    "    'feature_sensitivity': 1\n",
    "}\n",
    "\n",
    "visualize_fit(InstanceCMR, \n",
    "              {**parameters, \n",
    "               **{free_parameters[i]:result.x[i] for i in range(len(result.x))}}, \n",
    "              murd_events0, 'subject > -1', experiment_count=1000, savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-riding",
   "metadata": {},
   "source": [
    "## Free C-F Trace-Based Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'feature_sensitivity']\n",
    "\n",
    "lb = np.finfo(float).eps\n",
    "ub = 1-np.finfo(float).eps\n",
    "\n",
    "bounds = [\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, 100),\n",
    "    (lb, 100),\n",
    "    (lb, ub),\n",
    "    (lb, 10),\n",
    "    (lb, 10)\n",
    "]\n",
    "\n",
    "# cost function to be minimized\n",
    "# ours scales inversely with the probability that the data could have been \n",
    "# generated using the specified parameters and our model\n",
    "cost_function = icmr_murd_objective_function(\n",
    "    List([murd_trials0]), \n",
    "    {'item_counts': List([murd_length0]), 'context_sensitivity': 1, 'choice_sensitivity': 1}, \n",
    "    free_parameters)\n",
    "\n",
    "result = differential_evolution(cost_function, bounds, disp=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-emperor",
   "metadata": {},
   "source": [
    "## Results\n",
    "```\n",
    "     fun: 21073.071051444465\n",
    "     jac: array([ 6.78082866, 15.58983058,  2.69428709, 20.38614184, -3.04062267,\n",
    "       -1.71930879, -0.10331859,  0.        , -7.69286998, -7.72306521,\n",
    "        1.26165104])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 16443\n",
    "     nit: 86\n",
    " success: True\n",
    "       x: array([7.90931647e-01, 7.87233205e-03, 9.48041056e-01, 8.09406715e-04,\n",
    "       8.11152335e-01, 6.66612122e-02, 2.08401190e+01, 8.51443053e+01,\n",
    "       2.86499753e-02, 2.92199457e-01, 8.44628821e-01])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'item_count':murd_length0,\n",
    "    'presentation_count': murd_length0,\n",
    "    'context_sensitivity': 1,\n",
    "    'choice_sensitivity': 1\n",
    "}\n",
    "\n",
    "visualize_fit(InstanceCMR, \n",
    "              {**parameters, \n",
    "               **{free_parameters[i]:result.x[i] for i in range(len(result.x))}}, \n",
    "              murd_events0, 'subject > -1', experiment_count=1000, savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-massage",
   "metadata": {},
   "source": [
    "## Single-Parameter Free Trace-Based Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-characteristic",
   "metadata": {},
   "source": [
    "We want to make sure any difference between the trace-based and echo-based sensitivity parameters aren't because the former is responsible for both C->F and F->C transformations. Currently, when feature_sensitivity is held to 1.0, context_sensitivity organizes trace-based retrieval in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "\n",
    "free_parameters = [\n",
    "    'encoding_drift_rate',\n",
    "    'start_drift_rate',\n",
    "    'recall_drift_rate',\n",
    "    'shared_support',\n",
    "    'item_support',\n",
    "    'learning_rate',\n",
    "    'primacy_scale',\n",
    "    'primacy_decay',\n",
    "    'stop_probability_scale',\n",
    "    'stop_probability_growth',\n",
    "    'context_sensitivity']\n",
    "\n",
    "lb = np.finfo(float).eps\n",
    "ub = 1-np.finfo(float).eps\n",
    "\n",
    "bounds = [\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, ub),\n",
    "    (lb, 100),\n",
    "    (lb, 100),\n",
    "    (lb, ub),\n",
    "    (lb, 10),\n",
    "    (lb, 10)\n",
    "]\n",
    "\n",
    "# cost function to be minimized\n",
    "# ours scales inversely with the probability that the data could have been \n",
    "# generated using the specified parameters and our model\n",
    "cost_function = icmr_murd_objective_function(\n",
    "    List([murd_trials0]), \n",
    "    {'item_counts': List([murd_length0]), 'feature_sensitivity': 1, 'choice_sensitivity': 1}, \n",
    "    free_parameters)\n",
    "\n",
    "result = differential_evolution(cost_function, bounds, disp=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-dragon",
   "metadata": {},
   "source": [
    "## Results\n",
    "```\n",
    "     fun: 21094.09681817264\n",
    "     jac: array([ 12.23052089, -21.23088052,  12.16540107,   9.48493835,\n",
    "        -4.64242476,  -7.4684067 ,  -3.32292986,   0.        ,\n",
    "       -22.60858309,  11.04235708,   1.48029359])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 14109\n",
    "     nit: 64\n",
    " success: True\n",
    "       x: array([7.50643542e-01, 1.42483068e-02, 8.99035619e-01, 9.60870298e-04,\n",
    "       2.30959697e-01, 9.61676960e-02, 7.80989503e+00, 6.29317080e+01,\n",
    "       3.44289419e-02, 2.57667853e-01, 1.20133645e+00])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'item_count':murd_length0,\n",
    "    'presentation_count': murd_length0,\n",
    "    'feature_sensitivity': 1,\n",
    "    'choice_sensitivity': 1\n",
    "}\n",
    "\n",
    "visualize_fit(InstanceCMR, \n",
    "              {**parameters, \n",
    "               **{free_parameters[i]:result.x[i] for i in range(len(result.x))}}, \n",
    "              murd_events0, 'subject > -1', experiment_count=1000, savefig=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polished-expert",
   "metadata": {},
   "source": [
    "raw:      `[0.0664966  0.0375     0.08035714 0.02142857]`\n",
    "cmr_mrd:  `[0.16988776 0.02092857 0.00209524 0.00540476]`\n",
    "ecmr_mrd: `[0.16467857 0.02491071 0.00781548 0.00196429]`\n",
    "icmr_mrd: `[0.15796003 0.03146429 0.00825    0.00490476]`\n",
    "ecmr_1st: `[0.16042347 0.04060714 0.01940476 0.00465476]`\n",
    "icmr_1st: `[0.20694473 0.04041071 0.01882738 0.0052619 ]`\n",
    "cmr_all:  `[0.17528061 0.02973214 0.0131369  0.00580952]`\n",
    "ecmr_all: `[0.19628061 0.05201786 0.01555357 0.00479762]`\n",
    "ecmr_234: `[0.20060799 0.04823214 0.013375   0.00834524]`\n",
    "icmr_234: `[0.20093452 0.04871429 0.01308333 0.00708333]`\n",
    "cmr_4th:  `[0.20307908 0.05342857 0.01295238 0.00040476]`\n",
    "\n",
    "As fits get closer to the raw data, difference between N/A and massed repetition recall probabilities grows away from rather than shrinks toward raw values. Massed to 1-2 spaced is the same, but the gap is smaller. A shrink away from the true distance occurs when we compare between 1-2, 3-5, and 6-8. \n",
    "\n",
    "Do we get opposite patterns when we consider recall rates?\n",
    "[0.37278912 0.43928571 0.47678571 0.55714286 0.57857143]\n",
    "\n",
    "mrd\n",
    "[0.13571939 0.30560714 0.32653571 0.32863095 0.33403571]\n",
    "[0.14932143 0.314      0.33891071 0.34672619 0.34869048]\n",
    "[0.14936139 0.30732143 0.33878571 0.34703571 0.35194048]\n",
    "\n",
    "1st\n",
    "[0.33939796 0.49982143 0.54042857 0.55983333 0.5644881 ]\n",
    "[0.32584099 0.53278571 0.57319643 0.59202381 0.59728571]\n",
    "\n",
    "all\n",
    "[0.30971939 0.485      0.51473214 0.52786905 0.53367857]\n",
    "[0.29839796 0.49467857 0.54669643 0.56225    0.56704762]\n",
    "\n",
    "\n",
    "234\n",
    "[0.28996344 0.49057143 0.53880357 0.55217857 0.56052381]\n",
    "[0.29235119 0.49328571 0.542      0.55508333 0.56216667]\n",
    "\n",
    "4\n",
    "[0.31699235 0.52007143 0.5735     0.58645238 0.58685714]\n",
    "\n",
    "mrd recall rates are really low\n",
    "\n",
    "fitted to first condition, gap is highest for 0 lag and 1-2 lag items\n",
    "3-5 and 6-8 are pretty sound\n",
    "\n",
    "to all, that 0 and 1-2 lag gap is smaller. fits for 3-5 and 6-8 are better too.\n",
    "\n",
    "to 234, depressed N/A \n",
    "\n",
    "In general, improved fitting increasingly underrates recall rates for single presentations, doesn't affect overrated recalls for massed presentations, vastly overrates the 1-2 \n",
    "\n",
    "Egh, I'm basically concluding that fitting can't seem to distinguish between impact of low-spaced and highly-spaced repetitions. Across models. That's orthogonal from the goals of this project but interesting nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-feedback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
