{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unavailable-bahamas",
   "metadata": {},
   "source": [
    "# April Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-shakespeare",
   "metadata": {},
   "source": [
    "Let's review everything I have to remind myself of every gap and resource I still have at my disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-humor",
   "metadata": {},
   "source": [
    "## CMR_and_ICMR_Are_Different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-permission",
   "metadata": {},
   "source": [
    "January_22_2021 has same themes.\n",
    "\n",
    "I'm going to want to draw important distinctions between CMR and ICMR in my initial model comparison section. \n",
    "\n",
    "### We Can Conceptualize a Latent $M^{FC}$ and $M^{CF}$ in ICMR\n",
    "For exploring and demonstrating model equivalence, we can calculate for any state of ICMR's dual-store memory array $M$\n",
    "a corresponding $M^{FC}$ (or $M^{CF}$) by computing for each orthogonal $f_i$ (or $c_i$) the model's corresponding echo\n",
    "representation. \n",
    "\n",
    "Because echoes taken this way are how we retrieve $F\\rightarrow C$ and $C \\rightarrow F$ associations in the model, the\n",
    "resulting matrices characterize model behavior identically to how they would in classic CMR. This convention lets us\n",
    "compare model performance internally as well as externally.\n",
    "\n",
    "### Discarding Trace Context, or How We Don't Treat $M$ Like a Single Memory\n",
    "Even though traces in ICMR are unique, we discard their unique contextual content when configuring $c_{in}$ to cue\n",
    "successive recalls, instead using the equivalent of $M^{FC}f_i$ as performed by CMR. \n",
    "\n",
    "This information could instead be used as a direct cue for the next recall, with or without its accompanying feature\n",
    "content. Since that may be too insular, it could also included with the retrieved item's feature content as part of the\n",
    "memory cue to our equivalent of $M^{FC}f_i$. \n",
    "\n",
    "If we did either wrt to retrieval, successive recalls would be biased to a _particular trace context_ rather than a\n",
    "balanced sum of all contexts with relevant feature information, creating some interesting consequences for even single\n",
    "presentation experiments.\n",
    "\n",
    "<!--\n",
    "More generally, we decide against ever treating $M$ as a unitary memory system when it comes to free recall. Even though\n",
    "we use a single array to represent $M$, we only ever cue it with either feature information or contextual information.\n",
    "-->\n",
    "\n",
    "Probably only relevant if item feature representations are non-orthogonal. A potential extension worth a spot in my discussion section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-oklahoma",
   "metadata": {},
   "source": [
    "## ICMR as an Integrative Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-three",
   "metadata": {},
   "source": [
    "Reviews what instance-based models have covered (as reviewed by Jamieson et al, 2018). Outlines a framework for the paper that emphasizes ICMR's integrative components. I want to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-bearing",
   "metadata": {},
   "source": [
    "## Various Notes on Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-karma",
   "metadata": {},
   "source": [
    "- Jamieson_2018_1 and 2\n",
    "- Instance Context Thoughts\n",
    "\n",
    "These help remind me how ICMR and CMR differ substantively. Both linear associator and instance model can adaptively select relevant information based on a cue. But one model has nonlinear activation of stored instances based on similarity to the probe, improving retrieval of rarer concurrences. Leading to the more novel analyses I have planned.\n",
    "\n",
    "Previous work has emphasized the _timing_ of abstraction when distinguishing between prototype-based and instance-based models. Prototype-based models are characterized as performing abstraction-at-encoding, constructing and updating a single summary contextual representation for an item each time it is encoded, while instance-based models perform abstraction across instances on the fly at retrieval. When it comes at least to the _content_ of a retrieved contextual representation however, the timing of its construction is unimportant. PrototypeCMR's predictions about the temporal organization of free recall would be the same if the model stored records of item presentations until retrieval but applied the same linear mechanisms to perform abstraction. The main factor distinguishing the content of predictions made by prototype and corresponding instance-based models is the latter architecture's option to perform abstraction through nonlinear activation of traces based on relevance to a probe. Models that perform abstraction at encoding and do not preserve instances for probe-based comparison close off this option.\n",
    "\n",
    "What does nonlinear activation of stored traces get you?\n",
    "\n",
    "***\n",
    "\n",
    "Students seem confused why LSA (with BEAGLE) fails in this experiment but not ITS. Indeed, Jamieson et al only really gestures at an explanation of why: it reiterates that an instance-based model can produce nonlinear activation of stored instances, while DSMs cannot.\n",
    "\n",
    "It may be worth working through what this means why this matters, since it's the core of the critique. In ITS, the representation associated with a cue combining a word and a sense-disambiguating context (e.g. break/car) is generated by taking the product of activations associated with each word. The result is that representations in stored traces only figure substantially in the final resulting echo if they're similar to all words in the probe.\n",
    "\n",
    "In either LSA or BEAGLE, retrieval is linear: the centroid of the vectors associated with each word in a cue is used for comparisons w/ other words. The vector for \"break\" will always reflect a composition of all the contexts it was presented in, weighted by the statistical distribution of those contexts, and the same goes for \"car\". \n",
    "\n",
    "Technically, ITS could do this, too - the centroid of the echoes associated with each word in a context could be taken. (We actually do this in ICMR to achieve performance that looks like CMR's). But while ITS also has the option of building semantic representations through nonlinear coactivation of traces, linear sense disambiguation is distributional models' only option. \n",
    "\n",
    "This is a huge problem. At encoding, distributional models and ITS get the same scarce information about relatively uncommon use contexts for polysemous words, but distributional models discards - or constrains access to, depending on how you look at it- much of this information by the time retrieval happens. ITS's retrieval mechanism is more flexible: it can selectively retrieve the traces where ONLY both of the words in a joint probe occur while inhibiting activation of traces where they don't (with its cubic function). **DSMs constrain retrieval via the integrative commitments it makes at encoding.**\n",
    "\n",
    "***\n",
    "\n",
    "Maybe a good clarifying thought wrt to instance vs prototype models, though maybe it mostly reiterates what we've already discussed:\n",
    "\n",
    "Previous work has emphasized the _timing_ of abstraction when distinguishing between prototype-based and instance-based models. Prototype-based models are characterized as performing abstraction-at-encoding, constructing and updating a single summary contextual representation for an item each time it is encoded, while instance-based models perform abstraction across instances on the fly at retrieval. When it comes at least to the _content_ of a retrieved contextual representation however, the timing of its construction is unimportant.\n",
    "\n",
    "PrototypeCMR's predictions about the temporal organization of free recall would be the same if the model stored records of item presentations until retrieval but applied the same linear mechanisms to perform abstraction. Similarly, an abstraction-at-retrieval model that keeps instances in memory until retrieval could (given the same probes) produce the same representations as a model like LSA, and just do everything that latent semantic analysis entails at the moment that a probe is presented instead of earlier. \n",
    "\n",
    "The more substantive difference between prototype-based and instance-based models, at least when it comes to the content of abstractive representations, is not the timing of abstraction, but whether abstraction can involve nonlinear activation of traces based on relevance to a probe. Models technically don't have to perform abstraction in this way in order to count as instance-based or as performing abstraction at retrieval. However, only models that preserve instance representations in memory until retrieval have the option to have this feature in the first place.\n",
    "\n",
    "So in big ways, the main distinction of interest with our project and even with Jamieson et al's project isn't so much abstraction at encoding vs at retrieval or even instance-based or prototype-based per se. It's linear vs nonlinear activation of stored traces. Every other distinction between the models seems largely implementation-specific.\n",
    "\n",
    "I think my analysis of the Lohnas dataset helps reinforce this point. When I move the choice-sensitivity parameter we've been discussing outside the activation function in InstanceCMR so that there is no nonlinear activation of stored traces in the model, its predictions about how item repetitions impact recall become largely identical with prototype-based CMR's. The results will help reinforce that this specific mechanism is where there be dragons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-found",
   "metadata": {},
   "source": [
    "## Nonlinear Activation of Stored Instances\n",
    "> Notes from March_3_2021\n",
    "\n",
    "I think I've figured out the big reason CMR needs item feature representations to be orthogonal. It seems to be closely related to why ITS handles polysemous words better than DSMs.\n",
    "\n",
    "As Jamieson et al (2018) reiterate a lot, an integration-at-retrieval model can produce nonlinear activation of stored instances, while integration-at-encoding models (like CMR) cannot. CMR seems to sidestep this problem, though, with two commitments:\n",
    "\n",
    "1. **It makes feature unit activations corresponding to each item orthogonal**. The result is that each index of the activation vector generated during retrieval corresponds directly to support for a particular item/experience/memory trace. \n",
    "2. **When transforming activations into probabilities, it applies a sensitivity parameter that nonlinearly scales the contrast between well-supported and poorly supported items.** The result is that CMR effectively also produces nonlinear activation of stored instances. \n",
    "\n",
    "While item feature representations are orthogonal, this sensitivity scaling step has the same consequence whether you perform it before or after doing your sum of trace vectors. So you can do abstraction-at-encoding while still enjoying the dividends of activating traces nonlinearly based on cue similarity.\n",
    "\n",
    "As you break down the one-to-one correspondence between each feature unit and and some specific item/experience, the correspondence between outcomes of nonlinear scaling before and after abstraction also breakdown. Taking the cube of the first entry of your activation vector in CMR is no longer equivalent to scaling the activation of a particular trace based on cue similarity. \n",
    "\n",
    "Instead, learning can be distorted in this scaling step as feature information in experiences only weakly associated with the current contextual cue will be enhanced exponentially while original weightings encoded based on trace similarity only apply linearly.\n",
    "\n",
    "So, summing up. I said this was closely related to the ITS/DSM distinction, but it's not quite the same critique. In the Jamieson 2018 paper, nonlinear activation of traces in the context of DSMs is considered impossible. In CMR, nonlinear activation is achieved via softmax, but does not weight support effectively if feature units don't identify items (that is, if item representations aren't orthogonal). By performing nonlinear activation over traces instead of over the collapsed activation vector, ICMR can have non-orthogonal item representations _without_ any breakdown in learning. If my reasoning is correct.\n",
    "\n",
    "### Handling Repetitions\n",
    "The difference between applying a nonlinear transformation before and after integration is a difference between predicting an exponential scaling of retrieval support when an item gets repeated and predicting a linear increase, between (1+2+3+...)^3 and (1)^3 + (2)^3 + (3)^3 + .... Because of the way context evolves, we can't test that prediction directly, but it's something that will color less pure cases of item repetition as well.\n",
    "\n",
    "<!--\n",
    "Here, \"meaning\" corresponds to the company a word keeps, and a model of semantic memory is thus evaluable in terms of how capably it can retrieve contextual information about any cue. In that sense, the tasks of episodic and semantic memory are equally concerned with context. Successful semantic memory depends on tracking contextual features that occur consistently across experiences of an arbitrary item. Episodic memory is concerned rather with tracking item features associated with an arbitrary configuration of contextual features. In the context of ITS, this distinction isn't real except to the extent that episodic memory generally entails retrieval of item information specific enough to characterize a single memory trace.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-tomorrow",
   "metadata": {},
   "source": [
    "## Stray To-Dos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-moscow",
   "metadata": {},
   "source": [
    "- Plot how parameter modification affects shape of graph. Overlap of result of modifying parameter.\n",
    "- Use a more recent dataset than Murdoch - maybe from lab? Pierce2016, experiment 3. (exp 1 is good too?) of 2016 paper. Not the Sederberg one - had some quirks. Though could show model handles distraction okay.\n",
    "- Look into semantics. Can't get a sharp recency effect - either SPC, PFR, CRP - when feature vectors are nonorthogonal. Can use Glove or whatever to demonstrate. Capture the sharp effect w/o overpredicting semantic information. Predicting both temporal and semantic organization simultaneously is the problem.\n",
    "- **Quantitative fit comparison**. AIC can do the work; MortPolyn2016 models how to do this.\n",
    "- Repetitions analyses\n",
    "- Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
