# What to Cover?

## ICMR - Paper Framework
I feel like I need to do more than regular free recall if I want to fill up an hour long talk.

Paper strategy? 
Not trying to say ICMR does anything better.
Trying to instead argue for its integrative value - connecting instance and context theory under a common architecture.

What do we offer instance theorists? 
Instance theorists have:
- frequency judgments
- recognition
- confidence ratings
- ratio of decline in frequency/recognition judgments during forgetting
- recency judgments
- scheme abstraction
- paired associate learning
- classification
- availability and representativeness
- hypothesis generation from memory
- false memories
- maybe list length effects?
- categorization
- explainable AI

Huge wealth of evidence that people code instances in other memory research areas. For example, people can make largely independent judgments of item frequency relative to individual contexts, and otherwise can exhibit sensitivity to 

Hintzman and others have focused on the role of repetition in memory. Recency judgment paradigm as discussed by Hintzman already asserts broad agreement on a multiple traces account of the effects, though he asserts things are still ambiguous. Also brings up that instance theorists have used "time tags" to account for these patterns. Following these arguments and arguments like Logans' for a context-based representation of serial order embedded in instance models, we can similarly present our conceptualization context as a really nice option for bridging these phenomena.

Kahana's shadows of the past 2005 paper shows recognition memory - a MINERVA 2 specialty - is also sensitive to temporal associations, and comes with a dataset. I don't know if CMR has been wielded to cover recognition, but we can similarly outline quite simply or super precisely how context maintenance within an instance framework helps model these patterns.

Hintzman's accounts of spacing effects in context of MINERVA 2 are also weird; he adds new retrieval mechanisms that increase computational complexity. If there's been little further work on this, we can bring up CMR's contextual account of spacing effects and demonstrate it with our model.

So again: **Instance theorists: context helps you track rich serial/temporal effects in a super efficient way.**

What do we offer context theorists?
The ability to account for performance that depends on instance representations.

I think we can make a compelling argument that instances mediate memory based on other memory research paradigms, and can certainly illustrate that they are the basis of many popular accounts thereof.

Further we can at least compare CMR and ICMR performance on a repetition-heavy dataset, at least to illustrate how the models sync or don't wrt that phenomenon.

So: **Context theorists: an instance-based model helps connect your theoretical innovations to memory phenomena for which instance-based accounts figure strongly.**

So what's the most efficient way to show all this?
Draw up the model.
Show it fits well to free recall data.
Show it helps model some rich repetition effect. Simplest: Kahana's recognition judgments dataset. More interesting: impact of spacing. A little more unfamiliar: recency judgments

If I focus on instance-based traditions, that helps breadth. But if I focus on free recall, I get datasets and more help.

Do not try to show anything's better. Just try to explain what ICMR pulls together.
