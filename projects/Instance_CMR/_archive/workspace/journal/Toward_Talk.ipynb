{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convinced-liver",
   "metadata": {},
   "source": [
    "# Toward Talk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-brass",
   "metadata": {},
   "source": [
    "## Ideas for Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-living",
   "metadata": {},
   "source": [
    "Free Recall\n",
    "\n",
    "Retrieved Context Models\n",
    "\n",
    "Instance-Based vs Prototype-Based Models\n",
    "\n",
    "Analysis Plan\n",
    "\n",
    "How CMR Works\n",
    "\n",
    "How Instance-CMR Works\n",
    "\n",
    "Methodology (Data+Analysis)\n",
    "\n",
    "Results (Model Simulation)\n",
    "\n",
    "Results (Model Fits)\n",
    "\n",
    "Set Up Next Set of Analysis With Repetitions Dataset (e.g. w simulations)\n",
    "\n",
    "Methodology (Repetitions Dataset)\n",
    "\n",
    "Results (Model Fits)\n",
    "\n",
    "Discussion\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Even if I spend 2 minutes per section, that's less than half an hour. I have time for another batch of analyses (rule of threes!) but it will be a tough fit. I will finish the repetitions analysis, prepare the talk, and add additional material as time permits. Today, by 4pm, at minimum I will do a video-recorded walkthrough of my plans for the talk. Based on a reading of my background section, these plans could even turn out to be too ambitious for the space allowed by my talk. I hope the repetitions analysis turns up something cool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-improvement",
   "metadata": {},
   "source": [
    "## Repetitions Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-prime",
   "metadata": {},
   "source": [
    "As a reminder, the idea is that applying the choice sensitivity exponent to trace activations instead of to the echo has meaningful consequences for model behavior when an item gets repeated that might be obscured when items don't see repetition. I need to make sure that the comparison is well-motivated (with my simulation analysis and some reasoning - including probably a diagram), and \n",
    "\n",
    "I similarly want to compute fits both across all subjects/conditions, and within condition/subject. \n",
    "\n",
    "That is, I'll fit the model...\n",
    "1. Across conditions/subjects\n",
    "2. Between condition\n",
    "3. Between subject\n",
    "4. Between condition/subject\n",
    "\n",
    "Are all these conditions interesting? \n",
    "\n",
    "I will certainly be interested in overall fits. Dividing between conditions helps figure out if any dissociation in performance is because of the repetitions or not. Dividing between subjects helps clarify whether intersubject variance is a factor. Dividing between conditions _and_ subjects is mostly about exploring if there's any intersubject variance in the apparent dissociation. All interesting questions, sure. Between subject, though? Perhaps not so much.\n",
    "\n",
    "Then we need corresponding model analyses. That recall probability by repetition count plot is one step, sure; and using fitted values to underscore these differences matters too.\n",
    "\n",
    "When I'm fitting between conditions, I get to pose a conflict for the models. How much should item repetition impact recall probability vs how deterministically should recall proceed? The result of my fitting between and across conditions helps clarify how this conflict is negotiated.\n",
    "\n",
    "I am also supposed to connect this to Lohnas et al's analysis of spacing effects between conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-administration",
   "metadata": {},
   "source": [
    "### Strategy \n",
    "Start with notebooks introducing the dataset preparation algorithm and simulation analyses for arbitrary parameters.\n",
    "Then benchmark notebooks confirming a performant LL function that can work within and between conditions/subjects, ideally optimizing for deterministic presentation order, focusing on CMR at first.\n",
    "Then notebooks computing fits...across subjects, within condition; across subjects, between condition; within subjects, within condition; within subjects, between conditions. \n",
    "\n",
    "Then visualizations.\n",
    "When I do across subjects, the big objective is a set of fitted CRPs, SPCs, etc. I'll draw one for each condition; the difference will mainly be between whether I fit unique parameters for each condition, or the same parameters across conditions.\n",
    "When I do between subjects, I'm mostly comparing the distributions of log-likelihoods, as before.\n",
    "\n",
    "I'll also do a few generative analyses using parameters fit across conditions. I need a more concrete explanation of how model architecture corrodes performance that goes beyond log-likelihood distributions or my simulation analyses. I primarily need to show evidence that the model either breaks or doesn't break in the way I hypothesized. But I also might as well reproduce findings surrounding spacing and whatnot.\n",
    "\n",
    "Do that thoroughly enough and you don't really need a third act; or your Murdock 62 cross-list-length fits can be your second act. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-change",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
