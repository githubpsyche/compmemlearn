Models of memory encoding and retrieval typically include some mechanism for abstraction, a process by which information is extracted across individual experiences \markdownRendererCite{1}+{}{}{yee2019abstraction}. Abstraction is understood to involve identifying and highlighting features common across experiential episodes while disregarding or suppressing more idiosyncratic properties. This capacity for selective generalization across recurrent features of past experience is central to how memory systems retrieve relevant information even when prompted by novel cues.\markdownRendererInterblockSeparator
{}Depending on how they characterize the process abstraction, memory models can be categorized as either prototype- or instance-based. Prototype-based model conceptualize abstraction as a process enacted during encoding. Rather than stored as discrete traces, new experiences are conceived as updating prototype representations to reflect common features across past experience. Connectionist models such as the multilayer perceptron are typically examples of prototype-based models \markdownRendererCite{1}+{}{}{jamieson2018instance}; instead of being stored as separate records in memory, learning examples presented to a connectionist model each update a prototypical pattern of weights that eventually map memory probes to responses.\markdownRendererInterblockSeparator
{}On the other hand, the instance-based model architecture was originally specified to help understand how category learning might be possible without explicit storage of "abstract ideas" \markdownRendererCite{3}+{}{}{hintzman1984minerva}+{}{}{hintzman1986schema}+{}{}{hintzman1988judgments}. Instance-based models posit that memory encoding primarily involves accumulating a record of every experience as separate traces in memory. Abstraction over stored instances later occurs at retrieval rather than during encoding, and unfolds through comparison of a memory cue with each instance stored in memory. The abstractive representation finally retrieved is a blend of the content in each stored instance, weighted such that information in the instances most similar to the probe is substantially more prominent than information in instances that are dissimilar to the probe. Because instance-based models preserve a discrete record of all relevant events in memory, they can often selectively retrieve information about even rare events from memory with high flexibility.\markdownRendererInterblockSeparator
{}Instance-based accounts of memory have faced scrutiny for implying that the number of stored instances in memory can increase without limit and are all contacted upon retrieval, respectively placing extraordinary capacity and processing demands on the human nervous system \markdownRendererCite{1}+{e.g.,}{}{kahana2020computational}. However, at the same time as instance-based models have been critiqued for their architectural lack of data compression at storage, the way abstractive representations suppress idiosyncratic features of learning episodes to reflect a center of tendency is similarly recurrently cited as a limitation of prototype-based models. In research on categorization for example, "exemplar-similarity models" \markdownRendererCite{2}+{}{}{nosofsky2002exemplar}+{}{}{stanton2002comparisons} outperform comparable prototype-based models by representing categories as sets of stored instances paired with a process for comparison against probes. A related analysis extends these findings to critique prototype-based accounts of \markdownRendererEmphasis{semantic} memory. \markdownRendererTextCite{1}+{}{}{jamieson2018instance} found that because prototype-based distributional semantic models such as latent semantic analysis \markdownRendererCite{1}+{}{}{dumais2004latent} and Word2Vec \markdownRendererCite{1}+{}{}{church2017word2vec} "collapse the many contexts in which a word occurs to a single best-fitting representation", they lose the ability to represent rare senses of homonymous and polymsemous words. Consequently, prototype-based models exhibited measureably worse performance accounting for word similarity patterns in various natural language simulations compared to an instance-based account of semantic memory based on the MINERVA 2 multiple traces memory model \markdownRendererCite{1}+{}{}{hintzman1984minerva}. In the context of successes like these across diverse research conditions, instance-based accounts of memory have become increasingly prominent.\relax