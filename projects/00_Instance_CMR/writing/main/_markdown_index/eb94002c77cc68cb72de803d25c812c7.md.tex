--- jupyter: jupytext: text_representation: extension: .md format_name: pandoc format_version: 2.14.0.2 jupytext_version: 1.11.3 kernelspec: display_name: Python 3 (ipykernel) language: python name: python3 nbformat: 4\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{ nbformat_minor: 5}\markdownRendererInterblockSeparator
{}::: {.cell .markdown} Our instance-based implementation of the context maintenance and retrieval model (InstanceCMR) realizes the retrieved context account of memory search [@morton2016predictive as articulated by] within the instance-based MINERVA 2 model architecture [@hintzman1984minerva]. To make comparison of architectures as straightforward as possible, mechanisms were deliberately specified to be as similar to those of the original prototypical specification as possible except where required by the constraints of the instance-based architecture. Table Â `<a href="#table:instance_cmr_parameters" data-reference-type="ref" data-reference="table:instance_cmr_parameters">`{=html}1`</a>`{=html} provides an overview of the parameters and structures that determine model behavior.\markdownRendererInterblockSeparator
{}----------------------------------------------------------------------------------------------------------------------------------- Structure Type Symbol Name Description ----------------------- ------------------- ------------------------- ------------------------------------------------------------- Architecture \markdownRendererInterblockSeparator
{}\markdownRendererInputVerbatim{./_markdown_index/842b54f38085a72253be0f1245d37173.verbatim}\markdownRendererInterblockSeparator
{}Context Updating \markdownRendererInterblockSeparator
{}\markdownRendererInputVerbatim{./_markdown_index/2987263b0e41c84aec157c0dc8325840.verbatim}\markdownRendererInterblockSeparator
{}Associative Structure \markdownRendererInterblockSeparator
{}\markdownRendererInputVerbatim{./_markdown_index/4e3b707605e51c88271858798408a6d5.verbatim}\markdownRendererInterblockSeparator
{}Retrieval Dynamics \markdownRendererInterblockSeparator
{}\markdownRendererInputVerbatim{./_markdown_index/924b5c353689cafecfacf0cf99d3c658.verbatim}\markdownRendererInterblockSeparator
{}\markdownRendererHorizontalRule{}\markdownRendererInterblockSeparator
{}Parameters and structures specifying InstanceCMR\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Model Architecture}\markdownRendererInterblockSeparator
{}Prototypical CMR stores associations between item feature representations (represented a pattern of weights in an item layer $F$) and temporal context (represented in a contextual layer $C$) by integrating prototypical mappings between the representations via Hebbian learning over the course of encoding. In contrast, instance-based CMR tracks the history of interactions between context and item features by storing a discrete record of each experience, even repeated ones, as separate traces within in a memory store for later inspection. Memory for each experience is encoded as a separate row in an $m$ by $n$ memory matrix $M$ where rows correspond to memory traces and columns correspond to features. Each trace representing a pairing $i$ of a presented item's features $f_i$ and the temporal context of its presentation $c_i$ is encoded as a concatenated vector:\markdownRendererInterblockSeparator
{}$$M_i = (f_i, c_i)$$\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Initial State}\markdownRendererInterblockSeparator
{}Before the study phase is simulated, memory traces are intialized in $M$ to reflect pre-experimental associations between item features and contextual states. To set pre-experimental associations, a trace is encoded into memory for each relevant item. Each entry $j$ for each item feature component of pre-experimental memory traces trace $f_pre(i)$ is set according to\markdownRendererInterblockSeparator
{}$$f_{pre(i, j)} = \begin{cases} \begin{alignedat}{2} 1 - \gamma \text{, if } i=j \ 0 \text{, if } i \neq j\ \end{alignedat} \end{cases}$$\markdownRendererInterblockSeparator
{}This has the effect of relating each unit on $F$ to a unique unit on $C$ during retrieval. As within prototypical CMR, the $\gamma$ parameter controls the strength of these pre-experimental associations relative to experimental associations.\markdownRendererInterblockSeparator
{}Similarly to control pre-experimental context-to-item associations, the content of each entry $j$ for the contextual component of each pre-experimental trace $c_{pre(i,j)}$ is set by:\markdownRendererInterblockSeparator
{}$$c_{pre(ij)} = \begin{cases} \begin{alignedat}{2} \delta \text{, if } i=j \ \alpha \text{, if } i \neq j\ \end{alignedat} \end{cases}$$\markdownRendererInterblockSeparator
{}Here, $\delta$ works similarly to $\gamma$ to connect indices on $C$ to the corresponding index on $F$ during retrieval from a partial or mixed cue. The $\alpha$ parameter additionally allows all the items to support one another in the recall competition in a uniform manner.\markdownRendererInterblockSeparator
{}Before list-learning, context $C$ is initialized with a state orthogonal to the pre-experimental context associated with the set of items via the extra index that the representation vector has relative to items' feature vectors. Following the convention established for prototypical specifications of CMR, item features are further assumed to be orthonormal with respect to one another such that each unique unit on $F$ corresponds to one item.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Memory access}\markdownRendererInterblockSeparator
{}Memory is access at various points through both the study and free recall phases of model simulation using a probe representation. The probe is a vector with the same length as traces in $M$ and can contain item feature and/or contextual state information. During normal simulation of the model, however, the probe is always partial. To cue retrieval of contextual associations corresponding to a pattern of item features $f_i$, the probe can contain feature information but no contextual information:\markdownRendererInterblockSeparator
{}$$p_i = (f_i, 0)$$\markdownRendererInterblockSeparator
{}Similarly, a probe retrieving a pattern of item associated with an arbitrary contextual state $c_i$ might be represented as\markdownRendererInterblockSeparator
{}$$p_i = (0, c_i)$$\markdownRendererInterblockSeparator
{}Retrieval proceeds according to a two-step process. In the activation step $A$, the probe $p$ activates all traces in memory $M$ in parallel. Each trace's activation is computed as an exponent of its cosine similarity to the probe.\markdownRendererInterblockSeparator
{}$$a_i = \left({\frac {\sum^{j=n}_{j=1}{p_j \times M_{ij}}} {\sqrt{\sum^{j=n}_{j=1}{p^2_j}} \sqrt{\sum^{j=n}_{j=1}{M^2_{ij}}}}}\right)^{\tau}$$\markdownRendererInterblockSeparator
{}where $a_i$ is the activation of trace $i$ in memory, $p_j$ is the value of feature $j$ in the probe, $M_{ij}$ is the value if feature $j$ of trace $i$ in memory, $n$ is the number of columns in memory, and $\tau$ is a sensitivity parameter that determines the contrast between well-supported and poorly-supported traces. To avoid the possibility of assigning a probability of 0 to any possible recall, we set a minimal activation for each item to $10^{-7}$.\markdownRendererInterblockSeparator
{}Reflecting differences in how memories of contextual and item feature information are contacted during retrieval, activation is further modulated depending on the structure of the probe. For nonzero feature units in the probe ($p_f \neq 0$), activation for traces encoded during the experiment $a_{exp}$ are modulated by $\gamma$ to control the contribution of experimentally-accumulated associations to retrieved representations relative to pre-experimental associations.\markdownRendererInterblockSeparator
{}$$a_{exp(i)} = \gamma \cdot \left({\frac {\sum^{j=n}_{j=1}{p_j \times M_{ij}}} {\sqrt{\sum^{j=n}_{j=1}{p^2_j}} \sqrt{\sum^{j=n}_{j=1}{M^2_{ij}}}}}\right)^{\tau} \text{ if } p_f \neq 0$$\markdownRendererInterblockSeparator
{}Similarly, when contextual information is included in the probe ($p_c \neq 0$), activation for traces encoded during the experiment are modulated by $\phi$\markdownRendererInterblockSeparator
{}$$a_{exp(i)} = \phi_i \cdot \left({\frac {\sum^{j=n}_{j=1}{p_j \times M_{ij}}} {\sqrt{\sum^{j=n}_{j=1}{p^2_j}} \sqrt{\sum^{j=n}_{j=1}{M^2_{ij}}}}}\right)^{\tau} \text{ if } p_c \neq 0$$\markdownRendererInterblockSeparator
{}where $\phi_i$ scales the amount of learning, simulating increased attention to initial items in a list that has been proposed to explain the primacy effect. $phi_i$ depends on the serial position $i$ of the studied item:\markdownRendererInterblockSeparator
{}$$\phi_i = \phi_se^{-\phi_d(i-1)} + 1$$\markdownRendererInterblockSeparator
{}The free parameters $phi_s$ and $phi_d$ control the magnitude and decay of this learning-rate gradient, respectively.\markdownRendererInterblockSeparator
{}In the abstraction step $E$, a single representation reflecting this pattern of activations (called an echo $e$) is constructed by summing each trace, weighted by its corresponding activation.\markdownRendererInterblockSeparator
{}$$e_j = \sum^{i=m}_{i=1}\sum^{j=n}_{j=1}a_i \times M_{ij}$$\markdownRendererInterblockSeparator
{}where $e_j$ is feature $j$ in the echo, $m$ is the number of traces in memory, $a_i$ is the activation of trace $i$, and $M_{ij}$ is the value of feature $j$ in trace $i$ in memory.\markdownRendererInterblockSeparator
{}In this way, correspondences between item feature and contextual states across stored instances can be retrieved by presenting partial probe information as a cue to memory $M$.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Encoding phase}\markdownRendererInterblockSeparator
{}The above mechanisms outline how an instance-based memory architecture encodes and retrieves information differently from a prototype-based model. Next we relate these architectural features to interactions between contextual state and item feature representations that are proposed to account for memory search in a typical free recall task.\markdownRendererInterblockSeparator
{}When an item $i$ is presented during the study period, its representation on $F$, $f_i$, is activated. Pre-experimental context $c_{i}^{IN}$ is retrieved using the two-step echo $E$ mechanism described below by presenting $(f_i, 0)$ as a probe to memory. The input to context is retrieved by\markdownRendererInterblockSeparator
{}$$(_ , c_{i}^{IN}) = E\left[(f_i, 0)\right]$$\markdownRendererInterblockSeparator
{}where the contextual features of the retrieved echo determine contextual input. The retrieved pre-experimental context is then normalized to have length 1.\markdownRendererInterblockSeparator
{}After retrieval of pre-experimental context $c_{i}^{IN}$, the current state of context is updated according to.\markdownRendererInterblockSeparator
{}$$c_i = \rho_ic_{i-1} + \beta c_{i}^{IN}$$\markdownRendererInterblockSeparator
{}Where $\beta$ is set to $\beta_{enc}$ and $rho_i$ is set so that the length of $c_i$ is 1, according to\markdownRendererInterblockSeparator
{}$$\rho_i = \sqrt{1 + \beta^2\left[\left(c_{i-1} \cdot c^{IN}_i\right)^2 - 1\right]} - \beta\left(c_{i-1} \cdot c^{IN}_i\right)$$\markdownRendererInterblockSeparator
{}After context is updated, the current item $f_i$ and the current state of context $c_i$ become associated in memory $M$ by storing a concatenation of the two vectors as a new trace.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Retrieval Phase}\markdownRendererInterblockSeparator
{}Before initiating recall, we assume that some amount of the pre-list context is reinstated. We assume that context is updated according to\markdownRendererInterblockSeparator
{}$$c_{start} = \rho_{N+1}c_N + \beta_{start}c_0$$\markdownRendererInterblockSeparator
{}where $c_{start}$ is the state of context at the start of free recall, $N$ is the number of items in the list, $c_0$ is the state of context at the start of the list before any item presentation, and $\rho_{N+1}$ is calculated as specified above.\markdownRendererInterblockSeparator
{}At each recall attempt, calculate the probability of stopping recall (where no item was recalled and search terminated). It varies as a function of output position $j$ (where $j=0$ for the first attempt), according to\markdownRendererInterblockSeparator
{}$$P(stop, j) = \theta_se^{j\theta_r}$$\markdownRendererInterblockSeparator
{}where $\theta_s$ and $\theta_r$ are free parameters that determine the scaling and rate of increase, respectively, of the exponential function.\markdownRendererInterblockSeparator
{}At each recall attempt, memory $M$ is probed with the current state of context to obtain an echo representation reflecting the support $s$ for retrieval of each item:\markdownRendererInterblockSeparator
{}$$(s_i , _) = E[(0, c)]$$\markdownRendererInterblockSeparator
{}Each item feature unit in the retrieved vector represents the relative support $s_i$ for retrieving the corresponding item. The probability $P(i)$ of recalling a given item $i$ is defined conditional on recall not stopping at that position, and varies with this result:\markdownRendererInterblockSeparator
{}$$P(i) = (1-P(stop))\frac{s_i}{\sum_{k}^{N}s_k}$$\markdownRendererInterblockSeparator
{}If an item is recalled, its feature representation is reactivated on $F$. The reactivated item is then used to retrieve a representation of context according to\markdownRendererInterblockSeparator
{}$$(_ , c^{IN}) = E\left[(f, 0)\right]$$\markdownRendererInterblockSeparator
{}Context is then updated using the mechanism for it specified in the above section, and is used to cue for another recall attempt. The process continues until it is ended by the termination process.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Alternative Echo-Based Activation Function}\markdownRendererInterblockSeparator
{}Since instance-based models' unique trace-based nonlinear activation function has been emphasized in comparisons distinguishing them from prototype models, we further specify an alternative version of InstanceCMR that applies the nonlinear activation scaling parameter $\tau$ to retrieved echo representations supporting recall competition rather than to trace activations. This echo-based InstanceCMR (labeled eCMR in figures) removes $\tau$ from the model's trace activation function such that:\markdownRendererInterblockSeparator
{}$$a_i = \left({\frac {\sum^{j=n}_{j=1}{p_j \times M_{ij}}} {\sqrt{\sum^{j=n}_{j=1}{p^2_j}} \sqrt{\sum^{j=n}_{j=1}{M^2_{ij}}}}}\right)^{\tau}$$\markdownRendererInterblockSeparator
{}And applies the parameter to modulate differences between highly and less supported items in recall such that:\markdownRendererInterblockSeparator
{}$$P(i) = (1-P(stop))\frac{s^{\tau}_i}{\sum_{k}^{N}s^{\tau}_k}$$\markdownRendererInterblockSeparator
{}Application of the $\tau$ parameter to directly modulate item recall probabilities rather than trace activations more closely matches the dynamics of the original prototypical implementation of CMR. Where model comparison finds substantive differences between prototypical CMR and InstanceCMR, but not between InstanceCMR and this eCMR alternative implementation, this difference between how prototypical and instance-based implementations apply $\tau$ can be at least partially ruled out as an explanation of performance differences. :::\relax