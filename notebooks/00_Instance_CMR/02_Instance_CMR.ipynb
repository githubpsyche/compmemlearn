{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Maintenance and Retrieval within an Instance-Based Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "import numpy as np\n",
    "from numba import float64, int32, boolean\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "spec = [\n",
    "    ('item_count', int32), \n",
    "    ('encoding_drift_rate', float64),\n",
    "    ('start_drift_rate', float64),\n",
    "    ('recall_drift_rate', float64),\n",
    "    ('shared_support', float64),\n",
    "    ('item_support', float64),\n",
    "    ('learning_rate', float64),\n",
    "    ('primacy_scale', float64),\n",
    "    ('primacy_decay', float64),\n",
    "    ('stop_probability_scale', float64),\n",
    "    ('stop_probability_growth', float64),\n",
    "    ('choice_sensitivity', float64),\n",
    "    ('context_sensitivity', float64),\n",
    "    ('feature_sensitivity', float64),\n",
    "    ('context', float64[::1]),\n",
    "    ('preretrieval_context', float64[::1]),\n",
    "    ('recall', float64[::1]),\n",
    "    ('retrieving', boolean),\n",
    "    ('recall_total', int32),\n",
    "    ('item_weighting', float64[::1]),\n",
    "    ('context_weighting', float64[::1]),\n",
    "    ('all_weighting', float64[::1]),\n",
    "    ('probabilities', float64[::1]),\n",
    "    ('memory', float64[:,::1]),\n",
    "    ('encoding_index', int32),\n",
    "    ('items', float64[:,::1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@jitclass(spec)\n",
    "class Instance_CMR:\n",
    "\n",
    "    def __init__(\n",
    "        self, item_count, presentation_count, encoding_drift_rate, \n",
    "        start_drift_rate, recall_drift_rate, shared_support,\n",
    "        item_support, learning_rate, primacy_scale, primacy_decay, \n",
    "        stop_probability_scale, stop_probability_growth, choice_sensitivity, \n",
    "        context_sensitivity, feature_sensitivity):\n",
    "\n",
    "        # store initial parameters\n",
    "        self.item_count = item_count\n",
    "        self.encoding_drift_rate = encoding_drift_rate\n",
    "        self.start_drift_rate = start_drift_rate\n",
    "        self.recall_drift_rate = recall_drift_rate\n",
    "        self.shared_support = shared_support\n",
    "        self.item_support = item_support\n",
    "        self.learning_rate = learning_rate\n",
    "        self.primacy_scale = primacy_scale\n",
    "        self.primacy_decay = primacy_decay\n",
    "        self.stop_probability_scale = stop_probability_scale\n",
    "        self.stop_probability_growth = stop_probability_growth\n",
    "        self.choice_sensitivity = choice_sensitivity\n",
    "        self.context_sensitivity = context_sensitivity\n",
    "        self.feature_sensitivity = feature_sensitivity\n",
    "        \n",
    "        # at the start of the list context is initialized with a state \n",
    "        # orthogonal to the pre-experimental context associated with the set of items\n",
    "        self.context = np.zeros(item_count + 1)\n",
    "        self.context[0] = 1\n",
    "        self.preretrieval_context = self.context\n",
    "        self.recall = np.zeros(item_count) # recalls has at most `item_count` entries\n",
    "        self.retrieving = False\n",
    "        self.recall_total = 0\n",
    "\n",
    "        # predefine activation weighting vectors\n",
    "        self.item_weighting = np.ones(item_count+presentation_count)\n",
    "        self.context_weighting = np.ones(item_count+presentation_count)\n",
    "        self.item_weighting[item_count:] = learning_rate\n",
    "        self.context_weighting[item_count:] = \\\n",
    "            primacy_scale * np.exp(-primacy_decay * np.arange(presentation_count)) + 1\n",
    "        self.all_weighting = self.item_weighting * self.context_weighting\n",
    "\n",
    "        # preallocate for outcome_probabilities\n",
    "        self.probabilities = np.zeros((item_count + 1))\n",
    "\n",
    "        # initialize memory\n",
    "        # we now conceptualize it as a pairing of two stores Mfc and Mcf respectively\n",
    "        # representing feature-to-context and context-to-feature associations\n",
    "        mfc = np.eye(item_count, item_count + 1, 1) * (1 - learning_rate)\n",
    "        mcf = np.ones((item_count, item_count)) * shared_support\n",
    "        for i in range(item_count):\n",
    "            mcf[i, i] = item_support\n",
    "        mcf = np.hstack((np.zeros((item_count, 1)), mcf))\n",
    "        self.memory = np.zeros((item_count + presentation_count, item_count * 2 + 2))\n",
    "        self.memory[:item_count,] = np.hstack((mfc, mcf))\n",
    "        self.encoding_index = item_count\n",
    "        self.items = np.eye(item_count, item_count + 1, 1)\n",
    "\n",
    "    def experience(self, experiences):\n",
    "\n",
    "        for i in range(len(experiences)):\n",
    "            self.memory[self.encoding_index, :self.item_count+1] = experiences[i]\n",
    "            self.update_context(self.encoding_drift_rate, self.memory[self.encoding_index])\n",
    "            self.memory[self.encoding_index, self.item_count+1:] = self.context\n",
    "            self.encoding_index += 1\n",
    "\n",
    "    def update_context(self, drift_rate, experience=None):\n",
    "\n",
    "        # first pre-experimental or initial context is retrieved\n",
    "        if experience is not None:\n",
    "            context_input = self.echo(experience)[self.item_count + 1:]\n",
    "            context_input = context_input / np.sqrt(np.sum(np.square(context_input))) # norm to length 1\n",
    "        else:\n",
    "            context_input = np.zeros((self.item_count+1))\n",
    "            context_input[0] = 1\n",
    "\n",
    "        # updated context is sum of context and input, modulated by rho to have len 1 and some drift_rate\n",
    "        rho = np.sqrt(1 + np.square(drift_rate) * (np.square(self.context * context_input) - 1)) - (\n",
    "                drift_rate * (self.context * context_input))\n",
    "        self.context = (rho * self.context) + (drift_rate * context_input)\n",
    "\n",
    "    def echo(self, probe):\n",
    "\n",
    "        return np.dot(self.activations(probe), self.memory[:self.encoding_index])\n",
    "\n",
    "    def activations(self, probe):\n",
    "\n",
    "        # computes and cubes similarity value to find activation for each trace in memory\n",
    "        activation = np.dot(self.memory[:self.encoding_index], probe) / (\n",
    "            np.sqrt(np.sum(np.square(self.memory[:self.encoding_index]), axis=1)) * np.sqrt(\n",
    "                np.sum(np.square(probe))))\n",
    "\n",
    "        # weight activations based on whether probe contains item or contextual features or both\n",
    "        if np.any(probe[:self.item_count + 1]):\n",
    "            if np.any(probe[self.item_count + 1:]):\n",
    "                # both mfc and mcf weightings, see below\n",
    "                activation *= self.all_weighting[:self.encoding_index]\n",
    "            else:\n",
    "                # mfc weightings - scale by gamma for each experimental trace\n",
    "                activation *= self.item_weighting[:self.encoding_index]\n",
    "            activation = np.power(activation, self.context_sensitivity)\n",
    "        else:\n",
    "            # mcf weightings - scale by primacy/attention function based on experience position\n",
    "            activation *= self.context_weighting[:self.encoding_index]\n",
    "            if self.feature_sensitivity != 1.0:\n",
    "                activation = np.power(activation, self.feature_sensitivity)\n",
    "            else:\n",
    "                activation = np.power(activation, self.context_sensitivity)\n",
    "            \n",
    "        return activation + 10e-7\n",
    "\n",
    "    def outcome_probabilities(self, activation_cue):\n",
    "\n",
    "        echo = self.echo(activation_cue)[1:self.item_count+1]\n",
    "        echo = np.power(echo, self.choice_sensitivity)\n",
    "        \n",
    "        self.probabilities = np.zeros((self.item_count + 1))\n",
    "        self.probabilities[0] = min(self.stop_probability_scale * np.exp(\n",
    "            self.recall_total * self.stop_probability_growth), 1.0 - (self.item_count * 10e-7))\n",
    "\n",
    "        if self.probabilities[0] < 1:\n",
    "            for already_recalled_item in self.recall[:self.recall_total]:\n",
    "                echo[int(already_recalled_item)] = 0\n",
    "        self.probabilities[1:] = (1-self.probabilities[0]) * echo / np.sum(echo)\n",
    "        \n",
    "        return self.probabilities\n",
    "\n",
    "    def free_recall(self, steps=None):\n",
    "\n",
    "        # some pre-list context is reinstated before initiating recall\n",
    "        if not self.retrieving:\n",
    "            self.recall = np.zeros(self.item_count)\n",
    "            self.recall_total = 0\n",
    "            self.preretrieval_context = self.context\n",
    "            self.update_context(self.start_drift_rate)\n",
    "            self.retrieving = True\n",
    "            \n",
    "        # number of items to retrieve is infinite if steps is unspecified\n",
    "        if steps is None:\n",
    "            steps = self.item_count - self.recall_total\n",
    "        steps = self.recall_total + steps\n",
    "\n",
    "        # at each recall attempt\n",
    "        while self.recall_total < steps:\n",
    "\n",
    "            # the current state of context is used as a retrieval cue to \n",
    "            # attempt recall of a studied item compute outcome probabilities \n",
    "            # and make choice based on distribution\n",
    "            outcome_probabilities = self.outcome_probabilities(\n",
    "                np.hstack((np.zeros(self.item_count + 1), self.context)))\n",
    "            if np.any(outcome_probabilities[1:]):\n",
    "                choice = np.sum(\n",
    "                    np.cumsum(outcome_probabilities) < np.random.rand())\n",
    "            else:\n",
    "                choice = 0\n",
    "\n",
    "            # resolve and maybe store outcome\n",
    "            # we stop recall if no choice is made (0)\n",
    "            if choice == 0:\n",
    "                self.retrieving = False\n",
    "                self.context = self.preretrieval_context\n",
    "                break\n",
    "            self.recall[self.recall_total] = choice - 1\n",
    "            self.recall_total += 1\n",
    "            self.update_context(self.recall_drift_rate,\n",
    "                                np.hstack((self.items[choice - 1], \n",
    "                                           np.zeros(self.item_count + 1))))\n",
    "        return self.recall[:self.recall_total]\n",
    "    \n",
    "    def force_recall(self, choice=None):\n",
    "\n",
    "        if not self.retrieving:\n",
    "            self.recall = np.zeros(self.item_count)\n",
    "            self.recall_total = 0\n",
    "            self.preretrieval_context = self.context\n",
    "            self.update_context(self.start_drift_rate)\n",
    "            self.retrieving = True\n",
    "\n",
    "        if choice is None:\n",
    "            pass\n",
    "        elif choice > 0:\n",
    "            self.recall[self.recall_total] = choice - 1\n",
    "            self.recall_total += 1\n",
    "            self.update_context(\n",
    "                self.recall_drift_rate, \n",
    "                np.hstack((self.items[choice - 1], \n",
    "                           np.zeros(self.item_count + 1))))\n",
    "        else:\n",
    "            self.retrieving = False\n",
    "            self.context = self.preretrieval_context\n",
    "        return self.recall[:self.recall_total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype-based implementations of the retrieved context account of memory search generally suppose that learned item and contextual associations are encoded into abstractive prototype representations according to a Hebbian learning process and then retrieved based on activation from a cue. The memory architecture investigated in this paper alternatively supposes that learning episodes are stored as discrete instances in memory and only abstracted over at retrieval. Within previous examples of this architecture [e.g., @hintzman1984minerva; @jamieson2018instance], stored instances are represented as vectors stacked within a $m$ by $n$ memory matrix $M$. In model variations where vectors are not composed of binary values, at retrieval each trace is activated in parallel based on a positively accelerated transformation of its cosine similarity to a probe $p$:\n",
    "\n",
    "\\begin{equation} \\label{eq:14}\n",
    "a(p)_i = \\left({\\frac {\\sum^{j=n}_{j=1}{p_j \\times M_{ij}}} {\\sqrt{\\sum^{j=n}_{j=1}{p^2_j}}\n",
    "        \\sqrt{\\sum^{j=n}_{j=1}{M^2_{ij}}}}}\\right)^{\\tau}\n",
    "\\end{equation}\n",
    "\n",
    "Within this architecture, the parameter $\\tau$ exponentially scales this acceleration, effectively controlling the selectivity of retrieval by modulating the difference in activations between highly and less relevant traces. A sum of stored traces weighted by these nonlinearly scaled activations -- called an *echo*, $E(p)$, is taken to build an abstractive representation for retrieval:\n",
    "\n",
    "\\begin{equation} \\label{eq:15}\n",
    "E(p) = \\sum^{i=m}_{i=1}\\sum^{j=n}_{j=1}a(p)_i \\times M_{ij}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our instance-based implementation of the context maintenance and retrieval model (InstanceCMR) realizes the retrieved context account of memory search [as articulated by @morton2016predictive] by extending this instance-based architecture to capture how retrieved context theory avers that item and temporal contextual associations evolve and organize retrieval. To make comparison of architectures as straightforward as possible, mechanisms were deliberately specified to be as similar to those of the original prototypical specification as possible except where required by the constraints of the instance-based architecture.\n",
    "\n",
    "\n",
    "| Structure Type        | Symbol            | Name                    | Description                                                 |\n",
    "|:----------------------|:------------------|:------------------------|:------------------------------------------------------------|\n",
    "| Architecture          |                   |                         |                                                             |\n",
    "|                       | $M$               | memory                  | Array of accumulated memory traces                          |\n",
    "|                       | $C$               | temporal context        | A recency-weighted average of encoded items                 |\n",
    "|                       | $F$               | item features           | Current pattern of item feature unit activations            |\n",
    "| Context Updating      |                   |                         |                                                             |\n",
    "|                       | ${\\beta}_{enc}$   | encoding drift rate     | Rate of context drift during item encoding                  |\n",
    "|                       | ${\\beta}_{start}$ | start drift rate        | Amount of start-list context retrieved at start of recall   |\n",
    "|                       | ${\\beta}_{rec}$   | recall drift rate       | Rate of context drift during recall                         |\n",
    "| Associative Structure |                   |                         |                                                             |\n",
    "|                       | ${\\alpha}$        | shared support          | Amount of support items initially have for one another      |\n",
    "|                       | ${\\delta}$        | item support            | Initial pre-experimental contextual self-associations       |\n",
    "|                       | ${\\gamma}$        | learning rate           | Amount of experimental context retrieved by a recalled item |\n",
    "|                       | ${\\phi}_{s}$      | primacy scale           | Scaling of primacy gradient on trace activations            |\n",
    "|                       | ${\\phi}_{d}$      | primacy decay           | Rate of decay of primacy gradient                           |\n",
    "| Retrieval Dynamics    |                   |                         |                                                             |\n",
    "|                       | ${\\tau}$          | choice sensitivity      | Exponential weighting of similarity-driven activation       |\n",
    "|                       | ${\\theta}_{s}$    | stop probability scale  | Scaling of the stop probability over output position        |\n",
    "|                       | ${\\theta}_{r}$    | stop probability growth | Rate of increase in stop probability over output position   |\n",
    "\n",
    "  : Parameters and structures specifying InstanceCMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototypical CMR stores associations between item feature representations (represented a pattern of weights in an item layer $F$) and temporal context (represented in a contextual layer $C$) by integrating prototypical mappings between the representations via Hebbian learning over the course of encoding. In contrast, InstanceCMR tracks the history of interactions between context and item features by storing a discrete record of each experience, even repeated ones, as separate traces within in a memory store for later inspection. Memory for each experience is encoded as a separate row in an $m$ by $n$ memory matrix $M$ where rows correspond to memory traces and columns correspond to features. Each trace representing a pairing $i$ of a presented item’s features $f_i$ and the temporal context of its presentation $c_i$ is encoded as a concatenated vector:\n",
    "\n",
    "\\begin{equation} \\label{eq:16}\n",
    "M_i = (f_i, c_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structuring $M$ as a stack of concatenated item and contextual feature vectors $(f_i, c_i)$ makes it possible to define pre-experimental associations between items and contextual states similarly to the pattern by which PrototypeCMR's pre-experimental associations are specified in equations ~\\ref{eq:1} and ~\\ref{eq:2}. To set pre-experimental associations, a trace is encoded into memory $M$ for each relevant item. Each entry $j$ for each item feature component of pre-experimental memory traces trace $f_{pre}(i)$ is set according to\n",
    "\n",
    "\\begin{equation} \\label{eq:17}\n",
    "f_{pre(i, j)} = \\begin{cases} \\begin{alignedat}{2} 1 - \\gamma \\text{, if } i=j \\\\\\\n",
    "          0 \\text{, if } i \\neq j\n",
    "       \\end{alignedat} \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This has the effect of relating each unit on $F$ to a unique unit on $C$ during retrieval. As within prototypical CMR, the $\\gamma$ parameter controls the strength of these pre-experimental associations relative to experimental associations.\n",
    "\n",
    "Similarly to control pre-experimental context-to-item associations, the content of each entry $j$ for the contextual component of each pre-experimental trace $c_{pre(i,j)}$ is set by:\n",
    "\n",
    "\\begin{equation} \\label{eq:18}\n",
    "c_{pre(i,j)} = \\begin{cases} \\begin{alignedat}{2} \\delta \\text{, if } i=j \\\\\\\n",
    "          \\alpha \\text{, if } i \\neq j\n",
    "       \\end{alignedat} \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\delta$ works similarly to $\\gamma$ to connect indices on $C$ to the corresponding index on $F$ during\n",
    "retrieval from a partial or mixed cue. The $\\alpha$ parameter additionally allows all the items to support one\n",
    "another in the recall competition in a uniform manner.\n",
    "\n",
    "Before list-learning, context $C$ is initialized with a state orthogonal to the pre-experimental context associated\n",
    "with the set of items via the extra index that the representation vector has relative to items’ feature vectors. Following the convention established for prototypical specifications of CMR, item features are further assumed to be orthonormal with respect to one another such that each unique unit on $F$ corresponds to one item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a broad sense, the initial steps of item encoding within InstanceCMR proceed similarly to the process in PrototypeCMR. Just as with PrototypeCMR, when an item $i$ is presented during the study period, its corresponding feature representation $f_i$ is activated on $F$ and its contextual associations encoded into $M^{FC}$ are retrieved by presenting $f_i$ as a probe to memory. InstanceCMR, however, performs retrieval by applying an extension of the basic two-step echo $E$ mechanism outlined in equations ~\\ref{eq:14} and ~\\ref{eq:15}. \n",
    "\n",
    "The extension of the original mechanism differentiates between item- and context-based retrieval. When probes include item feature information ($p_f \\neq 0$), activation for traces encoded during the experiment are modulated by $\\gamma$ to control the contribution of experimentally-accumulated associations to retrieved representations relative to pre-experimental associations:\n",
    "\n",
    "\n",
    "\\begin{equation} \\label{eq:19}\n",
    "(, c^{IN}) = E(f_i, 0) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\gamma} \\times a(f_i, 0)_j \\times M_{jk}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The contextual features of the retrieved echo determine contextual input; this retrieved pre-experimental context is normalized to have length 1. Upon retrieval of $c^{IN}$, the current state of context is updated the same way as it is under the prototype-based framework, applying equations ~\\ref{eq:4} and ~\\ref{eq:5} to drift $c$ toward $c^{IN}$ and enforce its length to 1, respectively.\n",
    "\n",
    "\n",
    "After context is updated, the current item $f_i$ and the current state of context $c_i$ become associated in memory\n",
    "$M$ by storing a concatenation of the two vectors as a new trace $(f_i, c_i)$. This mechanism reserves abstraction over learning episodes for cue-based retrieval rather than at the point of encoding as in PrototypeCMR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the lead of the classic prototype-based implementation of CMR, before retrieval InstanceCMR reinstates some pre-list context according to ~\\ref{eq:9}. Similarly, at each recall attempt $i$, we calculate the probability of stopping recall (where no item is recalled and search is terminated) based on output position according to ~\\ref{eq:11}.\n",
    "\n",
    "To determine the probability of recalling an item given that recall does not terminate, first the current state of context is applied as a retrieval cue to retrieve an item feature presentation $f_{rec}$, again applying a modification of the echo-based retrieval mechanism characteristic of instance-based models that modulates trace activations before aggregation into an echo representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation} \\label{eq:20}\n",
    "(f_{rec},) = E(0, c_i) = \\sum^{j=m}_{j=1}\\sum^{k=n}_{k=1} {\\phi}_j \\times a(0, c_i)_j \\times M_{jk}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ${\\phi}_i$ scales the amount of learning, simulating increased attention to initial items in a list that has been\n",
    "proposed to explain the primacy effect. ${\\phi}_i$ depends on the serial position $i$ of the studied item the same as it does in PrototypeCMR (equation ~\\ref{eq:8}), with the free parameters ${\\phi}_s$ and ${\\phi}_d$ respectively controlling the magnitude and decay of the corresponding learning-rate gradient.\n",
    "\n",
    "Since item feature representations are presumed to be orthogonal for the purposes of the model, the content of $f_{rec}$ can be interpreted as a measure of the relative support in memory for retrieval of each item $i$, setting the probability distribution of item recalls $P(i)$ to \n",
    "\n",
    "\\begin{equation} \\label{eq:21}\n",
    "P(i) = (1-P(stop))\\frac{f_{rec}}{\\sum_{k}^{N}f_{rec}}\n",
    "\\end{equation}\n",
    "\n",
    "If an item is recalled, then that item is reactivated on $F$, and its contextual associations retrieved for integration into context again according to ~\\ref{eq:19}. Context is updated again based on this input (using $\\beta_{rec}$ instead of $\\beta_{enc}$) and used to cue a successive recall attempt. This process continues until recall stops.\n",
    "\n",
    "An important difference between equation ~\\ref{eq:21} and that applied in our specification of PrototypeCMR to compute $P(i)$ (equation ~\\ref{eq:12}) is that $\\tau$ is not applied as an exponent to retrieval supports to shape the contrast between well-supported and poorly supported items. Instead, instance-based models apply this transformation to discrete trace activations before aggregation of an echo representation. This difference still achieves the effect of ultimately either widening or shrinking the difference between item supports driving retrieval, but is not trivial. Its consequences are explored in later sections. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
