# Toward the Generalization Criterion as Justification for a CMR-DE


## Post Hoc and A Priori Model Selection Approaches
Let's start with a framing by Busemeyer and Wang (2000):

The most commonly used method of model comparison is to (1) fit each model to the entire dataset using a maximimum likelihood criteria and then to (2) compare the models using a chi-square test based on something like the log-likelihood ratio statistic or the Akaike information criterion. As the sample size increases, this approach tends toward selection of overly complex models, but the logic is that the discrepancy between the models with their best fitting parameters and the data is a good post-hoc (i.e. after data observation) justification for selection.

Scientists are usually interested in models that make good a priori predictions rather than just produce good post hoc fits. After all, especially complex models are generally going to perform better in tests dependent on the latter, especially as N increases. Model evaluation using a priori predictions often rely on strong inference tests (Platt, 1964), which is classically described as:

1. Devising alternative hypotheses;
2. Devising a crucial experiment (or several of them), with alternative possible outcomes, each of which will, as nearly as possible, exclude one or more of the hypotheses;
3. Carrying out the experiment(s) so as to get a clean result;
4. Recycling the procedure, making subhypotheses or sequential hypotheses to refine the possibilities that remain, and so on.

The method relies on deriving a priori and parameter-free predictions about the rank order of performance across a set of experimental conditions, where each model class predicts a unique rank order. Model selection is based on eliminating the class of models that predict rank orders contrary to the observed results. This approach tests the qualitative properties derived from the models, and so doesn't rely on parameter estimation or quantitative model fit. 

As we know though, particularly for even models as complex as CMR, it can be super hard to derive parameter-free model predictions, let alone devise appropriate hypotheses and crucial experiments for contrasting models based on them. Predictions by models like CMR are quite parameter-dependent, and even under a constrained parameter space, qualitative features of model performance can take a lot of work to identify, especially under novel research conditions. For example, immediate free recall datasets with item repetitions such as that described by Lohnas & Kahana (2014) are hard to characterize with benchmark analyses like the lag-CRP and serial position curve since study items do not have a unique serial position. For this reason, other analyses have been developed to characterize the effect of item repetition on memory, such as the OR-score and measurements of the spacing effect.

This past weekend, I've been unknowingly/informally working toward evaluation of CMR and justification for extending it using a method called the *generalization criterion*. Like the likelihood-based comparison technique we're familiar with, the generalization criterion relies on parameter-dependent quantitative predictions from complex models. Unlike strong inference though, it's based on a prior predictions made before observing the data rather than post hoc fits. This sidesteps issues with comparison of complex and simple models. It's also pretty intuitive, and already widely applied both informally and in a lot of model-based cognitive science research -- it's a standard topic of computational modeling texts like Lewandowsky and Farrell (2010). 


## The Generalization Criterion 
We can contrast the generalization criterion from the cross validity criterion. Applying the cross validity criterion involves taking two samples from exactly the same design, fitting models to data from one sample (a calibration sample), and then evaluating discrepancies between the fitted models' predictions and the data in the other sample (a validation sample). This approach is actually super popular in machine learning research, but for large sample sizes, cross-validation provides little or no additional information over a direct comparison of models using only the calibration stage.

The generalization criterion employs samples from two different research designs. During the calibration stage, competing models are fit to data from one design. Next, the previously estimated parameters from the first design are used again to compare predictive accuracy for the second design. If (A) model parameters were estimated with sufficient precision in the first stage and (B) model performance is distinguishable in the second stage, you have grounds for selecting one model over the other.

Because selection is based on the a priori predictions computed from each model, this approach can compare any set of models, nested or nonnested, complex or simple. Though complex models have an unfair advantage in the calibration stage, they're on equal footing in the generalization stage.


## CMR and List Length: A Relevant Historical Example
Polyn & Kahana (2009) includes a useful example of work exploring CMR's capacity for generalization across research designs. Classic behavioral data reported by Murdock (1962) reports performance under the immediate free recall paradigm as list length is manipulated between trials (20, 30, and 40 items). Results identified a consistent shape across list lengths of the primacy portion of the serial position curve even as the overall probability of recall for items presented early in a list dropped with increasing list lengths. The work also indicated that increasing list lengths reduced recall probability of mid-list items and had no effect on the recency slope corresponding to terminal items. 

A key result of Polyn & Kahana (2009) was that despite observed differences in serial position curves, a single set of parameters of the Context Maintenance and Retrieval (CMR) model could account for performance across all conditions. CMR could account for the behavioral effect of list length on immediate free recall in terms of the dynamics of the recall competition: as the umber of items competing for recall increases, the support for recall for any one item falls. By strongly supporting recall of terminal items though, the end of list context cue specified by CMR insulates those items from this effect. As part of our explorations of an instance-based variant of CMR, we reproduced this finding (with the original specification of CMR) using log-likelihood based fitting instead of curve-based fitting.

No model comparison based on the generalization criterion was actually applied for these analyses; instead it was CMR being explored on its own. Furthermore, even if models were being compared, comparison of fitted parameters and corresponding discrepancies with data across (rather than between) distinct research conditions is still post hoc comparison, and favors complex models. In other words, to evaluate a model's capacity to support a priori predictions, it's better to fit a model to just data from one subset of research designs and then measure the predictive accuracy of the model with those fitted parameters against the other set of research designs.

I've done some work this past though that recapitulates the finding that CMR does a great job of generalizing across conditions of the Murdock (1962) dataset. I've only known about Busemeyer and Wang (2000) for a few hours, so my work does not directly apply its technique. However, I fit CMR separately to each condition of the dataset and visualized evidence that fitted parameters from each condition readily generalized to other research conditions.


### Comparing Parameter Value Distributions From Subject-Level Fits Between Conditions
My mindset with my original analyses was to validate a method for characterizing the capacity of a cognitive model like CMR to account for behavior across distinct research conditions using the same parameter configuration. I fit the CMR to different conditions of the Murdock (1962) dataset, with unique parameter configurations fitted for each subject. Then I plotted from these fits and each parameter the variation between parameter values across conditions. If CMR has good generalizability with respect to list length, then we'd expect limited discrepancy in fitted parameter distributions between condition, though that's not the same test as the generalization criterion outlined above per se.

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="ticks")

for parameter in free_parameters:
    g = sns.catplot(x="list_length", y=parameter, data=param_df, kind='violin')
    plt.show()
```
