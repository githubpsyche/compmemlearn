[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "compmemlearn",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "library\\analyses\\Alternative_Contiguity.html#data-preparation",
    "href": "library\\analyses\\Alternative_Contiguity.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\nfrom compmemlearn.analyses import recall_by_all_study_positions\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Alternative_Contiguity.html#analysis",
    "href": "library\\analyses\\Alternative_Contiguity.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "import pandas as pd\nimport numpy as np\n\ndef indices_of_repeated_items(presentation_sequence):\n\n    values, counts = np.unique(presentation_sequence, return_counts=True)\n    repeated_items = {v: np.where(presentation_sequence == v)[0] for v in values if counts[v] > 1}\n\n    return {key:repeated_items[key] for key in repeated_items}\n\n# lag thresholding doesn't work reliably with this code, overcounting actual transitions\ndef alternative_contiguity_test(mixed_presentations, mixed_recalls, lag_threshold, repetition_count):\n    relevant_lags = list(range(-int(lag_threshold/2), int(lag_threshold/2+1)))\n    del relevant_lags[int(lag_threshold/2)]\n\n    actual_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n    possible_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n\n    for trial_index in range(len(mixed_presentations)):\n\n        # sequence of item indices ordered as they were studied\n        presentation = mixed_presentations[trial_index]\n\n        # sequence of initial study positions ordered as they were recalled\n        trial_by_study_position = mixed_recalls[trial_index]\n\n        # sequence of item indices ordered as they were recalled\n        trial_by_item_index = presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]\n\n        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items\n        i_and_j = indices_of_repeated_items(presentation)\n        \n        # then for each unique repeated item in the study list, \n        for repeated_item in i_and_j:\n\n            # search for relevant item(s) in recall sequence and skip if not found \n            look_for = trial_by_item_index == presentation[i_and_j[repeated_item][0]]\n            for k in range(1, repetition_count):\n                look_for = np.logical_or(\n                    look_for, trial_by_item_index == presentation[i_and_j[repeated_item][k]])\n            recall_positions = np.where(look_for)[0]\n\n            if np.size(recall_positions) == 0:\n                continue\n\n            # check each position the item was observed (always just 1 position; we loop for parallelism w control)\n            for recall_position in recall_positions:\n\n                # also skip if no successive recall was made, \n                if np.size(trial_by_item_index) == recall_position + 1:\n                    continue\n\n                # build list of study positions for items recalled up to repeated item\n                prior_lags = [[] for each in range(repetition_count)]\n                for i in range(recall_position):\n\n                    # if considered item is also repeated, we track lags wrt to all presentations\n                    if trial_by_item_index[i] in i_and_j:\n                        for considered in range(len(i_and_j[trial_by_item_index[i]])):\n                            for focal in range(repetition_count):\n                                prior_lags[focal].append(\n                                    int(i_and_j[trial_by_item_index[i]][considered] - i_and_j[repeated_item][focal]))\n                    else:\n                        for k in range(repetition_count):\n                            prior_lags[k].append(int(trial_by_study_position[i] - i_and_j[repeated_item][k]))\n\n                # transition of a given lag is possible if lag not present in prior_lags\n                for lag in relevant_lags:\n                    for k in range(repetition_count):\n                        \n                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][-1] < lag_threshold\n                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                            continue\n                        \n                        if lag not in prior_lags[k]:\n                            possible_lags[k][relevant_lags.index(lag)] += 1\n\n                # track each serial lag of actually transitioned-to item\n                if trial_by_item_index[recall_position+1] in i_and_j:\n                    positions = i_and_j[trial_by_item_index[recall_position+1]]\n                    for transition_study_position in positions:\n                        for k in range(repetition_count):\n\n                            # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold\n                            if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                                continue\n\n                            lag = int(transition_study_position - i_and_j[repeated_item][k])\n                            if lag in relevant_lags:\n                                actual_lags[k][relevant_lags.index(lag)] += 1\n                else:\n                    transition_study_position = trial_by_study_position[recall_position+1]-1\n                    for k in range(repetition_count):\n\n                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold\n                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                            continue\n\n                        lag = int(transition_study_position-i_and_j[repeated_item][k])\n                        if lag in relevant_lags:\n                            actual_lags[k][relevant_lags.index(lag)] += 1\n\n    result = []\n    for k in range(repetition_count):\n        \n        for i in range(len(possible_lags[k])):\n            if possible_lags[k][i] == 0:\n                possible_lags[k][i] += 1\n\n        result.append(pd.DataFrame(\n            {'lag': relevant_lags, 'prob': np.divide(actual_lags[k], possible_lags[k]), \n            'actual': actual_lags[k], 'possible': possible_lags[k]}))\n\n    return pd.concat(result, keys=['From Position {}'.format(i+1) for i in range(repetition_count)], names=['locus'])\n# export\n\nfrom numba import njit\nfrom numba import int32\nimport numpy as np\n\n@njit(nogil=True)\ndef alternative_contiguity(trials, presentations, lag_threshold = 3, max_repeats = 2):\n    \n    list_length = len(presentations[0])\n    lag_range = list_length - 1\n    total_actual_lags = np.zeros((max_repeats, lag_range * 2 + 1)) # extended dimension to split by pres positions\n    total_possible_lags = np.zeros((max_repeats, lag_range * 2 + 1))\n    terminus = np.sum(trials != 0, axis=1) # number of recalls per trial\n    recall_by_study_position = recall_by_all_study_positions(trials, presentations, max_repeats)\n    \n    for trial_index in range(len(trials)):\n        previous_item = 0\n        item_count = np.max(presentations[trial_index]) + 1\n        possible_items = np.arange(item_count) # initial pool of possible recalls, 1-indexed\n        possible_positions = np.zeros((item_count, max_repeats), dtype=int32)\n        \n        # we track possible positions using presentations and alt_presentations\n        for item in range(item_count):\n            pos = np.nonzero(presentations[trial_index] == item)[0] + 1\n            possible_positions[item, :len(pos)] = pos\n            \n        for recall_index in range(terminus[trial_index]):\n\n            current_item = presentations[trial_index][trials[trial_index, recall_index]-1]\n            \n            # track possible and actual lags; \n            # focus only on transitions from items with > 1 study positions\n            # and only when those multiple study positions have lag over lag\n            if recall_index > 0 and np.count_nonzero(\n                possible_positions[previous_item]) > 1 and (\n                possible_positions[previous_item][1] - possible_positions[previous_item][0] >= lag_threshold):\n                \n                # item indices don't help track lags anymore\n                # so more complex calculation needed to identify possible lags given previous item\n                current_index = np.nonzero(possible_items==current_item)[0]\n\n                index = 0\n                for x in range(len(recall_by_study_position)):\n                    for y in range(len(recall_by_study_position)):\n                        if possible_positions[previous_item, y] > 0:\n                        \n                            possible_lags = possible_positions[\n                                possible_items, x] - possible_positions[previous_item, y]\n                            \n                            # if tracked position is 0, then we don't actually want to count it in our lags\n                            possible_lags[possible_positions[possible_items, x] == 0] = 0\n                            \n                            # we track actual lag at each iteration\n                            actual_lag = possible_lags[current_index] + lag_range\n                            total_actual_lags[y][actual_lag] += 1\n\n                            # we track possible lag at each iteration\n                            possible_lags += lag_range\n                            total_possible_lags[y][possible_lags] += 1\n                        \n                        index += 1\n\n            # update pool to exclude recalled item (updated to still identify 1-indexed item)\n            previous_item = current_item\n            possible_items = possible_items[possible_items != previous_item]\n\n                                \n    # small correction to avoid nans and commit to excluding multiply-tracked single presentations \n    total_actual_lags[:, lag_range] = 0\n    for i in range(max_repeats):\n        total_possible_lags[i][total_actual_lags[i]==0] += 1\n    \n    return total_actual_lags/total_possible_lags\n\n#selection = list_types == 4\n#alternative_contiguity(trials[selection], presentations[selection], 6, 2)\n\n\nindividual_results = []\n\nfor subject in np.unique(subjects):\n    selection = np.logical_and(list_types == 4, subjects == subject)\n\n    individual_results.append(\n        alternative_contiguity(trials[selection], presentations[selection], 6, 2))\n\nnp.mean(np.array(individual_results), axis=0)\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.00952381, 0.00714286,\n        0.02357143, 0.00571429, 0.01285714, 0.01428571, 0.03505495,\n        0.00571429, 0.0155102 , 0.04619048, 0.00505495, 0.01569264,\n        0.03380952, 0.02346939, 0.02984127, 0.03125953, 0.02119417,\n        0.03478997, 0.02337276, 0.02438032, 0.04757342, 0.04654344,\n        0.03749433, 0.07345434, 0.06505376, 0.14134409, 0.        ,\n        0.17722157, 0.1074913 , 0.05463955, 0.06811201, 0.0654523 ,\n        0.07416891, 0.07620522, 0.06610723, 0.05367311, 0.05347731,\n        0.04190581, 0.02654247, 0.01420171, 0.02639528, 0.02579365,\n        0.01405699, 0.01496599, 0.01760597, 0.02278912, 0.01939549,\n        0.02042735, 0.01525903, 0.02315018, 0.00952381, 0.01360544,\n        0.02952381, 0.03979592, 0.0152381 , 0.02579365, 0.02380952,\n        0.00714286, 0.03428571, 0.01047619, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.00952381,\n        0.        , 0.        , 0.03979592, 0.00571429, 0.00571429,\n        0.0152381 , 0.02      , 0.02595238, 0.01598639, 0.04357143,\n        0.0052381 , 0.03107535, 0.00505495, 0.03098639, 0.03386724,\n        0.02527211, 0.03294539, 0.02006803, 0.00626566, 0.04196068,\n        0.03441261, 0.03458128, 0.05680602, 0.05606956, 0.06942044,\n        0.08609565, 0.15021033, 0.17164558, 0.09036054, 0.0704922 ,\n        0.07565847, 0.06124401, 0.07097237, 0.10146722, 0.        ,\n        0.09013582, 0.02819657, 0.04635616, 0.03943094, 0.03044437,\n        0.02622808, 0.01441877, 0.01373234, 0.02223182, 0.02584003,\n        0.01337868, 0.01845456, 0.01053391, 0.02727891, 0.01857928,\n        0.01658312, 0.01587302, 0.00408163, 0.05571429, 0.02142857,\n        0.02238095, 0.01071429, 0.02142857, 0.01666667, 0.        ,\n        0.        , 0.02      , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        ]])"
  },
  {
    "objectID": "library\\analyses\\Alternative_Contiguity.html#plotting",
    "href": "library\\analyses\\Alternative_Contiguity.html#plotting",
    "title": "compmemlearn",
    "section": "Plotting",
    "text": "minimum_lag = 6\nrepetition_count = 2\n\nindividual_results = []\n\nfor subject in np.unique(subjects):\n    selection = np.logical_and(list_types == 4, subjects == subject)\n\n    individual_results.append(\n        alternative_contiguity_test(presentations[selection], trials[selection], minimum_lag, repetition_count))\n\ndf = pd.concat(individual_results, keys=np.unique(subjects), names=['subject']).reset_index()\ndf.drop(['level_2'], axis=1, inplace=True)\ndf\n\n\n\n\n  \n    \n      \n      subject\n      locus\n      lag\n      prob\n      actual\n      possible\n    \n  \n  \n    \n      0\n      1\n      From Position 1\n      -3\n      0.000000\n      0\n      27\n    \n    \n      1\n      1\n      From Position 1\n      -2\n      0.115385\n      3\n      26\n    \n    \n      2\n      1\n      From Position 1\n      -1\n      0.153846\n      4\n      26\n    \n    \n      3\n      1\n      From Position 1\n      1\n      0.200000\n      8\n      40\n    \n    \n      4\n      1\n      From Position 1\n      2\n      0.314286\n      11\n      35\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      415\n      37\n      From Position 2\n      -2\n      0.125000\n      1\n      8\n    \n    \n      416\n      37\n      From Position 2\n      -1\n      0.142857\n      1\n      7\n    \n    \n      417\n      37\n      From Position 2\n      1\n      0.333333\n      2\n      6\n    \n    \n      418\n      37\n      From Position 2\n      2\n      0.000000\n      0\n      9\n    \n    \n      419\n      37\n      From Position 2\n      3\n      0.111111\n      1\n      9\n    \n  \n\n420 rows × 6 columns\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmax_lag = minimum_lag/2\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\ng = sns.FacetGrid(df, height=4.5)\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_neg), x='lag', y='prob', hue='locus', err_style='bars', **kws)\n)\n\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_pos), x='lag', y='prob', hue='locus', err_style='bars', **kws)\n)\n\nplt.legend(['First Presentation', 'Second Presentation'], loc='upper left')\nplt.title('Alternative Contiguity Analysis\\n(Minimum Lag Between Repetitions = {})'.format(minimum_lag))\nplt.tight_layout()\n\n\n\n\n\n%timeit alternative_contiguity(trials[selection], presentations[selection], 6, 2)\n\n4.14 ms ± 69.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "library\\analyses\\Conditional_Stop_Probability.html#data-preparation",
    "href": "library\\analyses\\Conditional_Stop_Probability.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "For our demonstrations, we’ll lean on the MurdockOkada1970 dataset. As a reminder, in this dataset each of 72 undergraduates was given 20 test lists with 20-word lists visually presented at either 60 or 120 words/min.\n\nfrom compmemlearn.datasets import prepare_murdock1970_data\n\ntrials, events, list_length = prepare_murdock1970_data('../../data/mo1970.txt')\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\analyses\\Conditional_Stop_Probability.html#analysis",
    "href": "library\\analyses\\Conditional_Stop_Probability.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "Fast Array Generation\nDataFrames contain granular subject-by-subject information and are easier to plot using the seaborn library. But sometimes we don’t need this granular information and mainly want to perform our analysis as quickly as possible – perhaps to help with model fitting or analysis. In that case, representing results with numpy arrays and performing just-in-time compilation of our function using numba might be preferred. We include analyses.fast_csp in our library for that purpose here.\n# export\n\nfrom numba import njit\nimport numpy as np\n\n@njit(fastmath=True, nogil=True)\ndef fast_csp(trials, item_count):\n    \n    # numerator is number of trials with zero in current column position\n    numerator = np.zeros(item_count+1)\n\n    # denominator is number of trials without zero in previous column positions\n    denominator = np.zeros(item_count+1)\n\n    stop_positions = trials == 0\n    for i in range(len(trials)):\n\n        # add 1 to index of final recall\n        numerator[np.argmax(stop_positions[i])] += 1\n        \n        # add 1 to each index up through final recall\n        denominator[:np.argmax(stop_positions[i])+1] += 1\n        \n    denominator[denominator==0] += 1\n    return numerator/denominator\n\nfast_csp(trials, list_length)\n\narray([0.        , 0.00138889, 0.01877608, 0.05740609, 0.18195489,\n       0.25919118, 0.33498759, 0.4608209 , 0.50519031, 0.55944056,\n       0.65079365, 0.77272727, 1.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        ])\n\n\n\n\nDataFrame\nThe psifr library doesn’t have a function to generate pandas DataFrames containing conditional stop probability information, so we make our own. For efficiency, it mainly consists of calls to fast_csp.\nimport pandas as pd\n\ndef csp(events, trials):\n    subjects = len(np.unique(events.subject))\n    trial_count = np.max(events.list)\n    list_length = np.max(events.input)\n    \n    result = {'subject': [], 'output': [], 'prob': []}\n    \n    for subject in range(subjects):\n        subject_result = fast_csp(trials[subject*trial_count:(subject+1)*trial_count], list_length)\n        subject_result[np.argmax(subject_result)+1:] = np.nan\n\n        result['subject'] += [subject+1]*(list_length+1)\n        result['output'] += np.arange(list_length+1, dtype=int).tolist()\n        result['prob'] += subject_result.tolist()\n        \n    return pd.DataFrame(result)\n\ncsp(events, trials)\n\n\n\n\n  \n    \n      \n      subject\n      output\n      prob\n    \n  \n  \n    \n      0\n      1\n      0\n      0.0\n    \n    \n      1\n      1\n      1\n      0.0\n    \n    \n      2\n      1\n      2\n      0.0\n    \n    \n      3\n      1\n      3\n      0.0\n    \n    \n      4\n      1\n      4\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1507\n      72\n      16\n      NaN\n    \n    \n      1508\n      72\n      17\n      NaN\n    \n    \n      1509\n      72\n      18\n      NaN\n    \n    \n      1510\n      72\n      19\n      NaN\n    \n    \n      1511\n      72\n      20\n      NaN\n    \n  \n\n1512 rows × 3 columns\n\n\n\nWe can compare the runtimes of our implementations using the %%timeit Jupyter magic:\n\n%%timeit\nfast_csp(trials, list_length)\n\n210 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n%%timeit\ncsp(events, trials)\n\n2.53 ms ± 156 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nfast_csp is slower than our serial position effect functions, but faster than our functions analyzing lag-contiguity."
  },
  {
    "objectID": "library\\analyses\\Conditional_Stop_Probability.html#plotting",
    "href": "library\\analyses\\Conditional_Stop_Probability.html#plotting",
    "title": "compmemlearn",
    "section": "Plotting",
    "text": "We’ll define a seaborn-based plotting function that uses our csp function.\n# export\nimport seaborn as sns\nfrom psifr import fr\n\ndef plot_csp(data, **facet_kws):\n    \n    trials = pd.pivot_table(\n        data, index=['subject', 'list'], \n        columns=['output'], values='input', \n        fill_value=0).to_numpy()\n\n    sns.lineplot(\n        data=csp(data, trials), \n        x='output', y='prob', **facet_kws)\n\nsns.set_theme(style=\"darkgrid\")\n\ng = sns.FacetGrid(dropna=False, data=events, height=4, aspect=1.2)\ng.map_dataframe(plot_csp, err_style='bars')\n\ng.set_xlabels('Recall Position')\ng.set_ylabels('Conditional Stop Probability');"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_Effect.html#data-preparation",
    "href": "library\\analyses\\Lag_Contiguity_Effect.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "For our demonstrations, we’ll lean on the MurdockOkada1970 dataset. As a reminder, in this dataset each of 72 undergraduates was given 20 test lists with 20-word lists visually presented at either 60 or 120 words/min.\n\nfrom compmemlearn.datasets import prepare_murdock1970_data\n\ntrials, events, list_length = prepare_murdock1970_data('../../data/mo1970.txt')\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_Effect.html#analysis",
    "href": "library\\analyses\\Lag_Contiguity_Effect.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "DataFrame\nWe can usually rely on the psifr library to generate pandas DataFrames containing lag-CRP information.\n\nfrom psifr import fr\n\ncrp = fr.lag_crp(events)\ncrp\n\n\n\n\n  \n    \n      \n      \n      prob\n      actual\n      possible\n    \n    \n      subject\n      lag\n      \n      \n      \n    \n  \n  \n    \n      1\n      -19\n      0.166667\n      2\n      12\n    \n    \n      -18\n      0.083333\n      2\n      24\n    \n    \n      -17\n      0.025641\n      1\n      39\n    \n    \n      -16\n      0.019608\n      1\n      51\n    \n    \n      -15\n      0.030303\n      2\n      66\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      72\n      15\n      0.000000\n      0\n      12\n    \n    \n      16\n      0.000000\n      0\n      3\n    \n    \n      17\n      0.000000\n      0\n      4\n    \n    \n      18\n      0.000000\n      0\n      1\n    \n    \n      19\n      NaN\n      0\n      0\n    \n  \n\n2808 rows × 3 columns\n\n\n\n\n\nFast Array-Based Generation\nDataFrames contain granular subject-by-subject information and are easier to plot using the seaborn library. But sometimes we don’t need this granular information and mainly want to perform our analysis as quickly as possible – perhaps to help with model fitting or analysis. In that case, representing results with numpy arrays and performing just-in-time compilation of our function using numba might be preferred. We include analyses.fast_crp in our library for that purpose here.\n# export\n\nfrom numba import njit\nimport numpy as np\n\n@njit(fastmath=True, nogil=True)\ndef fast_crp(trials, item_count):\n    \n    lag_range = item_count - 1\n    total_actual_lags = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    terminus = np.sum(trials != 0, axis=1)\n    \n    # compute actual serial lag b/t recalls\n    actual_lags = trials[:, 1:] - trials[:, :-1]\n    actual_lags += lag_range\n    \n    # tabulate bin totals for actual and possible lags\n    for i in range(len(trials)):\n        possible_items = np.arange(item_count) + 1\n        previous_item = 0\n        \n        for recall_index in range(terminus[i]):\n            \n            # track possible and actual lags\n            if recall_index > 0:\n                total_actual_lags[actual_lags[i, recall_index-1]] += 1\n                \n                # exploit equivalence b/t item index and study position to track possible lags\n                possible_lags = possible_items - previous_item \n                possible_lags += lag_range\n                total_possible_lags[possible_lags] += 1\n                \n            # update pool of possible items to exclude recalled item\n            previous_item = trials[i, recall_index]\n            possible_items = possible_items[possible_items != previous_item]\n    \n    # small correction to avoid nans\n    total_possible_lags[total_actual_lags==0] += 1\n    \n    return total_actual_lags/total_possible_lags\n\nfast_crp(trials, list_length)\n\narray([0.11545802, 0.03865462, 0.03426124, 0.02190876, 0.01986577,\n       0.02267685, 0.02739411, 0.02432077, 0.02417998, 0.02586558,\n       0.02197371, 0.02486135, 0.03163017, 0.03506209, 0.03603604,\n       0.04045307, 0.04037383, 0.06735022, 0.12302405, 0.        ,\n       0.42455483, 0.12300786, 0.0701565 , 0.04939064, 0.04814004,\n       0.04378356, 0.03933364, 0.03939865, 0.03775744, 0.04248573,\n       0.03664553, 0.0403481 , 0.02867384, 0.02528978, 0.03629537,\n       0.03250774, 0.04587156, 0.06060606, 0.02366864])\n\n\nWe can compare the runtimes of compmemlearn’s analyses.fast_crp and psifr’s fr.lag_crp using the %%timeit Jupyter magic:\n\n%%timeit\nfast_crp(trials, 20)\n\n5.65 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\nfr.lag_crp(events)\n\n470 ms ± 7.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nOur fast implementation is more than 100 times faster!"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_Effect.html#plotting",
    "href": "library\\analyses\\Lag_Contiguity_Effect.html#plotting",
    "title": "compmemlearn",
    "section": "Plotting",
    "text": "psifr’s plotting library creates a separate figure for each plot, when sometimes we want to to include multiple plots in one figure, so we define our own.\n# export\nimport seaborn as sns\nfrom psifr import fr\n\ndef plot_lag_crp(data, max_lag=5, **facet_kws):\n    \n    filt_neg = f'{-max_lag} <= lag < 0'\n    filt_pos = f'0 < lag <= {max_lag}'\n    \n    crp_data = fr.lag_crp(data)\n    \n    sns.lineplot(\n        data=crp_data.query(filt_neg), \n        x='lag', y='prob', **facet_kws)\n    sns.lineplot(\n        data=crp_data.query(filt_pos), \n        x='lag', y='prob', **facet_kws)\n\nsns.set_theme(style=\"darkgrid\")\n\ng = sns.FacetGrid(dropna=False, data=events, height=4, aspect=1.2)\ng.map_dataframe(plot_lag_crp, err_style='bars')\ng.set(ylim=(0, .5))\n\ng.set_xlabels('Item\\'s Lag In Study List From Last Recalled Item')\ng.set_ylabels('Conditional Recall Rate');"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#contrast-with-stochastic-approach-demoed-in-lohnas-kahana-2014",
    "href": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#contrast-with-stochastic-approach-demoed-in-lohnas-kahana-2014",
    "title": "compmemlearn",
    "section": "Contrast With Stochastic Approach Demoed in Lohnas & Kahana (2014)",
    "text": "For lists containing repeated spaced items, the lags for such an item is ambiguous. For instance, the transition between an item presented in serial position 5 to an item presented in serial positions 3 and 9 could be considered a lag of −2 or +4.\n\nI kind of disagree with the specifics of this framework. The lags are not ambiguous – we know what they are. Rather, they are numerous, and to account for that we can just track each lag in our analysis.\n\nFor any transition with an ambiguous lag (i.e. a transition including a repeated item), we randomly selected the lag value from the set of possible lags.\n\nThe lag-CRP is a ratio of lag transitions that were made over transitions that could have been made across each output in a dataset. However, for lists containing repeated spaced items, the lags for such an item is numerous. For instance, the transition between an item presented in serial position 5 to an item presented in serial positions 3 and 9 is both a lag of −2 and of +4. In our tally of actual and possible lag transitions, we accordingly count both these values. Relating this approach to the example above, a lag transition of -2 and of +4 is possible (and actual) for the considered output.\nRandomly selecting the lag value from the set of possible lags for each analysis and aggregating over multiple replicates will (probably) converge to this result. However, that approach is imprecise, nondeterministic, and takes longer to run."
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#data-preparation",
    "href": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#function",
    "href": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#function",
    "title": "compmemlearn",
    "section": "Function",
    "text": "That recalled items can have two study positions forces us to complicate the function. Along with the scenario where the current and the previous item have just one relevant serial lag to consider, we must also consider the case where there are two serial lags to consider when either the current item or the previous item had two serial positions. Then there’s one more scenario – when both the the current and the previous item were encountered twice during encoding. This corresponds with four serial lags to consider.\nHow do we weigh these? The approach in the SPC was just to interpret the description of the analysis literally. In the SPC, we tracked the probability of recalling an item given that it was studied at each selected position – even if it was also studied at some other position. Here, we’ll do the same thing, except using lag instead of study position. Extensions of this analysis can exclude items on chosen bases, but this gets us a solid general foundation.\nTo achieve this, we have to change how we track lag lag across trials (in actual) so that lags can occur with multiple lags simultaneously.\nWe must also change how we track changes to the pool of possible lag lags. This probably requires adding some representation of items that have positions as attributes and are looped across to identify relevant lags.\nprevious_recall can identify those items.\nfrom compmemlearn.analyses import recall_by_second_study_position, recall_by_all_study_positions\n# export\n\nfrom numba import njit\nfrom numba import int32\nimport numpy as np\nfrom compmemlearn.datasets import find_first\n\n\n@njit(nogil=True)\ndef fast_mixed_crp(trials, presentations):\n    \n    list_length = len(presentations[0])\n    lag_range = list_length - 1\n    total_actual_lags = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    terminus = np.sum(trials != 0, axis=1) # number of recalls per trial\n    \n    # compute actual serial lag b/t recalls, considering all possible positions\n    alt_presentations = np.fliplr(presentations)\n    alt_trials = recall_by_second_study_position(trials, presentations)\n    actual_lags = np.zeros((4, len(trials), len(trials[0])-1), dtype=int32)\n    actual_lags[0] = trials[:, 1:] - trials[:, :-1]\n    actual_lags[1] = trials[:, 1:] - alt_trials[:, :-1]\n    actual_lags[2] = alt_trials[:, 1:] - trials[:, :-1]\n    actual_lags[3] = alt_trials[:, 1:] - alt_trials[:, :-1]\n    \n    # if actual[0, x, y] == actual[2, x, y] (or 1, 3) then lagged-to item has 1 presentation\n    # if actual[0, x, y] == actual[1, x, y] (or 2, 3 respectively) then lagged-from item has 1 pres\n    # avoid counting single presentations twice by giving those lags a value of 0\n    previous_item_equivalence = actual_lags[0] == actual_lags[2]\n    current_item_equivalence = actual_lags[0] == actual_lags[1]\n    either_item_equivalence = np.logical_or(previous_item_equivalence, current_item_equivalence)\n\n    for i in range(len(actual_lags[0])):\n        actual_lags[1, i][current_item_equivalence[i]] = 0\n        actual_lags[2, i][previous_item_equivalence[i]] = 0\n        actual_lags[3, i][either_item_equivalence[i]] = 0\n        \n    # we add lag_range to have result identify indices in total_ to bin counts\n    actual_lags += lag_range\n    \n    for trial_index in range(len(trials)):\n        \n        previous_item = 0\n        item_count = np.max(presentations[trial_index]) + 1\n        possible_items = np.arange(item_count) # initial pool of possible recalls, 1-indexed\n        possible_positions = np.zeros((item_count, 2))\n        \n        # we track possible positions using presentations and alt_presentations\n        for item in range(item_count):\n            possible_positions[item, 0] = find_first(item, presentations[trial_index])\n            possible_positions[item, 1] = list_length - find_first(item, alt_presentations[trial_index]) -1\n        \n        for recall_index in range(terminus[trial_index]):\n            \n            # track possible and actual lags\n            if recall_index > 0:\n\n                # we add to actual_lags total for each lag transition made for this recall\n                total_actual_lags[actual_lags[:, trial_index, recall_index-1]] += 1\n                \n                # item indices don't help track lags anymore\n                # so more complex calculation needed to identify possible lags given previous item\n                possible_lags = np.zeros((4, len(possible_items)), dtype=int32)\n                possible_lags[0] = possible_positions[possible_items, 0] - possible_positions[previous_item, 0]\n                possible_lags[1] = possible_positions[possible_items, 0] - possible_positions[previous_item, 1]\n                possible_lags[2] = possible_positions[possible_items, 1] - possible_positions[previous_item, 0]\n                possible_lags[3] = possible_positions[possible_items, 1] - possible_positions[previous_item, 1]\n                \n                # avoid redundant counting of single presentations\n                previous_item_equivalence = possible_lags[0] == possible_lags[2]\n                current_item_equivalence = possible_lags[0] == possible_lags[1]\n                either_item_equivalence = np.logical_or(previous_item_equivalence, current_item_equivalence)\n                possible_lags[1][current_item_equivalence] = 0\n                possible_lags[2][previous_item_equivalence] = 0\n                possible_lags[3][either_item_equivalence] = 0\n    \n                possible_lags += lag_range\n                total_possible_lags[possible_lags.flatten()] += 1\n                \n            # update pool to exclude recalled item (updated to still identify 1-indexed item)\n            previous_item = presentations[trial_index][trials[trial_index, recall_index]-1]\n            possible_items = possible_items[possible_items != previous_item]\n    \n    # small correction to avoid nans and commit to excluding multiply-tracked single presentations \n    total_actual_lags[lag_range] = 0\n    total_possible_lags[total_actual_lags==0] += 1\n    \n    return total_actual_lags/total_possible_lags\n# export\n\n@njit(nogil=True)\ndef flex_mixed_crp(trials, presentations, max_repeats=2):\n    \n    list_length = len(presentations[0])\n    lag_range = list_length - 1\n    total_actual_lags = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    terminus = np.sum(trials != 0, axis=1) # number of recalls per trial\n    recall_by_study_position = recall_by_all_study_positions(trials, presentations, max_repeats)\n    \n    for trial_index in range(len(trials)):\n        \n        previous_item = 0\n        item_count = np.max(presentations[trial_index]) + 1\n        possible_items = np.arange(item_count) # initial pool of possible recalls, 1-indexed\n        possible_positions = np.zeros((item_count, max_repeats), dtype=int32)\n        \n        # we track possible positions using presentations and alt_presentations\n        for item in range(item_count):\n            pos = np.nonzero(presentations[trial_index] == item)[0] + 1\n            possible_positions[item, :len(pos)] = pos\n            \n        for recall_index in range(terminus[trial_index]):\n            \n            current_item = presentations[trial_index][trials[trial_index, recall_index]-1]\n            \n            # track possible and actual lags\n            if recall_index > 0:\n                \n                # item indices don't help track lags anymore\n                # so more complex calculation needed to identify possible lags given previous item\n                current_index = np.nonzero(possible_items==current_item)[0]\n                possible_lags = np.zeros((len(recall_by_study_position) ** 2, len(possible_items)), dtype=int32)\n\n                index = 0\n                for x in range(len(recall_by_study_position)):\n                    for y in range(len(recall_by_study_position)):\n                        if possible_positions[previous_item, y] > 0:\n                        \n                            possible_lags[index] = possible_positions[\n                                possible_items, x] - possible_positions[previous_item, y]\n                            \n                            # if tracked position is 0, then we don't actually want to count it in our lags\n                            possible_lags[index][possible_positions[possible_items, x] == 0] = 0\n                        \n                        index += 1\n\n                possible_lags += lag_range\n                total_actual_lags[possible_lags[:, current_index].flatten()] += 1\n                total_possible_lags[possible_lags.flatten()] += 1\n                        \n\n            # update pool to exclude recalled item (updated to still identify 1-indexed item)\n            previous_item = current_item\n            possible_items = possible_items[possible_items != previous_item]\n                    \n    # small correction to avoid nans and commit to excluding multiply-tracked single presentations \n    total_actual_lags[lag_range] = 0\n    total_possible_lags[total_actual_lags==0] += 1\n    \n    return total_actual_lags/total_possible_lags"
  },
  {
    "objectID": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#demo",
    "href": "library\\analyses\\Lag_Contiguity_with_Repetition_Data.html#demo",
    "title": "compmemlearn",
    "section": "Demo",
    "text": "Comparison With Regular fast_csp Using Control Lists\n\nfrom compmemlearn.analyses import fast_crp\n\nfast_crp(trials[list_types==1], 40) == fast_mixed_crp(trials[list_types==1], presentations[list_types==1])\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True])\n\n\n\n%timeit fast_mixed_crp(trials[list_types==1], presentations[list_types==1])\n\n15.7 ms ± 440 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit fast_crp(trials[list_types==1], 40)\n\n3.25 ms ± 40.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit flex_mixed_crp(trials[list_types==1], presentations[list_types==1])\n\n20.9 ms ± 336 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ncondition = 4\nflex_mixed_crp(trials[list_types==condition], presentations[list_types==condition]) == fast_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True])\n\n\n\nflex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\n\narray([0.04545455, 0.07741935, 0.02702703, 0.03448276, 0.01239669,\n       0.01867572, 0.01267606, 0.0255164 , 0.0311493 , 0.02812803,\n       0.02410714, 0.02390767, 0.02356638, 0.01918819, 0.02116041,\n       0.02271252, 0.02799757, 0.02669762, 0.0268757 , 0.0295858 ,\n       0.02862883, 0.02987952, 0.02465116, 0.0199027 , 0.02997859,\n       0.02920609, 0.03033885, 0.03862333, 0.04053552, 0.04422869,\n       0.04406539, 0.05585831, 0.0589584 , 0.05534106, 0.0639848 ,\n       0.06148171, 0.08683975, 0.09879092, 0.17528821, 0.        ,\n       0.28192131, 0.12563791, 0.08370591, 0.06432161, 0.06537967,\n       0.05540897, 0.05018991, 0.04840941, 0.03874644, 0.0437536 ,\n       0.03453365, 0.03611195, 0.0264234 , 0.03119922, 0.0254065 ,\n       0.01907074, 0.02382671, 0.02185995, 0.02243714, 0.01709742,\n       0.01704782, 0.01742919, 0.0191344 , 0.01429933, 0.01544594,\n       0.01411396, 0.02090209, 0.01302546, 0.01519949, 0.01491525,\n       0.01538462, 0.01579779, 0.0123131 , 0.01383399, 0.01030928,\n       0.00693481, 0.01202749, 0.01428571, 0.01298701])\n\n\n\nimport matplotlib.pyplot as plt\n\nfor condition in range(4):\n    test_crp= fast_mixed_crp(trials[list_types==condition+1], presentations[list_types==condition+1])\n    test_crp[len(presentations[0])-1] = np.nan\n    plt.plot(np.arange(len(test_crp)), test_crp, label=str(condition+1))\n    plt.xticks(np.arange(0, len(test_crp), 4), np.arange(0, len(test_crp), 4) - 39)\n    \nplt.legend()\n\n<matplotlib.legend.Legend at 0x1f0e28a8070>"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#controlling-for-serial-position",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#controlling-for-serial-position",
    "title": "compmemlearn",
    "section": "Controlling for Serial Position",
    "text": "To control for serial position effects of repeated items, analyses are often performed on randomized recall sequences with recall probabilities matched to the serial position curve of controls lists. Recall of each item is then calculated randomly and independently across trials.\n# export\n\nfrom numba import njit, prange\nimport numpy as np\n\n@njit(parallel=True)\ndef randomize_dataset(recall_rate_by_serial_position, sample_size):\n    \"\"\"\n    To control for serial position effects of repeated items, \n    analyses are often performed on randomized recall sequences with recall probabilities \n    matched to the serial position curve of controls lists. Recall of each item is then \n    calculated randomly and independently across trials. \n    \"\"\"\n\n    samples = np.random.rand(sample_size, len(recall_rate_by_serial_position)) < recall_rate_by_serial_position\n    result = np.zeros((sample_size, len(recall_rate_by_serial_position)))\n\n    for i in prange(sample_size):\n        sample = samples[i].nonzero()[0] + 1\n        np.random.shuffle(sample)\n        result[i, :len(sample)] = sample\n\n    return result\nrecall_rates = events.pivot_table(index='input', values='recall').to_numpy().flatten()\n\nrandomize_dataset(recall_rates, 10000000)"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#recall-probability-by-lag",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#recall-probability-by-lag",
    "title": "compmemlearn",
    "section": "Recall Probability by Lag",
    "text": "NOTE: this version is deprecated and only maintained to aid updating the codebase; see devoted notebook “recall probability by spacing” for the much (>5x) faster replacement\nfrom numba import njit\nimport numpy as np\n\n@njit(fastmath=True, nogil=True)\ndef recall_probability_by_lag(presentations, study_positions_in_recall_order, experiment_count=1):\n\n    presented, retrieved = np.zeros(5), np.zeros(5)\n\n    for trial_index, sequence in enumerate(presentations):\n\n        for experiment in range(experiment_count):\n            \n            # retrieve trial information\n            trial = study_positions_in_recall_order[trial_index*experiment_count + experiment]\n\n            # extract list of recalled items\n            recalled = sequence[trial-1][trial != 0]\n            \n\n            for item in np.unique(sequence):\n                locations = np.where(sequence == item)[0]\n\n                # presented just once\n                if len(locations) == 1:\n                    index = 0\n                else: \n\n                    # no intervening items (massed)\n                    lag = locations[1] - locations[0] - 1\n                    if lag == 0:\n                        index = 1\n\n                    # 1-2 intervening items\n                    elif lag <= 2:\n                        index = 2\n\n                    # 3-5\n                    elif lag <= 5:\n                        index = 3\n\n                    # 6-8\n                    else:\n                        index = 4\n\n                presented[index] += 1\n                retrieved[index] += item in recalled\n        \n    return retrieved, presented, retrieved/presented\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\nsns.set_theme(style=\"dark\")\ncondition = 4\nsource = 'Lohnas & Kahana (2014)'\n\nresult = recall_probability_by_lag(presentations[list_types>=condition], trials[list_types>=condition])[-1]\nprint(result)\nprint(result[1:] - result[:-1])\nax = sns.barplot(x=['N/A', '0', '1-2', '3-5', '6-8'], \n                 y=result)\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\nplt.title('Condition {}, {}'.format(condition, source))\nplt.show()\n\n[0.37278912 0.43928571 0.47678571 0.55714286 0.57857143]\n[0.0664966  0.0375     0.08035714 0.02142857]\n\n\n\n\n\nfrom compmemlearn.models import Classic_CMR\nfrom numba import int64\n\n@njit(fastmath=True, nogil=True)\ndef sim_recall_probability_by_lag(presentations, experiment_count, model_class, parameters):\n    \"\"\"\n    Apply organizational analyses to visually compare the behavior of the model \n    with these parameters against specified dataset.\n    \"\"\"\n    \n    max_lag = 8\n    total_ratio = np.zeros(max_lag+2)\n    total = 0\n    \n    # generate simulation data from model\n    for experiment in range(experiment_count):\n\n        sim = np.zeros(np.shape(presentations), dtype=int64) # set as int64 when using numba, int otherwise\n        for trial_index in range(len(presentations)):\n            presentation = presentations[trial_index]\n            \n            item_count = np.max(presentation)+1\n            model = model_class(item_count, len(presentation), parameters)\n\n            # simulate study events\n            model.experience(model.items[presentation])\n            \n            # simulate and collect sequence of recalled item indices\n            recalled = model.free_recall()\n            xsorted = np.argsort(presentation)\n            ypos = np.searchsorted(presentation[xsorted], recalled)\n            sim[trial_index, :len(recalled)] = xsorted[ypos]+1\n            \n        total_ratio += recall_probability_by_lag(presentations, sim)\n        total += 1\n        \n    return total_ratio/total"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#or-score",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#or-score",
    "title": "compmemlearn",
    "section": "OR Score",
    "text": "Given that the probability of recalling an item repeated in positions i and j is equivalent to the probability of recalling either the occurrence of the item in position i or the occurrence of the item in position j, contextual variability predicts that the probability of recalling either of two once-presented items in positions i and j, termed the OR score, should increase with their spacing |i - j|.\nTo control for serial position effects of repeated items, they determined OR scores from randomized recall sequences with recall probabilities matched to the serial position curve of controls lists. They subtracted the OR scores based on these randomized recall sequences from the observed OR scores, and for illustrative purposes added the mean randomized OR score at each lag.\n#TBD"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#rate-of-transition-between-items-that-follow-presentation-of-the-same-item",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#rate-of-transition-between-items-that-follow-presentation-of-the-same-item",
    "title": "compmemlearn",
    "section": "Rate of transition between items that follow presentation of the same item",
    "text": "They considered transitions between items following a shared repeated item. They calculated the proportion of those items recalled in \\(S_j = {j + 1, j + 2}\\) of which CMR then recalled an item in the set \\(S_i = {i + 1, i + 2}\\). They also calculated the proportion of recalls \\(S_i\\) of which CMR then transitioned to an item in the set \\(S_j\\). They calculated the proportion of transitions for each of lags \\(j - i >= 4\\), and represented the mean percent of transitions across these lags.\nTo estimate the proportion of transitions that CMR would make at these lags in the absence of repeated items, they considered transitions in control lists matched to the same serial positions considered in the mixed lists. They matched these serial positions to 100 random shuffles of the control lists, and took the mean across the reshuffled datasets.\n#TBD"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#how-do-values-of-zeta_s-configure-temporal-contiguity-with-secondary-item-presentations",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#how-do-values-of-zeta_s-configure-temporal-contiguity-with-secondary-item-presentations",
    "title": "compmemlearn",
    "section": "How Do Values of \\(\\zeta_s\\) Configure Temporal Contiguity With Secondary Item Presentations?",
    "text": "Weights in \\(M^{CF}\\) do not just configure the overall probability of recalling an item. Since the state of context depends on prior items encoded or retrieved and \\(M^{CF}\\) encodes context-to-item associations, differences in \\(M^{CF}\\) learning rates for presentations of an item as enforced by the differential encoding mechanism must also the rate of transition between different items during recall.\nIn particular, weaker learning for the second presentation of a repeated item compared to the first should enforce greater rate of transitions between the repeated item and other items presented near its first presentation.\n# export\n\nimport pandas as pd\nimport numpy as np\n\ndef indices_of_repeated_items(presentation_sequence):\n\n    values, counts = np.unique(presentation_sequence, return_counts=True)\n    repeated_items = {v: np.where(presentation_sequence == v)[0] for v in values if counts[v] > 1}\n\n    return {key:repeated_items[key] for key in repeated_items}\n\n    \ndef alternative_contiguity_test(mixed_presentations, mixed_recalls, lag_threshold, repetition_count):\n    relevant_lags = list(range(-int(lag_threshold/2), int(lag_threshold/2+1)))\n    del relevant_lags[int(lag_threshold/2)]\n\n    actual_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n    possible_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n\n    for trial_index in range(len(mixed_presentations)):\n\n        # sequence of item indices ordered as they were studied\n        presentation = mixed_presentations[trial_index]\n\n        # sequence of initial study positions ordered as they were recalled\n        trial_by_study_position = mixed_recalls[trial_index]\n\n        # sequence of item indices ordered as they were recalled\n        trial_by_item_index = presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]\n\n        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items\n        i_and_j = indices_of_repeated_items(presentation)\n        \n        # then for each unique repeated item in the study list, \n        for repeated_item in i_and_j:\n\n            # search for relevant item(s) in recall sequence and skip if not found \n            look_for = trial_by_item_index == presentation[i_and_j[repeated_item][0]]\n            for k in range(1, repetition_count):\n                look_for = np.logical_or(\n                    look_for, trial_by_item_index == presentation[i_and_j[repeated_item][k]])\n            recall_positions = np.where(look_for)[0]\n\n            if np.size(recall_positions) == 0:\n                continue\n\n            # check each position the item was observed (always just 1 position; we loop for parallelism w control)\n            for recall_position in recall_positions:\n\n                # also skip if no successive recall was made, \n                if np.size(trial_by_item_index) == recall_position + 1:\n                    continue\n\n                # build list of study positions for items recalled up to repeated item\n                prior_lags = [[] for each in range(repetition_count)]\n                for i in range(recall_position):\n\n                    # if considered item is also repeated, we track lags wrt to all presentations\n                    if trial_by_item_index[i] in i_and_j:\n                        for considered in range(len(i_and_j[trial_by_item_index[i]])):\n                            for focal in range(repetition_count):\n                                prior_lags[focal].append(\n                                    int(i_and_j[trial_by_item_index[i]][considered] - i_and_j[repeated_item][focal]))\n                    else:\n                        for k in range(repetition_count):\n                            prior_lags[k].append(int(trial_by_study_position[i] - i_and_j[repeated_item][k]))\n\n                # transition of a given lag is possible if lag not present in prior_lags\n                for lag in relevant_lags:\n                    for k in range(repetition_count):\n                        \n                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][-1] < lag_threshold\n                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                            continue\n                        \n                        if lag not in prior_lags[k]:\n                            possible_lags[k][relevant_lags.index(lag)] += 1\n\n                # track each serial lag of actually transitioned-to item\n                if trial_by_item_index[recall_position+1] in i_and_j:\n                    positions = i_and_j[trial_by_item_index[recall_position+1]]\n                    for transition_study_position in positions:\n                        for k in range(repetition_count):\n\n                            # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold\n                            if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                                continue\n\n                            lag = int(transition_study_position - i_and_j[repeated_item][k])\n                            if lag in relevant_lags:\n                                actual_lags[k][relevant_lags.index(lag)] += 1\n                else:\n                    transition_study_position = trial_by_study_position[recall_position+1]-1\n                    for k in range(repetition_count):\n\n                        # skip increment if i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold\n                        if (k > 0) and (i_and_j[repeated_item][k] - i_and_j[repeated_item][k-1] < lag_threshold):\n                            continue\n\n                        lag = int(transition_study_position-i_and_j[repeated_item][k])\n                        if lag in relevant_lags:\n                            actual_lags[k][relevant_lags.index(lag)] += 1\n\n    result = []\n    for k in range(repetition_count):\n        \n        for i in range(len(possible_lags[k])):\n            if possible_lags[k][i] == 0:\n                possible_lags[k][i] += 1\n\n        result.append(pd.DataFrame(\n            {'lag': relevant_lags, 'prob': np.divide(actual_lags[k], possible_lags[k]), \n            'actual': actual_lags[k], 'possible': possible_lags[k]}))\n\n    return pd.concat(result, keys=['From Position {}'.format(i+1) for i in range(repetition_count)], names=['locus'])\n\n\ndef alternative_contiguity_control(\n    mixed_presentations, control_presentations, control_recalls, lag_threshold, repetition_count):\n    relevant_lags = list(range(-int(lag_threshold/2), int(lag_threshold/2+1)))\n    del relevant_lags[int(lag_threshold/2)]\n\n    actual_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n    possible_lags = [[0 for each in relevant_lags] for i in range(repetition_count)]\n\n    for trial_index in range(len(mixed_presentations)):\n\n        # sequence of item indices ordered as they were studied\n        presentation = mixed_presentations[trial_index]\n        control_presentation = control_presentations[trial_index]\n\n        # sequence of initial study positions ordered as they were recalled\n        trial_by_study_position = control_recalls[trial_index]\n\n        # sequence of item indices ordered as they were recalled\n        trial_by_item_index = control_presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]\n\n        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items\n        i_and_j = indices_of_repeated_items(presentation)\n        \n        # then for each unique repeated item in the study list, \n        for repeated_item in i_and_j:\n\n            # search for relevant item(s) in recall sequence and skip if not found \n            look_for = trial_by_item_index == control_presentation[i_and_j[repeated_item][0]]\n            for k in range(1, repetition_count):\n                look_for = np.logical_or(\n                    look_for, trial_by_item_index == control_presentation[i_and_j[repeated_item][k]])\n            recall_positions = np.where(look_for)[0]\n\n            for recall_position in recall_positions:\n\n                # also skip if no successive recall was made, \n                if np.size(trial_by_item_index) == recall_position + 1:\n                    continue\n\n                # build list of study positions for items recalled up to repeated item\n                prior_lags = [[] for each in range(repetition_count)]\n                for i in range(recall_position):\n                    if trial_by_item_index[i] in i_and_j:\n                        for considered in range(len(i_and_j[trial_by_item_index[i]])):\n                            for focal in range(repetition_count):\n                                prior_lags[focal].append(\n                                    int(i_and_j[trial_by_item_index[i]][considered] - i_and_j[repeated_item][focal]))\n                    else:\n                        for k in range(repetition_count):\n                            prior_lags[k].append(int(trial_by_study_position[i] - i_and_j[repeated_item][k]))\n\n                # transition of a given lag is possible if lag not present in prior_lags\n                for lag in relevant_lags:\n                    for k in range(repetition_count):\n                        if lag not in prior_lags[k]:\n                            possible_lags[k][relevant_lags.index(lag)] += 1\n\n                # track each serial lag of actually transitioned-to item\n                if trial_by_item_index[recall_position+1] in i_and_j:\n                    positions = i_and_j[trial_by_item_index[recall_position+1]]\n                    for transition_study_position in positions:\n                        for k in range(repetition_count):\n                            lag = int(transition_study_position - i_and_j[repeated_item][k])\n                            if lag in relevant_lags:\n                                actual_lags[k][relevant_lags.index(lag)] += 1\n                else:\n                    transition_study_position = trial_by_study_position[recall_position+1]-1\n                    for k in range(repetition_count):\n                        lag = int(transition_study_position-i_and_j[repeated_item][k])\n                        if lag in relevant_lags:\n                            actual_lags[k][relevant_lags.index(lag)] += 1\n\n    result = []\n    for k in range(repetition_count):\n        result.append(pd.DataFrame(\n            {'lag': relevant_lags, 'prob': np.divide(actual_lags[k], possible_lags[k]), \n            'actual': actual_lags[k], 'possible': possible_lags[k]}))\n\n    return pd.concat(result, keys=['From Position {}'.format(i+1) for i in range(repetition_count)], names=['locus'])\n\ndef sim_alternative_contiguity_test(presentations, experiment_count, lag_threshold, repetition_count, encoding_drift_rate,         \n    start_drift_rate, recall_drift_rate, shared_support, item_support, learning_rate, \n    primacy_scale, primacy_decay, stop_probability_scale, \n    stop_probability_growth, choice_sensitivity, delay_drift_rate, \n    drift_familiarity_scale, mfc_familiarity_scale, mcf_familiarity_scale, sampling_rule):\n    \"\"\"\n    Apply organizational analyses to visually compare the behavior of the model \n    with these parameters against specified dataset.\n    \"\"\"\n\n    results = []\n    # generate simulation data from model\n    for experiment in range(experiment_count):\n\n        sim = np.zeros(np.shape(presentations), dtype=int)\n        for trial_index in range(len(presentations)):\n            presentation = presentations[trial_index]\n            \n            item_count = np.max(presentation)+1\n            model = Classic_CMR( item_count, len(presentations),  encoding_drift_rate,         \n                start_drift_rate, recall_drift_rate, shared_support, item_support, learning_rate, \n                primacy_scale, primacy_decay, stop_probability_scale, \n                stop_probability_growth, choice_sensitivity, delay_drift_rate, drift_familiarity_scale, \n                 mfc_familiarity_scale, mcf_familiarity_scale, sampling_rule)\n\n            # simulate study events\n            model.experience(np.eye(model.item_count, model.item_count)[presentation])\n            \n            # simulate and add recall events to trials array\n            recalled = model.free_recall()\n            xsorted = np.argsort(presentation)\n            ypos = np.searchsorted(presentation[xsorted], recalled)\n            sim[trial_index, :len(recalled)] = xsorted[ypos]+1\n\n        # apply contiguity test\n        results.append(alternative_contiguity_test(presentations, sim, lag_threshold, repetition_count))\n\n    return pd.concat(results, keys=list(range(experiment_count)), names=['experiment']).reset_index()\n\nminimum_lag = 6\nrepetition_count = 2\n\nindividual_results = []\n\nfor subject in np.unique(subjects):\n    selection = np.logical_and(list_types > 3, subjects == subject)\n\n    individual_results.append(\n        alternative_contiguity_test(presentations[selection], trials[selection], minimum_lag, repetition_count))\n\ndf = pd.concat(individual_results, keys=np.unique(subjects), names=['subject']).reset_index()\ndf.drop(['level_2'], axis=1, inplace=True)\ndf\n\n\n\n\n  \n    \n      \n      subject\n      locus\n      lag\n      prob\n      actual\n      possible\n    \n  \n  \n    \n      0\n      1\n      From Position 1\n      -3\n      0.000000\n      0\n      27\n    \n    \n      1\n      1\n      From Position 1\n      -2\n      0.115385\n      3\n      26\n    \n    \n      2\n      1\n      From Position 1\n      -1\n      0.153846\n      4\n      26\n    \n    \n      3\n      1\n      From Position 1\n      1\n      0.200000\n      8\n      40\n    \n    \n      4\n      1\n      From Position 1\n      2\n      0.314286\n      11\n      35\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      415\n      37\n      From Position 2\n      -2\n      0.125000\n      1\n      8\n    \n    \n      416\n      37\n      From Position 2\n      -1\n      0.142857\n      1\n      7\n    \n    \n      417\n      37\n      From Position 2\n      1\n      0.333333\n      2\n      6\n    \n    \n      418\n      37\n      From Position 2\n      2\n      0.000000\n      0\n      9\n    \n    \n      419\n      37\n      From Position 2\n      3\n      0.111111\n      1\n      9\n    \n  \n\n420 rows × 6 columns\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmax_lag = minimum_lag/2\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\ng = sns.FacetGrid(df, height=4.5)\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_neg), x='lag', y='prob', hue='locus', err_style=None,**kws)\n)\n\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_pos), x='lag', y='prob', hue='locus', err_style=None, **kws)\n)\n\nplt.legend(['From Position {}'.format(i+1) for i in range(repetition_count)], loc='upper left')\n#plt.title('Stronger Contiguity For First Presentation of an Item\\n(Minimum Lag = {})'.format(minimum_lag))\nplt.tight_layout()\n\n\n\n\n\nControl Version\nHow do we modify this for control lists? - Still use presentation vectors from mixed lists to define i_and_j, but use control list presentations and data for trial_by_study_position and trial_by_item_index and recall_position (except for i_and_j keys and values) - Don’t continue if trial_by_item_index[recall_position+1] in i_and_j\n\nfrom tqdm import tqdm\n\nindividual_results = []\n\nfor iteration in tqdm(range(1000)):\n    for subject in np.unique(subjects):\n        mixed_selection = np.logical_and(list_types == 4, subjects == subject)\n        control_selection = np.logical_and(list_types == 1, subjects == subject)\n        shuffled_trials = trials[control_selection]\n        np.random.shuffle(shuffled_trials)\n\n        individual_results.append(\n            alternative_contiguity_control(presentations[mixed_selection], presentations[control_selection], shuffled_trials, minimum_lag))\n\ndf = pd.concat(individual_results, keys=np.unique(subjects), names=['subject']).reset_index()\ndf.drop(['level_2'], axis=1, inplace=True)\ndf\n\n 77%|███████▋  | 766/1000 [01:58<00:36,  6.37it/s]C:\\Users\\gunnj\\AppData\\Local\\Temp/ipykernel_7552/90211409.py:169: RuntimeWarning: invalid value encountered in true_divide\n  {'lag': relevant_lags, 'prob': np.divide(actual_lagsB, possible_lagsB),\n 87%|████████▋ | 872/1000 [02:14<00:19,  6.63it/s]C:\\Users\\gunnj\\AppData\\Local\\Temp/ipykernel_7552/90211409.py:166: RuntimeWarning: invalid value encountered in true_divide\n  {'lag': relevant_lags, 'prob': np.divide(actual_lagsA, possible_lagsA),\n100%|██████████| 1000/1000 [02:33<00:00,  6.50it/s]\n\n\n\n\n\n  \n    \n      \n      subject\n      locus\n      lag\n      prob\n      actual\n      possible\n    \n  \n  \n    \n      0\n      1\n      From First Position\n      -3\n      0.000000\n      0\n      13\n    \n    \n      1\n      1\n      From First Position\n      -2\n      0.047619\n      1\n      21\n    \n    \n      2\n      1\n      From First Position\n      -1\n      0.050000\n      1\n      20\n    \n    \n      3\n      1\n      From First Position\n      1\n      0.230769\n      6\n      26\n    \n    \n      4\n      1\n      From First Position\n      2\n      0.047619\n      1\n      21\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      415\n      37\n      From Second Position\n      -2\n      0.000000\n      0\n      7\n    \n    \n      416\n      37\n      From Second Position\n      -1\n      0.000000\n      0\n      7\n    \n    \n      417\n      37\n      From Second Position\n      1\n      0.000000\n      0\n      7\n    \n    \n      418\n      37\n      From Second Position\n      2\n      0.000000\n      0\n      6\n    \n    \n      419\n      37\n      From Second Position\n      3\n      0.000000\n      0\n      6\n    \n  \n\n420 rows × 6 columns\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmax_lag = minimum_lag/2\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\ng = sns.FacetGrid(df, height=4.5)\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_neg), x='lag', y='prob', hue='locus', err_style='bars', **kws)\n)\n\ng.map_dataframe(\n    lambda data, **kws: sns.lineplot(\n        data=data.query(filt_pos), x='lag', y='prob', hue='locus', err_style='bars', **kws)\n)\n\nplt.legend(['First Presentation', 'Second Presentation'], loc='upper left')\nplt.title('Control Analysis \\n(Minimum Lag = {})'.format(minimum_lag))\nplt.tight_layout()"
  },
  {
    "objectID": "library\\analyses\\Measuring_Repetition_Effects.html#recall-rate-by-lag-from-repetitions",
    "href": "library\\analyses\\Measuring_Repetition_Effects.html#recall-rate-by-lag-from-repetitions",
    "title": "compmemlearn",
    "section": "Recall Rate by Lag From Repetitions",
    "text": "def recall_rate_by_lag(mixed_presentations, mixed_recalls, lag_threshold, lag):\n    \"\"\"\n    Find each item repetition in mixed_presentations and compute the recall rate for items at specified lag from each presentation position. Return the rate relative to the first presentation of each item and relative to the second.\n    \"\"\"\n\n    recalls_A = 0\n    recalls_B = 0\n    total = 0\n\n    for trial_index in range(len(mixed_presentations)):\n\n        # sequence of item indices ordered as they were studied\n        presentation = mixed_presentations[trial_index]\n\n        # sequence of initial study positions ordered as they were recalled\n        trial_by_study_position = mixed_recalls[trial_index]\n\n        # sequence of item indices ordered as they were recalled\n        trial_by_item_index = presentation[trial_by_study_position-1][:np.size(np.nonzero(trial_by_study_position))]\n\n        # {item_index: [presentation indices]} for each repeated item in presentation sequence spaced by >4 items\n        i_and_j = indices_of_repeated_items(presentation, lag_threshold)\n        \n        # then for each unique repeated item in the study list, \n        for repeated_item in i_and_j:\n\n            # get the indices of neighbors to each presentation\n            neighborA = presentation[i_and_j[repeated_item][0] + lag]\n            neighborB = presentation[i_and_j[repeated_item][1] + lag]\n\n            # skip if the neighbor is also a repeatedly presented item\n            if (neighborA in i_and_j) or (neighborB in i_and_j):\n                continue\n\n            # track if they were recalled\n            recalls_A += np.any(neighborA == trial_by_item_index)\n            recalls_B += np.any(neighborB == trial_by_item_index)\n            total += 1\n\n    return recalls_A/total, recalls_B/total, total\n\nindividual_resultsA = []\nindividual_resultsB = []\n\nfor subject in np.unique(subjects):\n    selection = np.logical_and(list_types > 2, subjects == subject)\n\n    result = recall_rate_by_lag(presentations[selection], trials[selection], 4, 1)\n    individual_resultsA.append(result[0])\n    individual_resultsB.append(result[1])\n\nnp.mean(individual_resultsA), np.mean(individual_resultsB)\n\n(0.3857720129269049, 0.382558043145297)"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall.html#data-preparation",
    "href": "library\\analyses\\Probability_of_First_Recall.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "For our demonstrations, we’ll lean on the MurdockOkada1970 dataset. As a reminder, in this dataset each of 72 undergraduates was given 20 test lists with 20-word lists visually presented at either 60 or 120 words/min.\n\nfrom compmemlearn.datasets import prepare_murdock1970_data\n\ntrials, events, list_length = prepare_murdock1970_data(\"../../data/mo1970.txt\")\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall.html#analysis",
    "href": "library\\analyses\\Probability_of_First_Recall.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "DataFrame\nWe can usually rely on the psifr library to generate pandas DataFrames containing PFR information.\n\nfrom psifr import fr\n\npfr = fr.pnr(events).query(\"output <= 1\")\npfr\n\n\n\n\n  \n    \n      \n      \n      \n      prob\n      actual\n      possible\n    \n    \n      subject\n      output\n      input\n      \n      \n      \n    \n  \n  \n    \n      1\n      1\n      1\n      0.00\n      0\n      20\n    \n    \n      2\n      0.00\n      0\n      20\n    \n    \n      3\n      0.00\n      0\n      20\n    \n    \n      4\n      0.00\n      0\n      20\n    \n    \n      5\n      0.00\n      0\n      20\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      72\n      1\n      16\n      0.05\n      1\n      20\n    \n    \n      17\n      0.50\n      10\n      20\n    \n    \n      18\n      0.15\n      3\n      20\n    \n    \n      19\n      0.10\n      2\n      20\n    \n    \n      20\n      0.05\n      1\n      20\n    \n  \n\n1440 rows × 3 columns\n\n\n\n\n\nFast Array Generation\nDataFrames contain granular subject-by-subject information and are easier to plot using the seaborn library. But sometimes we don’t need this granular information and mainly want to perform our analysis as quickly as possible – perhaps to help with model fitting or analysis. In that case, representing results with numpy arrays and performing just-in-time compilation of our function using numba might be preferred. We include analyses.fast_crp in our library for that purpose here.\n# export\n\nfrom numba import njit\nimport numpy as np\n\n\n@njit(fastmath=True, nogil=True)\ndef fast_pfr(trials, item_count):\n    return np.bincount(trials[:, 0], minlength=item_count + 1)[1:] / len(trials)\n\nfast_pfr(trials, list_length)\n\narray([0.06666667, 0.00347222, 0.00208333, 0.00208333, 0.00416667,\n       0.00138889, 0.00694444, 0.00416667, 0.00763889, 0.01458333,\n       0.00902778, 0.01597222, 0.03055556, 0.03472222, 0.05694444,\n       0.09027778, 0.13402778, 0.18611111, 0.15347222, 0.17569444])\n\n\nWe can compare the runtimes of compmemlearn’s and psifr’s functions using the %%timeit Jupyter magic:\n\n%%timeit\nfast_pfr(trials, list_length)\n\n3.88 µs ± 87.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\n\n%%timeit\nfr.pnr(events)\n\n530 ms ± 4.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nOur fast implementation is more than 100,000 times faster!"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall.html#plotting",
    "href": "library\\analyses\\Probability_of_First_Recall.html#plotting",
    "title": "compmemlearn",
    "section": "Plotting",
    "text": "psifr’s plotting library creates a separate figure for each plot, when sometimes we want to to include multiple plots in one figure, so we define our own.\n# export\nimport seaborn as sns\nfrom psifr import fr\n\n\ndef plot_pfr(data, **facet_kws):\n\n    pfr_data = fr.pnr(data).query(\"output <= 1\")\n\n    sns.lineplot(data=pfr_data, x=\"input\", y=\"prob\", **facet_kws)\n\nsns.set_theme(style=\"darkgrid\")\n\ng = sns.FacetGrid(dropna=False, data=events, height=4, aspect=1.2)\ng.map_dataframe(plot_pfr, err_style=\"bars\")\n\ng.set_xlabels(\"Study Position\")\ng.set_ylabels(\"Probability of First Recall\")"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#data-preparation",
    "href": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#function",
    "href": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#function",
    "title": "compmemlearn",
    "section": "Function",
    "text": "from compmemlearn.analyses import recall_by_second_study_position\nfrom compmemlearn.analyses import recall_by_all_study_positions\n# export\n\nfrom numba import njit\nimport numpy as np\nfrom numba import int32\n\n@njit(nogil=True)\ndef fast_mixed_pfr(trials, presentations):\n    \n    list_length = len(presentations[0])\n    result = np.zeros(list_length, dtype=int32)\n    alt_trials = recall_by_second_study_position(trials, presentations)\n    first_recalls = np.hstack((trials[:, :1], alt_trials[:, :1]))\n    \n    for trial_index in range(len(trials)):\n        for i in range(list_length):\n            result[i] += i+1 in first_recalls[trial_index]\n    \n    return result/len(trials)\n\n@njit(nogil=True)\ndef flex_mixed_pfr(trials, presentations):\n    \n    list_length = len(presentations[0])\n    result = np.zeros(list_length, dtype=int32)\n    all_study_positions = recall_by_all_study_positions(trials, presentations) \n    first_recalls = all_study_positions[:, :, :1]\n    \n    for trial_index in range(len(trials)):\n        for i in range(list_length):\n            result[i] += i+1 in first_recalls[:, trial_index]\n    \n    return result/len(trials)"
  },
  {
    "objectID": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#demo",
    "href": "library\\analyses\\Probability_of_First_Recall_in_Repetition_Data.html#demo",
    "title": "compmemlearn",
    "section": "Demo",
    "text": "fast_mixed_pfr(trials[list_types==1], presentations[list_types==1])\n\narray([0.21666667, 0.06190476, 0.03333333, 0.04047619, 0.00952381,\n       0.01666667, 0.        , 0.02142857, 0.01190476, 0.01190476,\n       0.01428571, 0.01428571, 0.0047619 , 0.00714286, 0.00952381,\n       0.00238095, 0.00952381, 0.01190476, 0.0047619 , 0.00952381,\n       0.0047619 , 0.01428571, 0.00952381, 0.00714286, 0.01190476,\n       0.01190476, 0.00952381, 0.02142857, 0.01428571, 0.01190476,\n       0.02619048, 0.01190476, 0.01428571, 0.01904762, 0.01190476,\n       0.03571429, 0.05238095, 0.06190476, 0.06428571, 0.07380952])\n\n\n\nflex_mixed_pfr(trials[list_types==1], presentations[list_types==1])\n\narray([0.21666667, 0.06190476, 0.03333333, 0.04047619, 0.00952381,\n       0.01666667, 0.        , 0.02142857, 0.01190476, 0.01190476,\n       0.01428571, 0.01428571, 0.0047619 , 0.00714286, 0.00952381,\n       0.00238095, 0.00952381, 0.01190476, 0.0047619 , 0.00952381,\n       0.0047619 , 0.01428571, 0.00952381, 0.00714286, 0.01190476,\n       0.01190476, 0.00952381, 0.02142857, 0.01428571, 0.01190476,\n       0.02619048, 0.01190476, 0.01428571, 0.01904762, 0.01190476,\n       0.03571429, 0.05238095, 0.06190476, 0.06428571, 0.07380952])\n\n\n\nfast_mixed_pfr(trials[list_types==2], presentations[list_types==2])\n\narray([0.34761905, 0.34761905, 0.06904762, 0.06904762, 0.0452381 ,\n       0.0452381 , 0.03333333, 0.03333333, 0.01666667, 0.01666667,\n       0.01666667, 0.01666667, 0.01428571, 0.01428571, 0.01666667,\n       0.01666667, 0.01190476, 0.01190476, 0.02380952, 0.02380952,\n       0.01666667, 0.01666667, 0.01904762, 0.01904762, 0.02142857,\n       0.02142857, 0.02619048, 0.02619048, 0.03333333, 0.03333333,\n       0.03095238, 0.03095238, 0.04047619, 0.04047619, 0.04285714,\n       0.04285714, 0.06904762, 0.06904762, 0.1       , 0.1       ])\n\n\n\nflex_mixed_pfr(trials[list_types==2], presentations[list_types==2])\n\narray([0.34761905, 0.34761905, 0.06904762, 0.06904762, 0.0452381 ,\n       0.0452381 , 0.03333333, 0.03333333, 0.01666667, 0.01666667,\n       0.01666667, 0.01666667, 0.01428571, 0.01428571, 0.01666667,\n       0.01666667, 0.01190476, 0.01190476, 0.02380952, 0.02380952,\n       0.01666667, 0.01666667, 0.01904762, 0.01904762, 0.02142857,\n       0.02142857, 0.02619048, 0.02619048, 0.03333333, 0.03333333,\n       0.03095238, 0.03095238, 0.04047619, 0.04047619, 0.04285714,\n       0.04285714, 0.06904762, 0.06904762, 0.1       , 0.1       ])\n\n\n\nfast_mixed_pfr(trials[list_types==3], presentations[list_types==3])\n\narray([0.24285714, 0.06428571, 0.05      , 0.06428571, 0.09285714,\n       0.06428571, 0.04047619, 0.04285714, 0.0547619 , 0.05238095,\n       0.03333333, 0.01904762, 0.02380952, 0.01666667, 0.03095238,\n       0.03809524, 0.01428571, 0.00714286, 0.01904762, 0.02857143,\n       0.01428571, 0.02857143, 0.01428571, 0.02619048, 0.02142857,\n       0.01666667, 0.02619048, 0.02142857, 0.0452381 , 0.03809524,\n       0.0452381 , 0.03095238, 0.05      , 0.0547619 , 0.07142857,\n       0.09285714, 0.06666667, 0.11428571, 0.0952381 , 0.09761905])\n\n\n\nfast_mixed_pfr(trials[list_types==4], presentations[list_types==4])\n\narray([0.21190476, 0.04285714, 0.05      , 0.02142857, 0.02857143,\n       0.02857143, 0.01904762, 0.01666667, 0.01666667, 0.02619048,\n       0.01666667, 0.03809524, 0.02380952, 0.02619048, 0.01428571,\n       0.01190476, 0.01190476, 0.01666667, 0.01428571, 0.00952381,\n       0.01666667, 0.02142857, 0.0047619 , 0.00952381, 0.01666667,\n       0.02142857, 0.00952381, 0.03095238, 0.03333333, 0.01428571,\n       0.02142857, 0.02380952, 0.02857143, 0.03571429, 0.02857143,\n       0.04761905, 0.03333333, 0.05714286, 0.04047619, 0.04285714])"
  },
  {
    "objectID": "library\\analyses\\Recall_Probability_by_Spacing.html#data-preparation",
    "href": "library\\analyses\\Recall_Probability_by_Spacing.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Recall_Probability_by_Spacing.html#analysis",
    "href": "library\\analyses\\Recall_Probability_by_Spacing.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "We optimize this with numba (and some creative Python) to speed up the calculation.\n\nFast Array Generation\nDataFrames contain granular subject-by-subject information and are easier to plot using the seaborn library. But sometimes we don’t need this granular information and mainly want to perform our analysis as quickly as possible – perhaps to help with model fitting or analysis. In that case, representing results with numpy arrays and performing just-in-time compilation of our function using numba might be preferred. We include analyses.fast_rpl in our library for that purpose here.\nThe function assumes items are repeated up to 2 times.\n# export\n\nfrom numba import njit, prange\nimport numpy as np\n\n@njit(nogil=True, parallel=True)\ndef fast_rpl(study_positions_in_recall_order, presentations, max_lag=8):\n    \n    assert(len(presentations) == len(study_positions_in_recall_order))\n\n    total_presented, total_retrieved = np.zeros(max_lag+2), np.zeros(max_lag+2)\n\n    for trial_index in prange(len(presentations)):\n        presented, retrieved = np.zeros(max_lag+2), np.zeros(max_lag+2)\n        trial = study_positions_in_recall_order[trial_index]\n        trial = trial[trial > 0]-1\n        \n        for item in np.unique(presentations[trial_index]):\n            for idx, val in np.ndenumerate(presentations[trial_index]):\n                if val == item:\n                    locationA = idx[0]\n                    break\n\n            lag = 0\n            if locationA < len(presentations[trial_index]):\n                for idx, val in np.ndenumerate(presentations[trial_index][locationA+1:]):\n                    if val == item:\n                        lag = 1 + idx[0]\n                        break\n\n            presented[lag] += 1\n            retrieved[lag] += locationA in trial\n            \n        total_presented += presented\n        total_retrieved += retrieved\n\n    return total_retrieved/total_presented\n\ncondition = 4\n\nresult = fast_rpl(\n    trials[list_types>=condition], presentations[list_types>=condition], max_lag=8)\n\n# lohnas 2014 bins result by N/A, 0, 1-2, 3-5, and 6-8 number of intervening items\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\n[0.37278912 0.43928571 0.47678571 0.55714286 0.57857143]\n\n\n\n%%timeit\nfast_rpl(presentations, trials, max_lag=8)\n\n2.74 ms ± 253 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nDataFrame\nThe psifr library doesn’t have a function to generate pandas DataFrames containing conditional stop probability information, so we make our own. For efficiency, it mainly consists of calls to fast_rpl. Since we normally compare our analyses with the results from Lohnas & Kahana (2014), we’ll automatically bin results between ['N/A', '0', '1-2', '3-5', '6-8'] even though fast_rpl provides more granular information.\n# export\n\nimport pandas as pd\n\ndef rpl(presentations, trials, subjects, trial_count, list_length, max_lag=8):\n    #subjects = len(np.unique(events.subject))\n    #trial_count = np.max(events.list)\n    #list_length = np.max(events.input)\n    #lags = ['N/A'] + list(range(max_lag+1))\n    lags = ['N/A', '0', '1-2', '3-5', '6-8']\n    \n    result = {'subject': [], 'lag': [], 'prob': []}\n    \n    for subject in range(subjects):\n\n        subject_result = fast_rpl(\n            trials[subject*trial_count:(subject+1)*trial_count], presentations[subject*trial_count:(subject+1)*trial_count], max_lag)\n        \n        binned = np.zeros(5)\n        binned[0] = subject_result[0]\n        binned[1] = subject_result[1]\n        binned[2] = (subject_result[2] + subject_result[3])/2\n        binned[3] = (subject_result[4] + subject_result[5] + subject_result[6])/3\n        binned[4] = (subject_result[7] + subject_result[8] + subject_result[9])/3\n\n        result['subject'] += [subject+1]*len(lags)\n        result['lag'] += lags\n        result['prob'] += binned.tolist()\n        \n    return pd.DataFrame(result)"
  },
  {
    "objectID": "library\\analyses\\Recall_Probability_by_Spacing.html#plotting-demo",
    "href": "library\\analyses\\Recall_Probability_by_Spacing.html#plotting-demo",
    "title": "compmemlearn",
    "section": "Plotting Demo",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncondition = 4\nsource = 'Lohnas & Kahana (2014)'\n\nsubject_count = len(np.unique(events.subject))\ntrial_count = int(np.max(events.list)/4)\n\ndata = rpl(\n    presentations[list_types>=4], trials[list_types>=4], \n    subject_count, trial_count, list_length)\n\nsns.barplot(data=data, x='lag', y='prob')\n\nplt.title('Condition {}, {}'.format(condition, source))\nplt.xlabel('Number of Intervening Items Between Repetitions')\nplt.ylabel('Recall Probability')\n\nText(0, 0.5, 'Recall Probability')"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect.html#data-preparation",
    "href": "library\\analyses\\Serial_Position_Effect.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "For our demonstrations, we’ll lean on the MurdockOkada1970 dataset. As a reminder, in this dataset each of 72 undergraduates was given 20 test lists with 20-word lists visually presented at either 60 or 120 words/min.\n\nfrom compmemlearn.datasets import prepare_murdock1970_data\n\ntrials, events, list_length = prepare_murdock1970_data('../../data/mo1970.txt')\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect.html#analysis",
    "href": "library\\analyses\\Serial_Position_Effect.html#analysis",
    "title": "compmemlearn",
    "section": "Analysis",
    "text": "DataFrame\nWe can usually rely on the psifr library to generate pandas DataFrames containing serial position curve information.\n\nfrom psifr import fr\n\nspc = fr.spc(events)\nspc \n\n\n\n\n  \n    \n      \n      \n      recall\n    \n    \n      subject\n      input\n      \n    \n  \n  \n    \n      1\n      1\n      0.40\n    \n    \n      2\n      0.10\n    \n    \n      3\n      0.25\n    \n    \n      4\n      0.30\n    \n    \n      5\n      0.10\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      72\n      16\n      0.20\n    \n    \n      17\n      0.80\n    \n    \n      18\n      0.85\n    \n    \n      19\n      0.95\n    \n    \n      20\n      1.00\n    \n  \n\n1440 rows × 1 columns\n\n\n\n\n\nFast Array Generation\nDataFrames contain granular subject-by-subject information and are easier to plot using the seaborn library. But sometimes we don’t need this granular information and mainly want to perform our analysis as quickly as possible – perhaps to help with model fitting or analysis. In that case, representing results with numpy arrays and performing just-in-time compilation of our function using numba might be preferred. We include analyses.fast_spc in our library for that purpose here.\n# export\n\nfrom numba import njit\nimport numpy as np\n\n@njit(nogil=True)\ndef fast_spc(trials, item_count):\n    return np.bincount(trials.flatten(), minlength=item_count+1)[1:]/len(trials)\n\nfast_spc(trials, 20)\n\narray([0.44305556, 0.29097222, 0.22222222, 0.18958333, 0.13888889,\n       0.15694444, 0.15486111, 0.14097222, 0.16041667, 0.18958333,\n       0.15347222, 0.1875    , 0.21875   , 0.25347222, 0.27847222,\n       0.3125    , 0.39722222, 0.5875    , 0.68819444, 0.7875    ])\n\n\nWe can compare the runtimes of compmemlearn’s analyses.fast_spc and psifr’s fr.spc using the %%timeit Jupyter magic:\n\n%%timeit\nfast_spc(trials, 20)\n\n34.5 µs ± 1.04 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n\n%%timeit\nfr.spc(events)\n\n5.14 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nOur fast implementation is more than 100 times faster!"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect.html#plotting",
    "href": "library\\analyses\\Serial_Position_Effect.html#plotting",
    "title": "compmemlearn",
    "section": "Plotting",
    "text": "psifr’s plotting library creates a separate figure for each plot, when sometimes we want to to include multiple plots in one figure, so we define our own.\n# export\nfrom psifr import fr\nimport seaborn as sns\n\ndef plot_spc(data, **facet_kws):\n    \n    sns.lineplot(\n        data=fr.spc(data), x='input', y='recall', **facet_kws)\n\nsns.set_theme()\ng = sns.FacetGrid(dropna=False, data=events, aspect=1.5)\ng.map_dataframe(plot_spc, err_style='bars')\ng.set_xlabels('Study Position')\ng.set_ylabels('Recall Rate')\ng.set(ylim=(0, 1));"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#data-preparation",
    "href": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#functions",
    "href": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#functions",
    "title": "compmemlearn",
    "section": "Functions",
    "text": "Tracking the Study Positions of Repeatedly Presented Items\n# export\n\nfrom numba import njit, prange\nimport numpy as np\nfrom numba import int32\nfrom compmemlearn.datasets import find_first\n\n@njit(nogil=True)\ndef recall_by_second_study_position(trials, presentations):\n    \n    flipped_presentations = np.fliplr(presentations)\n    list_length = len(presentations[0])\n    result = np.zeros(np.shape(trials), dtype=int32)\n    \n    for trial_index in range(len(trials)):\n        \n        trial = trials[trial_index]\n        presentation = presentations[trial_index]\n        flipped_presentation = flipped_presentations[trial_index]\n        \n        for recall_index in range(len(trial)):\n            \n            if trial[recall_index] == 0:\n                continue\n                \n            item_index = presentation[trial[recall_index]-1]\n            result[trial_index, recall_index] = list_length - find_first(\n                item_index, flipped_presentation)\n        \n    return result\n\n@njit(nogil=True)\ndef recall_by_all_study_positions(recall_by_first_study_position, presentations, max_repeats=3):\n    \n    trials_shape = np.shape(recall_by_first_study_position)\n    result = np.zeros(\n            (max_repeats, trials_shape[0], trials_shape[1]), dtype=int32)\n\n    for trial_index in range(len(recall_by_first_study_position)):\n\n        trial = recall_by_first_study_position[trial_index]\n        presentation = presentations[trial_index]\n        \n        for recall_index in range(len(trial)):\n\n            if trial[recall_index] == 0:\n                continue\n\n            presentation_positions = np.nonzero(\n                presentation[trial[recall_index] - 1] == presentation)[0] + 1\n\n            result[:len(presentation_positions), trial_index, recall_index] = presentation_positions\n\n    return result\n\nrecall_by_all_study_positions(trials, presentations, 2)[1, 0]\n\narray([ 0,  0,  0,  0,  0,  0,  0,  0, 19,  0,  0,  0, 13,  0, 31,  0, 34,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0])\n\n\n\n%timeit recall_by_all_study_positions(trials, presentations)\n\n6.55 ms ± 231 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nrecall_by_second_study_position(trials, presentations)[0]\n\narray([ 1,  2,  3,  4,  5,  6,  7,  9, 19, 11, 17, 14, 13, 15, 31, 20, 34,\n       30, 39, 38, 37, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0])\n\n\n\n%timeit recall_by_second_study_position(trials, presentations)\n\n488 µs ± 3.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nFast Serial Position Curve\n# export\n\n@njit(nogil=True)\ndef fast_mixed_spc(trials, presentations):\n    \n    list_length = len(presentations[0])\n    result = np.zeros(list_length, dtype=int32)\n    alt_trials = recall_by_second_study_position(trials, presentations)\n    trials = np.hstack((trials, alt_trials))\n    \n    for trial_index in range(len(trials)):\n        for study_position in range(list_length):\n            result[study_position] += study_position+1 in trials[trial_index]\n    \n    return result/len(trials)\n\n\n@njit(nogil=True)\ndef flex_mixed_spc(trials, presentations):\n    \n    list_length = len(presentations[0])\n    result = np.zeros(list_length, dtype=int32)\n    all_study_positions = recall_by_all_study_positions(trials, presentations) \n    \n    for trial_index in range(len(trials)):\n        for study_position in range(list_length):\n            result[study_position] += study_position+1 in all_study_positions[:,trial_index]\n    \n    return result/len(trials)"
  },
  {
    "objectID": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#demo",
    "href": "library\\analyses\\Serial_Position_Effect_in_Repetition_Datasets.html#demo",
    "title": "compmemlearn",
    "section": "Demo",
    "text": "fast_mixed_spc(trials[list_types==2], presentations[list_types==2])\n\narray([0.79761905, 0.79761905, 0.72380952, 0.72380952, 0.65      ,\n       0.65      , 0.62857143, 0.62857143, 0.6047619 , 0.6047619 ,\n       0.57857143, 0.57857143, 0.55714286, 0.55714286, 0.56666667,\n       0.56666667, 0.54285714, 0.54285714, 0.56190476, 0.56190476,\n       0.55952381, 0.55952381, 0.55952381, 0.55952381, 0.55238095,\n       0.55238095, 0.54761905, 0.54761905, 0.56904762, 0.56904762,\n       0.56190476, 0.56190476, 0.6047619 , 0.6047619 , 0.59285714,\n       0.59285714, 0.5952381 , 0.5952381 , 0.59285714, 0.59285714])\n\n\n\nflex_mixed_spc(trials[list_types==2], presentations[list_types==2])\n\narray([0.79761905, 0.79761905, 0.72380952, 0.72380952, 0.65      ,\n       0.65      , 0.62857143, 0.62857143, 0.6047619 , 0.6047619 ,\n       0.57857143, 0.57857143, 0.55714286, 0.55714286, 0.56666667,\n       0.56666667, 0.54285714, 0.54285714, 0.56190476, 0.56190476,\n       0.55952381, 0.55952381, 0.55952381, 0.55952381, 0.55238095,\n       0.55238095, 0.54761905, 0.54761905, 0.56904762, 0.56904762,\n       0.56190476, 0.56190476, 0.6047619 , 0.6047619 , 0.59285714,\n       0.59285714, 0.5952381 , 0.5952381 , 0.59285714, 0.59285714])\n\n\n\nfast_mixed_spc(trials[list_types==1], presentations[list_types==1])\n\narray([0.64285714, 0.56904762, 0.5047619 , 0.45238095, 0.40952381,\n       0.38571429, 0.35      , 0.33571429, 0.33571429, 0.3       ,\n       0.32380952, 0.32619048, 0.32619048, 0.32380952, 0.29761905,\n       0.31190476, 0.32380952, 0.29761905, 0.27857143, 0.27142857,\n       0.29047619, 0.28333333, 0.27857143, 0.32857143, 0.28095238,\n       0.33095238, 0.32857143, 0.31666667, 0.32619048, 0.33809524,\n       0.37380952, 0.32380952, 0.36190476, 0.34047619, 0.35952381,\n       0.37619048, 0.39047619, 0.42380952, 0.41428571, 0.3452381 ])\n\n\n\nflex_mixed_spc(trials[list_types==1], presentations[list_types==1])\n\narray([0.64285714, 0.56904762, 0.5047619 , 0.45238095, 0.40952381,\n       0.38571429, 0.35      , 0.33571429, 0.33571429, 0.3       ,\n       0.32380952, 0.32619048, 0.32619048, 0.32380952, 0.29761905,\n       0.31190476, 0.32380952, 0.29761905, 0.27857143, 0.27142857,\n       0.29047619, 0.28333333, 0.27857143, 0.32857143, 0.28095238,\n       0.33095238, 0.32857143, 0.31666667, 0.32619048, 0.33809524,\n       0.37380952, 0.32380952, 0.36190476, 0.34047619, 0.35952381,\n       0.37619048, 0.39047619, 0.42380952, 0.41428571, 0.3452381 ])\n\n\n\nfast_mixed_spc(trials[list_types==3], presentations[list_types==3])\n\narray([0.75714286, 0.70952381, 0.68809524, 0.66666667, 0.66428571,\n       0.6952381 , 0.67380952, 0.67619048, 0.64761905, 0.64761905,\n       0.63809524, 0.59761905, 0.62857143, 0.5952381 , 0.64285714,\n       0.56904762, 0.6047619 , 0.59285714, 0.58571429, 0.6047619 ,\n       0.55238095, 0.56904762, 0.57380952, 0.55238095, 0.61190476,\n       0.56666667, 0.6047619 , 0.61190476, 0.62142857, 0.61666667,\n       0.6452381 , 0.62380952, 0.61666667, 0.65714286, 0.66666667,\n       0.68333333, 0.6952381 , 0.68571429, 0.67142857, 0.67857143])\n\n\n\nfast_mixed_spc(trials[list_types==4], presentations[list_types==4])\n\narray([0.63809524, 0.5452381 , 0.46666667, 0.43333333, 0.4547619 ,\n       0.41666667, 0.42142857, 0.42619048, 0.40714286, 0.4       ,\n       0.40714286, 0.43333333, 0.44285714, 0.43333333, 0.44285714,\n       0.40238095, 0.40238095, 0.43809524, 0.41666667, 0.42857143,\n       0.36190476, 0.38809524, 0.36904762, 0.38333333, 0.42142857,\n       0.40714286, 0.42619048, 0.39761905, 0.42619048, 0.39285714,\n       0.39285714, 0.41666667, 0.42142857, 0.44761905, 0.42619048,\n       0.43095238, 0.40714286, 0.37142857, 0.38333333, 0.30952381])\n\n\n\nfrom compmemlearn.analyses import fast_spc\n\n%timeit fast_spc(trials[list_types==1], list_length)\n\n53.8 µs ± 19.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%timeit fast_mixed_spc(trials[list_types==1], presentations[list_types==1])\n\n1.13 ms ± 3.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit flex_mixed_spc(trials[list_types==1], presentations[list_types==1])\n\n4.26 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "library\\analyses\\Shared_Contiguity.html#data-preparation",
    "href": "library\\analyses\\Shared_Contiguity.html#data-preparation",
    "title": "compmemlearn",
    "section": "Data Preparation",
    "text": "from compmemlearn.datasets import prepare_lohnas2014_data\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\analyses\\Shared_Contiguity.html#functions",
    "href": "library\\analyses\\Shared_Contiguity.html#functions",
    "title": "compmemlearn",
    "section": "Functions",
    "text": "def shared_contiguity(trials, presentations, max_repeats=2, min_lag=4):\n    \n    list_length = len(presentations[0])\n    total_transitions = 0\n    complementary_transitions = 0\n    terminus = np.sum(trials != 0, axis=1) - 1 # number of recalls per trial\n    \n    for trial_index in range(len(trials)):\n        \n        item_count = np.max(presentations[trial_index]) + 1\n        si = np.zeros((item_count, 2), dtype=int)\n        sj = np.zeros((item_count, 2), dtype=int)\n        \n        # we track possible positions using presentations and alt_presentations\n        for item in range(item_count):\n            pos = np.nonzero(presentations[trial_index] == item)[0]\n            \n            # then we track Si and Sj by identifying repeated items and storing item indices\n            if (len(pos) > 1) and (pos[1] - pos[0] >= min_lag):\n                \n                assert np.all(pos < list_length - 2) # bounds issues don't affect result in this case - might later\n                si[item, :] = presentations[trial_index, pos[0]+1:pos[0]+3] + 1\n                sj[item, :] = presentations[trial_index, pos[1]+1:pos[1]+3] + 1\n            \n        # i can track at each recall index whether si or sj was seen, \n        for recall_index in range(terminus[trial_index]):\n            \n            current_item = presentations[trial_index][trials[trial_index, recall_index]-1] + 1\n            next_item = presentations[trial_index][trials[trial_index, recall_index+1]-1] + 1\n            \n            # then peek at whether the next item was in complementary set, \n            # aggregating totals with each comparison\n            for si_match in np.nonzero(si == current_item)[0]:\n                complementary_transitions += next_item in sj[si_match]\n            for sj_match in np.nonzero(sj == current_item)[0]:\n                complementary_transitions += next_item in si[sj_match]\n \n            total_transitions += 1\n            \n    return complementary_transitions/total_transitions\nIt looks like there are two remaining things to clarify:\nFirst I have to figure out whether and how to track whether it’s possible to make a transition to si or sj.\nNext I have to figure out how often to increment total transitions.\n\nshared_contiguity(trials[list_types == 4], presentations[list_types == 4], 2, 4)\n\n0.0391566265060241"
  },
  {
    "objectID": "library\\datasets\\ClairExpt6.html#data-overview",
    "href": "library\\datasets\\ClairExpt6.html#data-overview",
    "title": "compmemlearn",
    "section": "Data Overview",
    "text": "import numpy as np\nfrom numba import njit, prange\nfrom compmemlearn.models import Classic_CMR\nfrom numba.typed import Dict\nfrom numba.core import types\n\n@njit(fastmath=True, nogil=True, parallel=True)\ndef lohnas_data_likelihood(trials, presentations, model_class, parameters):\n\n    list_length = len(presentations[0])\n    likelihood = np.ones((len(trials), list_length))\n\n    for trial_index in prange(len(trials)):\n\n        item_count = np.max(presentations[trial_index])\n        trial = trials[trial_index]\n        model = model_class(item_count, list_length, parameters)\n        presentation = presentations[trial_index][presentations[trial_index] > 0] -1 ## modify to support odd indexing\n        model.experience(model.items[presentation])\n\n        model.force_recall()\n        for recall_index in range(len(trial) + 1):\n\n            # identify index of item recalled; if zero then recall is over\n            if recall_index == len(trial) and len(trial) < item_count:\n                recall = 0\n            elif trial[recall_index] == 0:\n                recall = 0\n            else:\n                recall = presentation[trial[recall_index]-1] + 1\n\n            # store probability of and simulate recalling item with this index\n            likelihood[trial_index, recall_index] = \\\n                model.outcome_probabilities()[recall] + 10e-7\n\n            if recall == 0:\n                break\n            model.force_recall(recall)\n\n        # reset model to its pre-retrieval (but post-encoding) state\n        model.force_recall(0)\n\n    return -np.sum(np.log(likelihood))\n\ndef lohnas_objective_function(data_to_fit, presentations, model_class, fixed_parameters, free_parameters):\n\n    \"\"\"\n    Generates and returns an objective function for input to support search \n    through parameter space for model fit using an optimization function.\n\n    Returns a function that accepts a vector x specifying arbitrary values for \n    free parameters and returns evaluation of likelihood using the model \n    class, all parameters, and provided data.\n    \"\"\"\n\n    parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\n    for name, value in fixed_parameters.items():\n        parameters[name] = value\n\n    def objective_function(x):\n        for i in range(len(free_parameters)):\n            parameters[free_parameters[i]] = x[i]\n        return lohnas_data_likelihood(data_to_fit, presentations, model_class, parameters)\n\n    return objective_function"
  },
  {
    "objectID": "library\\datasets\\ClairExpt6.html#condition-wise-fitting",
    "href": "library\\datasets\\ClairExpt6.html#condition-wise-fitting",
    "title": "compmemlearn",
    "section": "Condition-Wise Fitting",
    "text": "cmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n]\n\nconditions = ['Control', 'RP']\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\ncmr_results = []\nfor condition in [0, 1]:\n    selection = list_types == condition\n    cost_function = lohnas_objective_function(\n        trials[selection], \n        presentations[selection],\n        init_cmr,\n        {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0, 'delay_drift_rate': 0}, \n        cmr_free_parameters)\n\n    cmr_results.append(differential_evolution(cost_function, cmr_bounds, disp=True))\n\ndifferential_evolution step 1: f(x)= 5050.66\ndifferential_evolution step 2: f(x)= 3739.1\ndifferential_evolution step 3: f(x)= 3280.2\ndifferential_evolution step 4: f(x)= 3280.2\ndifferential_evolution step 5: f(x)= 3158.42\ndifferential_evolution step 6: f(x)= 3158.42\ndifferential_evolution step 7: f(x)= 3133.04\ndifferential_evolution step 8: f(x)= 3122.74\ndifferential_evolution step 9: f(x)= 3122.74\ndifferential_evolution step 10: f(x)= 3114.31\ndifferential_evolution step 11: f(x)= 3114.31\ndifferential_evolution step 12: f(x)= 3106.81\ndifferential_evolution step 13: f(x)= 3106.81\ndifferential_evolution step 14: f(x)= 3106.81\ndifferential_evolution step 15: f(x)= 3103.33\ndifferential_evolution step 16: f(x)= 3083.54\ndifferential_evolution step 17: f(x)= 3042.02\ndifferential_evolution step 18: f(x)= 3042.02\ndifferential_evolution step 19: f(x)= 3033.52\ndifferential_evolution step 20: f(x)= 3007.12\ndifferential_evolution step 21: f(x)= 3007.12\ndifferential_evolution step 22: f(x)= 3007.12\ndifferential_evolution step 23: f(x)= 3007.12\ndifferential_evolution step 24: f(x)= 3007.12\ndifferential_evolution step 25: f(x)= 3007.12\ndifferential_evolution step 26: f(x)= 3007.12\ndifferential_evolution step 27: f(x)= 3007.12\ndifferential_evolution step 28: f(x)= 3007.12\ndifferential_evolution step 29: f(x)= 3002.45\ndifferential_evolution step 30: f(x)= 3002.45\ndifferential_evolution step 31: f(x)= 3002.45\ndifferential_evolution step 32: f(x)= 3002.45\ndifferential_evolution step 33: f(x)= 3002.45\ndifferential_evolution step 34: f(x)= 3002.45\ndifferential_evolution step 35: f(x)= 3002.45\ndifferential_evolution step 36: f(x)= 3002.45\ndifferential_evolution step 37: f(x)= 2996.18\ndifferential_evolution step 38: f(x)= 2996.18\ndifferential_evolution step 39: f(x)= 2996.18\ndifferential_evolution step 40: f(x)= 2992.88\ndifferential_evolution step 1: f(x)= 5564.48\ndifferential_evolution step 2: f(x)= 4870.95\ndifferential_evolution step 3: f(x)= 4369.42\ndifferential_evolution step 4: f(x)= 3623.9\ndifferential_evolution step 5: f(x)= 3448.84\ndifferential_evolution step 6: f(x)= 3448.84\ndifferential_evolution step 7: f(x)= 3448.84\ndifferential_evolution step 8: f(x)= 3420.41\ndifferential_evolution step 9: f(x)= 3417.5\ndifferential_evolution step 10: f(x)= 3394.18\ndifferential_evolution step 11: f(x)= 3317.27\ndifferential_evolution step 12: f(x)= 3317.27\ndifferential_evolution step 13: f(x)= 3317.27\ndifferential_evolution step 14: f(x)= 3317.27\ndifferential_evolution step 15: f(x)= 3293.09\ndifferential_evolution step 16: f(x)= 3293.09\ndifferential_evolution step 17: f(x)= 3293.09\ndifferential_evolution step 18: f(x)= 3271.59\ndifferential_evolution step 19: f(x)= 3271.59\ndifferential_evolution step 20: f(x)= 3271.59\ndifferential_evolution step 21: f(x)= 3271.59\ndifferential_evolution step 22: f(x)= 3267.88\ndifferential_evolution step 23: f(x)= 3267.88\ndifferential_evolution step 24: f(x)= 3267.72\ndifferential_evolution step 25: f(x)= 3250.9\ndifferential_evolution step 26: f(x)= 3250.9\ndifferential_evolution step 27: f(x)= 3250.9\ndifferential_evolution step 28: f(x)= 3232.47\ndifferential_evolution step 29: f(x)= 3194.53\ndifferential_evolution step 30: f(x)= 3194.53\ndifferential_evolution step 31: f(x)= 3194.53\n\n\n\ncmr_results\n\n[     fun: 2975.134867581669\n      jac: array([-0.06730261, -0.19517756, -0.01059561,  0.04656613, -0.01514309,\n        -9.09153637,  7.70419319,  0.        ,  0.07862582, -0.03537934,\n        -0.04138201])\n  message: 'Optimization terminated successfully.'\n     nfev: 7377\n      nit: 40\n  success: True\n        x: array([6.94621761e-01, 9.00820304e-01, 8.27694497e-01, 6.29486794e-01,\n        8.02814419e-02, 1.00000000e+00, 2.22044605e-16, 6.19704899e+01,\n        4.03765537e-02, 1.29131693e-01, 5.84921887e+00]),\n      fun: 3160.6782599545445\n      jac: array([ 0.05397851, -0.10190888, -0.00568434,  0.00204636, -0.00422915,\n         0.00104592, -0.00304681,  0.01127773,  0.02678462,  0.01732587,\n        -0.00927685])\n  message: 'Optimization terminated successfully.'\n     nfev: 6900\n      nit: 31\n  success: True\n        x: array([0.77422693, 0.84571273, 0.82856235, 0.13214982, 0.88051187,\n        0.29551315, 4.76381681, 0.31516321, 0.03725594, 0.12108304,\n        1.3268154 ])]\n\n\nfrom numba import int32\n\n@njit(nogil=True)\ndef simulate_array_from_presentations(model_class, parameters, presentations, experiment_count):\n\n    # simulate retrieval for the specified number of times, tracking results in trials array\n    trials = np.zeros((experiment_count * len(presentations), np.max(presentations)), dtype=int32)\n    \n    for experiment in range(experiment_count):\n        for trial_index in range(len(presentations)):\n        \n            # retrieve presentation sequence for this trial and measure number of unique items\n            #presentation = presentations[trial_index]\n            presentation = presentations[trial_index][presentations[trial_index] > 0] -1\n            item_count = np.max(presentation)+1\n            \n            # simulate recall and identify first study position of each recalled item\n            model = model_class(item_count, len(presentation), parameters)\n            model.experience(model.items[presentation])\n            recalled = model.free_recall()\n            \n            for i in range(len(recalled)):\n                trials[experiment*len(presentations) + trial_index, i] = find_first(recalled[i], presentation) + 1\n    \n    return trials\n\n@njit(nogil=True)\ndef find_first(item, vec):\n    \"\"\"return the index of the first occurence of item in vec\"\"\"\n    for i in range(len(vec)):\n        if item == vec[i]:\n            return i\n    return -1\n# simulate data corresponding to each cmr_result\n\nfrom numba.typed import Dict\nfrom numba.core import types\nfrom numpy import matlib\n\nexperiment_count = 1000\n\nsim_trials = []\nsim_presentations = []\nfitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\n\nfor i, cmr_result in enumerate(cmr_results):\n\n    for j in range(len(cmr_result.x)):\n        fitted_parameters[cmr_free_parameters[j]] = cmr_result.x[j]\n        \n    fitted_parameters['sampling_rule'] = 0\n    fitted_parameters['mfc_familiarity_scale'] = 0\n    fitted_parameters['mcf_familiarity_scale'] = 0\n    fitted_parameters['drift_familiarity_scale'] = 0\n    fitted_parameters['delay_drift_rate'] = 0\n\n    sim_trials.append(simulate_array_from_presentations(\n        init_cmr, fitted_parameters, presentations[list_types==i], experiment_count))\n    sim_presentations.append(np.matlib.repmat(presentations[list_types==i], experiment_count, 1))\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15/2), sharey=True)\n\nfor condition in range(2):\n    \n    test_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition, :-3])\n    axes[0].plot(np.arange(len(test_spc)), test_spc, label=conditions[condition])\n\n    sim_spc = flex_mixed_spc(sim_trials[condition], sim_presentations[condition][:, :-3])\n    axes[1].plot(np.arange(len(sim_spc)), sim_spc, label=conditions[condition])\n\n#axes[0].set(xlabel='Presentation Position', ylabel='Recall Rate') \n#axes[1].set(xlabel='Presentation Position', ylabel='Recall Rate') \nfig.suptitle(\"Serial Position Curve\")\naxes[1].legend(title='Condition', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=3)\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15/2), sharey=True)\n\nitem_count = 25\nfor condition in range(2):\n\n    test_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition, :-3])\n    test_crp[item_count-1] = np.nan\n    axes[0].plot(np.arange(len(test_crp)), test_crp, label=conditions[condition])\n\n    sim_crp = flex_mixed_crp(sim_trials[condition], sim_presentations[condition][:, :-3])\n    sim_crp[item_count-1] = np.nan\n    axes[1].plot(np.arange(len(sim_crp)), sim_crp, label=conditions[condition])\n\n#plt.xlabel('Lag')\n#plt.ylabel('Conditional Response Probability')   \naxes[0].set_xticks(np.arange(0, len(test_crp), 4))\naxes[0].set_xticklabels(np.arange(0, len(test_crp), 4) - (item_count - 1))\naxes[1].set_xticks(np.arange(0, len(sim_crp), 4))\naxes[1].set_xticklabels(np.arange(0, len(sim_crp), 4) - (item_count - 1))\nfig.suptitle('Lag-CRP')\naxes[1].legend(title='Condition', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=3)\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15/2), sharey=True)\n\nfor condition in range(2):\n\n    test_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition, :-3])\n    axes[0].plot(np.arange(len(test_pfr)), test_pfr, label=conditions[condition])\n\n    sim_pfr = flex_mixed_pfr(sim_trials[condition], sim_presentations[condition][:, :-3])\n    axes[1].plot(np.arange(len(sim_pfr)), sim_pfr, label=conditions[condition])\n\n#axes[0].set(xlabel='Presentation Position', ylabel='First Recall Rate') \n#axes[1].set(xlabel='Presentation Position', ylabel='First Recall Rate') \nfig.suptitle(\"Probability of First Recall\")\naxes[1].legend(title='Condition', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=3)\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15/2), sharey=True)\n\nfor condition in range(2):\n    test_csp = fast_csp(trials[list_types==condition], list_length)\n    test_csp[test_csp==0] = np.nan\n    axes[0].plot(np.arange(list_length+1), test_csp, label=conditions[condition])\n\n    sim_csp = fast_csp(sim_trials[condition], list_length)\n    sim_csp[sim_csp==0] = np.nan\n    axes[1].plot(np.arange(list_length+1), sim_csp, label=conditions[condition])\n\n#plt.xlabel('Recall Position')\n#plt.ylabel('Conditional Stop Probability')\nfig.suptitle('Conditional Stop Probability')\naxes[1].legend(title='Condition', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=3)"
  },
  {
    "objectID": "library\\datasets\\Data_Simulation.html#demo-dependencies",
    "href": "library\\datasets\\Data_Simulation.html#demo-dependencies",
    "title": "compmemlearn",
    "section": "Demo Dependencies",
    "text": "from compmemlearn.models import Classic_CMR\nfrom compmemlearn.datasets import prepare_murdock1970_data, prepare_lohnas2014_data\nfrom compmemlearn.analyses import fast_rpl, fast_spc\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nfrom numba.core import types\nfrom psifr import fr\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numba import njit\nimport numpy.matlib\n\nmurd_trials0, murd_events0, murd_length0 = prepare_murdock1970_data('../../data/mo1970.txt')\n\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\ncmr_result = np.array(\n    [5.84259066e-01, 4.27375824e-03, 7.21454638e-01, 8.17704509e-01,\n     1.00000000e+00, 9.88623591e-02, 9.31571732e+00, 7.54040329e+01,\n     3.14204629e-02, 3.36598109e-01, 9.99452206e+00, 9.95457387e-01])\n\nfitted_parameters = Dict.empty(\n    key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(cmr_result)):\n    fitted_parameters[cmr_free_parameters[i]] = cmr_result[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\n@njit(nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)"
  },
  {
    "objectID": "library\\datasets\\Data_Simulation.html#functions",
    "href": "library\\datasets\\Data_Simulation.html#functions",
    "title": "compmemlearn",
    "section": "Functions",
    "text": "Pure List Simulation\n# export\nimport pandas as pd\nfrom psifr import fr\nimport numpy as np\nfrom numba import int32\nfrom numba import njit\n\ndef simulate_df(model, experiment_count, first_recall_item=None):\n    \"\"\"\n    Initialize a model with specified parameters and experience sequences and \n    then populate a psifr-formatted dataframe with the outcomes of performing `free recall`. \n    \n    **Required model attributes**:\n    - item_count: specifies number of items encoded into memory\n    - context: vector representing an internal contextual state\n    - experience: adding a new trace to the memory model\n    - free_recall: function that freely recalls a given number of items or until recall stops\n    \"\"\"\n    \n    # encode items\n    model.experience(model.items)\n\n    # simulate retrieval for the specified number of times, tracking results in df\n    data = []\n    for experiment in range(experiment_count):\n        data += [[experiment, 0, 'study', i + 1, i] for i in range(model.item_count)]\n    for experiment in range(experiment_count):\n        if first_recall_item is not None:\n            model.force_recall(first_recall_item)\n        data += [[experiment, 0, 'recall', i + 1, o] for i, o in enumerate(model.free_recall())]\n    data = pd.DataFrame(data, columns=['subject', 'list', 'trial_type', 'position', 'item'])\n    merged = fr.merge_free_recall(data)\n    \n    return merged\n\nsimulate_data = simulate_df\n# export\n\n@njit(fastmath=True, nogil=True)\ndef simulate_array(model, experiment_count, first_recall_item=None):\n    \n    # encode items\n    model.experience(model.items)\n\n    # simulate retrieval for the specified number of times, tracking results in array\n    trials = np.zeros((experiment_count, len(model.items)), dtype=int32)\n    \n    for trial_index in range(len(trials)):\n        \n        recalled = model.free_recall()\n        trials[trial_index, :len(recalled)] = recalled + 1\n        \n    return trials\n\n\nImpure Lists (Possible Repetitions)\n# export\n\n@njit(nogil=True)\ndef simulate_array_from_presentations(model_class, parameters, presentations, experiment_count):\n\n    # simulate retrieval for the specified number of times, tracking results in trials array\n    trials = np.zeros((experiment_count * len(presentations), np.max(presentations)+1), dtype=int32)\n    \n    for experiment in range(experiment_count):\n        for trial_index in range(len(presentations)):\n        \n            # retrieve presentation sequence for this trial and measure number of unique items\n            presentation = presentations[trial_index]\n            item_count = np.max(presentation)+1\n            \n            # simulate recall and identify first study position of each recalled item\n            model = model_class(item_count, len(presentation), parameters)\n            model.experience(model.items[presentation])\n            recalled = model.free_recall()\n            \n            for i in range(len(recalled)):\n                trials[experiment*len(presentations) + trial_index, i] = find_first(recalled[i], presentation) + 1\n    \n    return trials\n\n@njit(nogil=True)\ndef find_first(item, vec):\n    \"\"\"return the index of the first occurence of item in vec\"\"\"\n    for i in range(len(vec)):\n        if item == vec[i]:\n            return i\n    return -1"
  },
  {
    "objectID": "library\\datasets\\Data_Simulation.html#demo",
    "href": "library\\datasets\\Data_Simulation.html#demo",
    "title": "compmemlearn",
    "section": "Demo",
    "text": "Mixed Lists: simulate_array_from_presentations\nMake sure: - presentation array is matched to trial array - mixed list trials are selected - experiment_count doesn’t disrupt alignment\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nexperiment_count = 1000\n\nfit_sources = ['lohnas_all', 'lohnas_4', 'murdock1962', 'murdock1970']\n\nfit_stats = [\n    np.array([8.65828835e-01, 2.26715503e-01, 9.52028097e-01, 2.63844603e-02,\n       1.47259363e-07, 4.31890546e-01, 2.63745217e+00, 2.98606729e+01,\n       2.51644003e-02, 1.01406301e-01, 1.02305123e+00, 9.80106784e-01]),\n    np.array([8.06135392e-01, 3.07112592e-01, 9.55038268e-01, 1.15022323e-01,\n       1.60855931e-02, 5.07853225e-01, 4.61059897e-01, 7.16087569e+01,\n       2.52322009e-02, 9.37792238e-02, 2.02696856e+00, 9.24630111e-01]),\n    np.array([5.88304182e-01, 3.76144942e-02, 7.51294302e-01, 2.91680115e-01,\n       1.00000000e+00, 1.39633721e-01, 5.62625588e+00, 4.28789782e+01,\n       2.40537436e-02, 2.61824232e-01, 5.32941045e+00, 9.34036191e-01]),\n    np.array([5.79524319e-01, 4.07083020e-03, 7.24717634e-01, 7.47425733e-01,\n       1.00000000e+00, 9.58358158e-02, 9.55947397e+00, 8.71434638e+01,\n       3.13827247e-02, 3.36754300e-01, 9.25336064e+00, 9.95710836e-01])\n]\n\nfor i in range(len(fit_sources)):\n    cmr_result = fit_stats[i]\n\n    fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\n    for j in range(len(cmr_result)):\n        fitted_parameters[cmr_free_parameters[j]] = cmr_result[j]\n        \n    fitted_parameters['sampling_rule'] = 0\n    fitted_parameters['mfc_familiarity_scale'] = 0\n    fitted_parameters['mcf_familiarity_scale'] = 0\n    fitted_parameters['drift_familiarity_scale'] = 0\n\n    new_sim_array = simulate_array_from_presentations(init_cmr, fitted_parameters, presentations[list_types==4], experiment_count)\n    \n    result = fast_rpl(np.matlib.repmat(presentations[list_types==4], experiment_count, 1), new_sim_array)\n    binned = np.zeros(5)\n    binned[0] = result[0]\n    binned[1] = result[1]\n    binned[2] = (result[2] + result[3])/2\n    binned[3] = (result[4] + result[5] + result[6])/3\n    binned[4] = (result[7] + result[8] + result[9])/3\n    print(fit_sources[i], ':')\n    print(binned)\n    print()\n\nlohnas_all :\n[0.35386862 0.43319286 0.46776964 0.48948095 0.49651786]\n\nlohnas_4 :\n[0.35961046 0.50194643 0.53528214 0.55109286 0.55234048]\n\nmurdock1962 :\n[0.19713656 0.34533214 0.34188036 0.34759405 0.34767381]\n\nmurdock1970 :\n[0.15170502 0.26838214 0.25996429 0.25957381 0.25287619]\n\n\n\n\nfit_sources = ['lohnas_all', 'lohnas_4', 'murdock1962', 'murdock1970']\n\nfit_rpls = [[0.35386862, 0.43319286, 0.46776964, 0.48948095, 0.49651786], \n            [0.35961046, 0.50194643, 0.53528214, 0.55109286, 0.55234048],\n            [0.19713656, 0.34533214, 0.34188036, 0.34759405, 0.34767381],\n            [0.15170502, 0.26838214, 0.25996429, 0.25957381, 0.25287619]]\n\nfor i in range(len(fit_sources)):\n    plt.plot(fit_rpls[i], label=fit_sources[i])\n\nresult = fast_rpl(presentations[list_types==4], trials[list_types==4])\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nplt.plot(binned, label='data')\nlags = ['N/A', '0', '1-2', '3-5', '6-8']\nplt.xticks(np.arange(len(lags)), lags)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x1f757ae98e0>\n\n\n\n\n\n\n\nPure Lists: simulate_df\n\nmodel = Classic_CMR(murd_length0, murd_length0, fitted_parameters)\nsim_df = simulate_df(model, 1000)\n\nsim_df.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      0\n      0\n      1\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      0\n      0\n      2\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      0\n      0\n      3\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      0\n      0\n      4\n      5\n      2.0\n      True\n      True\n      0\n      False\n    \n  \n\n\n\n\n\n\nPure Lists: simulate_array vs simulate_array_from_presentations\n\n# implementation that assumes pure lists\nmodel = Classic_CMR(murd_length0, murd_length0, fitted_parameters)\noriginal_sim_array = simulate_array(model, len(murd_trials0))#*100)\noriginal_spc = fast_spc(original_sim_array, murd_length0)\n\noriginal_sim_array\n\narray([[18, 14, 19, ...,  0,  0,  0],\n       [17,  0,  0, ...,  0,  0,  0],\n       [18,  1, 19, ...,  0,  0,  0],\n       ...,\n       [ 8, 20, 15, ...,  0,  0,  0],\n       [15, 20, 17, ...,  0,  0,  0],\n       [19, 20,  1, ...,  0,  0,  0]])\n\n\n\n# presentation-based implementation\npresentations = np.zeros((len(murd_trials0), murd_length0), dtype=int)\npresentations[:] = np.arange(murd_length0)\n\nnew_sim_array = simulate_array_from_presentations(init_cmr, fitted_parameters, presentations, 1)\nnew_spc = fast_spc(new_sim_array, murd_length0)\n\nnew_sim_array\n\narray([[18, 20,  5, ...,  0,  0,  0],\n       [13, 12,  8, ...,  0,  0,  0],\n       [19, 20,  3, ...,  0,  0,  0],\n       ...,\n       [20, 19, 15, ...,  0,  0,  0],\n       [17, 18, 14, ...,  0,  0,  0],\n       [19,  1, 20, ...,  0,  0,  0]])\n\n\n\n# comparison\nimport matplotlib.pyplot as plt\n\nplt.plot(fast_spc(original_sim_array, murd_length0), label='original')\nplt.plot(fast_spc(new_sim_array, murd_length0), label='new')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x2ddc4396460>"
  },
  {
    "objectID": "library\\datasets\\HowaKaha05.html",
    "href": "library\\datasets\\HowaKaha05.html",
    "title": "compmemlearn",
    "section": "",
    "text": "# default_exp datasets\n\nHowaKaha05 Dataset\n\nKahana, M. J., & Howard, M. W. (2005). Spacing and lag effects in free recall of pure lists. Psychonomic Bulletin & Review, 12(1), 159-164.\n\nSixty-six students studied and attempted free recall of 15 different lists of high-frequency nouns drawn from the Toronto Noun Pool (Friendly, Franklin, Hoffman, & Rubin, 1982). The lists consisted of 30 words, each repeated three times for a total of 90 presentations per list. List presentation was auditory, and the subjects made their responses vocally into a headset microphone. The words were presented at a rate of 1.5 sec. After list presentation, the subjects were given a distractor task involving simple arithmetic problems of the form A B C ?. The subjects had to correctly answer 15 problems in a row before they could proceed to the recall phase.\nThere were three list types: massed, spaced short, and spaced long. In the massed lists, each word was repeated three times successively. In the spaced-short lists, the presentation order was randomized, subject to the constraint that the lag between repetitions was at least 2 and no more than 6. For the spaced-long lists, presentation order was randomized, subject to the constraint that interrepetition lags were at least 6 and not more than 20.\nAs is typical in free recall studies, we took mea-sures to eliminate warm-up effects by excluding the first 2 lists from our data analyses. One of these first 2 practice lists was massed, and the other was randomly chosen to be either spaced short or spaced long. Of the subsequent 12 lists, 4 were massed, 4 were spaced short, and 4 were spaced long, presented in an individually randomized order for each subject.\nimport scipy.io as sio\nimport numpy as np\nimport pandas as pd\nfrom psifr import fr\n# export\ndef prepare_howakaha05_data(path):\n    \"\"\"\n    Prepares data formatted like `../data/HowaKaha05.dat` for fitting.\n    \"\"\"\n    \n    with open(path) as f:\n        howa_data = f.read()\n\n    subject_count = 66\n    trial_count = 15\n    total_lines = 66 * 15 * 5\n    list_length = 90\n\n    lines = [each.split('\\t') for each in howa_data.split('\\n')]\n    trial_info_inds = np.arange(1, total_lines, 5)\n    presentation_info_inds = np.arange(2, total_lines, 5)\n    recall_info_inds = np.arange(4, total_lines, 5)\n\n    # build vectors/matrices tracking list types and presentation item numbers across trials\n    list_types = np.array([int(lines[trial_info_inds[i]-1][2]) for i in range(subject_count * trial_count)])\n    subjects = np.array([int(lines[trial_info_inds[i]-1][0]) for i in range(subject_count * trial_count)])\n    pres_itemnos = np.array([[int(each) for each in lines[presentation_info_inds[i]-1][:-1]] for i in range(\n        subject_count * trial_count)])\n        \n    # convert pres_itemnos into rows of unique indices for easier model encoding\n    presentations = []\n    for i in range(len(pres_itemnos)):\n        seen = []\n        presentations.append([])\n        for p in pres_itemnos[i]:\n            if p not in seen:\n                seen.append(p)\n            presentations[-1].append(seen.index(p))\n    presentations = np.array(presentations)\n\n    # track recalls, discarding intrusions\n    trials = []\n    for i in range(subject_count * trial_count):\n        trials.append([])\n        \n        # if it can be cast as a positive integer and is not yet in the recall sequence, it's not an intrusion\n        trial = lines[recall_info_inds[i]-1][:-1]\n        for t in trial:\n            try:\n                t = int(t)\n                if (t in pres_itemnos[i]):\n                    item = presentations[i][np.where(pres_itemnos[i] == t)[0][0]]+1\n                    if item not in trials[-1]:\n                        trials[-1].append(item)\n            except ValueError:\n                continue\n        \n        # pad with zeros to make sure the list is the right length\n        while len(trials[-1]) < list_length:\n            trials[-1].append(0)\n            \n    trials = np.array(trials)\n\n    # encode dataset into psifr format\n    data = []\n    for trial_index, trial in enumerate(trials):\n        presentation = presentations[trial_index]\n        \n        # every time the subject changes, reset list_index\n        if not data or data[-1][0] != subjects[trial_index]:\n            list_index = 0\n        list_index += 1\n        \n        # add study events\n        for presentation_index, presentation_event in enumerate(presentation):\n            data += [[subjects[trial_index], \n                      list_index, 'study', presentation_index+1, presentation_event,  list_types[trial_index]\n                     ]]\n            \n        # add recall events\n        for recall_index, recall_event in enumerate(trial):\n            if recall_event != 0:\n                data += [[subjects[trial_index], list_index, \n                          'recall', recall_index+1, presentation[recall_event-1], list_types[trial_index]\n                         ]]\n                \n    data = pd.DataFrame(data, columns=[\n        'subject', 'list', 'trial_type', 'position', 'item', 'condition'])\n    merged = fr.merge_free_recall(data, list_keys=['condition'])\n    \n    return trials, merged, list_length, presentations, list_types, data, subjects\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_howakaha05_data(\n    '../../data/HowaKaha05.dat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      118\n      1\n      0\n      1\n      3.0\n      True\n      True\n      0\n      False\n      0\n    \n    \n      1\n      118\n      1\n      0\n      1\n      9.0\n      False\n      True\n      1\n      False\n      0\n    \n    \n      2\n      118\n      1\n      0\n      2\n      3.0\n      True\n      True\n      0\n      False\n      0\n    \n    \n      3\n      118\n      1\n      0\n      2\n      9.0\n      False\n      True\n      1\n      False\n      0\n    \n    \n      4\n      118\n      1\n      0\n      3\n      3.0\n      True\n      True\n      0\n      False\n      0"
  },
  {
    "objectID": "library\\datasets\\Lohnas2014.html",
    "href": "library\\datasets\\Lohnas2014.html",
    "title": "compmemlearn",
    "section": "",
    "text": "# default_exp datasets\n\nLohnas2014 Dataset\n\nSiegel, L. L., & Kahana, M. J. (2014). A retrieved context account of spacing and repetition effects in free recall. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(3), 755.\n\nAcross 4 sessions, 35 subjects performed delayed free recall of 48 lists. Subjects were University of Pennsylvania undergraduates, graduates and staff, age 18-32. List items were drawn from a pool of 1638 words taken from the University of South Florida free association norms (Nelson, McEvoy, & Schreiber, 2004; Steyvers, Shiffrin, & Nelson, 2004, available at http://memory.psych.upenn.edu/files/wordpools/PEERS_wordpool.zip). Within each session, words were drawn without replacement. Words could repeat across sessions so long as they did not repeat in two successive sessions. Words were also selected to ensure that no strong semantic associates co-occurred in a given list (i.e., the semantic relatedness between any two words on a given list, as determined using WAS (Steyvers et al., 2004), did not exceed a threshold value of 0.55).\nSubjects encountered four different types of lists: 1. Control lists that contained all once-presented items;\n2. pure massed lists containing all twice-presented items; 3. pure spaced lists consisting of items presented twice at lags 1-8, where lag is defined as the number of intervening items between a repeated item’s presentations; 4. mixed lists consisting of once presented, massed and spaced items. Within each session, subjects encountered three lists of each of these four types.\nIn each list there were 40 presentation positions, such that in the control lists each position was occupied by a unique list item, and in the pure massed and pure spaced lists, 20 unique words were presented twice to occupy the 40 positions. In the mixed lists 28 once-presented and six twice-presented words occupied the 40 positions. In the pure spaced lists, spacings of repeated items were chosen so that each of the lags 1-8 occurred with equal probability. In the mixed lists, massed repetitions (lag=0) and spaced repetitions (lags 1-8) were chosen such that each of the 9 lags of 0-8 were used exactly twice within each session. The order of presentation for the different list types was randomized within each session. For the first session, the first four lists were chosen so that each list type was presented exactly once. An experimenter sat in with the subject for these first four lists, though no subject had difficulty understanding the task.\nThe data for this experiment is stored in data/repFR.mat. We define a unique prepare_lohnas2014_data function to build structures from the dataset that works with our existing data analysis and fitting functions.\nLike in prepare_murdock1962_data, we need list lengths, a data frame for visualizations with psifir, and a trials array encoding recall events as sequences of presentation positions. But we’ll also need an additional array tracking presentation order, too.\nimport scipy.io as sio\nimport numpy as np\nimport pandas as pd\nfrom psifr import fr\n# export\n\ndef prepare_lohnas2014_data(path):\n    \"\"\"\n    Prepares data formatted like `data/repFR.mat` for fitting.\n    \"\"\"\n    \n    # load all the data\n    matfile = sio.loadmat(path, squeeze_me=True)['data'].item()\n    subjects = matfile[0]\n    pres_itemnos = matfile[4]\n    recalls = matfile[6]\n    list_types = matfile[7]\n    list_length = matfile[12]\n    \n    # convert pres_itemnos into rows of unique indices for easier model encoding\n    presentations = []\n    for i in range(len(pres_itemnos)):\n        seen = []\n        presentations.append([])\n        for p in pres_itemnos[i]:\n            if p not in seen:\n                seen.append(p)\n            presentations[-1].append(seen.index(p))\n    presentations = np.array(presentations)\n\n    # discard intrusions from recalls\n    trials = []\n    for i in range(len(recalls)):\n        trials.append([])\n        \n        trial = list(recalls[i])\n        for t in trial:\n            if (t > 0) and (t not in trials[-1]):\n                trials[-1].append(t)\n        \n        while len(trials[-1]) < list_length:\n            trials[-1].append(0)\n            \n    trials = np.array(trials)\n    \n    # encode dataset into psifr format\n    data = []\n    for trial_index, trial in enumerate(trials):\n        presentation = presentations[trial_index]\n        \n        # every time the subject changes, reset list_index\n        if not data or data[-1][0] != subjects[trial_index]:\n            list_index = 0\n        list_index += 1\n        \n        # add study events\n        for presentation_index, presentation_event in enumerate(presentation):\n            data += [[subjects[trial_index], \n                      list_index, 'study', presentation_index+1, presentation_event,  list_types[trial_index]\n                     ]]\n            \n        # add recall events\n        for recall_index, recall_event in enumerate(trial):\n            if recall_event != 0:\n                data += [[subjects[trial_index], list_index, \n                          'recall', recall_index+1, presentation[recall_event-1], list_types[trial_index]\n                         ]]\n                \n    data = pd.DataFrame(data, columns=[\n        'subject', 'list', 'trial_type', 'position', 'item', 'condition'])\n    merged = fr.merge_free_recall(data, list_keys=['condition'])\n    \n    return trials, merged, list_length, presentations, list_types, data, subjects\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nevents.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n      condition\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      1\n      1.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      1\n      1\n      1\n      1\n      2\n      2.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      2\n      1\n      1\n      2\n      3\n      3.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      3\n      1\n      1\n      3\n      4\n      4.0\n      True\n      True\n      0\n      False\n      4\n    \n    \n      4\n      1\n      1\n      4\n      5\n      5.0\n      True\n      True\n      0\n      False\n      4"
  },
  {
    "objectID": "library\\datasets\\Murdock1962.html",
    "href": "library\\datasets\\Murdock1962.html",
    "title": "compmemlearn",
    "section": "",
    "text": "# default_exp datasets\n\nMurdock1962 Dataset\n\nMurdock, B. B., Jr. (1962). The serial position effect of free recall. Journal of Experimental Psychology, 64(5), 482-488. https://doi.org/10.1037/h0045106\n\nOur data structure associated with Murdock (1962) has three LL structures that each seem to correspond to a different data set with different list lengths. Inside each structure is: - recalls with 1200 rows and 50 columns. Each row presumably represents a subject, and each column seems to correspond to a recall position, with -1 coded for intrusions. MurdData_clean.mat probably doesn’t have these intrusions coded at all. - listlength is an integer indicating how long the studied list is. - subject is a 1200x1 vector coding the identities of each subject for each row. Each subject seems to get 80 rows a piece. He really got that much data for each subject? - session similarly codes the index of the session under consideration, and it’s always 1 in this case. - presitemnumbers probably codes the number associated with each item. Is just its presentation index.\nWe’ll enable selection of relevant information from these structures based on which LL structure we’re interested in using a dataset_index parameter.\n# export\n\nimport scipy.io as sio\nimport numpy as np\nimport pandas as pd\nfrom psifr import fr\n\ndef prepare_murdock1962_data(path, dataset_index=0):\n    \"\"\"\n    Prepares data formatted like `data/MurdData_clean.mat` for fitting.\n\n    Loads data from `path` with same format as `data/MurdData_clean.mat` and\n    returns a selected dataset as an array of unique recall trials and a\n    dataframe of unique study and recall events organized according to `psifr`\n    specifications.\n\n    **Arguments**:\n    - path: source of data file\n    - dataset_index: index of the dataset to be extracted from the file\n\n    **Returns**:\n    - trials: int64-array where rows identify a unique trial of responses and\n        columns corresponds to a unique recall index.\n    - merged: as a long format table where each row describes one study or\n        recall event.\n    - list_length: length of lists studied in the considered dataset\n    \"\"\"\n\n    # load all the data\n    matfile = sio.loadmat(path, squeeze_me=True)\n    murd_data = [matfile['data'].item()[0][i].item() for i in range(3)]\n\n    # encode dataset into psifr format\n    trials, list_length, subjects = murd_data[dataset_index][:3]\n    trials = trials.astype('int64')\n\n    data = []\n    for trial_index, trial in enumerate(trials):\n\n        # every time the subject changes, reset list_index\n        if not data or data[-1][0] != subjects[trial_index]:\n            list_index = 0\n        list_index += 1\n\n        # add study events\n        for i in range(list_length):\n            data += [[subjects[trial_index],\n                      list_index, 'study', i+1, i+1]]\n\n        # add recall events\n        for recall_index, recall_event in enumerate(trial):\n            if recall_event != 0:\n                data += [[subjects[trial_index], list_index,\n                          'recall', recall_index+1, recall_event]]\n\n    data = pd.DataFrame(data, columns=[\n        'subject', 'list', 'trial_type', 'position', 'item'])\n    merged = fr.merge_free_recall(data)\n    return trials, merged, list_length\n\nmurd_trials, murd_events, murd_length = prepare_murdock1962_data(\n    '../../data/MurdData_clean.mat', 0)\n\nmurd_events.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      5.0\n      True\n      True\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      7.0\n      True\n      True\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\datasets\\MurdockOkada1970.html#murdockokada1970-dataset",
    "href": "library\\datasets\\MurdockOkada1970.html#murdockokada1970-dataset",
    "title": "compmemlearn",
    "section": "MurdockOkada1970 Dataset",
    "text": "Murdock, B. B., & Okada, R. (1970). Interresponse times in single-trial free recall. Journal of Experimental Psychology, 86(2), 263.\n\nAuthors investigated interresponse times in single-trial free recall. Each of 72 undergraduates was given 20 test lists with 20-word lists visually presented at either 60 or 120 words/min. The format of the data in these files is as follows:\nRow 1: subject/trial information\nRow 2: serial position as a function of output position.\nRow 3: inter-response time as a function of output position\nThe code 88 means that the subject made an extra-list intrusion.\nimport scipy.io as sio\nimport numpy as np\nimport pandas as pd\nfrom psifr import fr\n# export\n\ndef prepare_murdock1970_data(path):\n    \"\"\"\n    Prepares data formatted like `data/MurdData_clean.mat` for fitting.\n\n    Loads data from `path` with same format as `data/MurdData_clean.mat` and \n    returns a selected dataset as an array of unique recall trials and a \n    dataframe of unique study and recall events organized according to `psifr`\n    specifications.  \n\n    **Arguments**:  \n    - path: source of data file  \n    - dataset_index: index of the dataset to be extracted from the file\n\n    **Returns**:\n    - trials: int64-array where rows identify a unique trial of responses and \n        columns corresponds to a unique recall index.  \n    - merged: as a long format table where each row describes one study or \n        recall event.  \n    - list_length: length of lists studied in the considered dataset\n    \"\"\"\n    \n    with open(path) as f:\n        oka_data = f.read()\n\n    counter = 0\n    trials = []\n    subjects = []\n    list_length = 20\n\n    for line in oka_data.split('\\n'):\n\n        if not line:\n            continue\n\n        # build subjects array\n        if counter == 0:\n            subjects.append(int(line.strip().split('    ')[1]))\n\n        # build trials array\n        if counter == 1:\n\n            trial = [int(each) for each in line.strip().split('    ')]\n            trial = [each for each in trial if each <= 20]\n            already = []\n            for each in trial:\n                if each not in already:\n                    already.append(each)\n            trial = already\n            \n            while len(trial) < 13:\n                trial.append(0)\n\n            trials.append(trial)\n\n        # keep track of which row we are on for the given trial\n        counter += 1\n        if counter == 3:\n            counter = 0\n\n    trials = np.array(trials).astype('int64')\n    \n    data = []\n    for trial_index, trial in enumerate(trials):\n\n        # every time the subject changes, reset list_index\n        if not data or data[-1][0] != subjects[trial_index]:\n            list_index = 0\n        list_index += 1\n\n        # add study events\n        for i in range(list_length):\n            data += [[subjects[trial_index], \n                      list_index, 'study', i+1, i+1]]\n\n        # add recall events\n        for recall_index, recall_event in enumerate(trial):\n            if recall_event != 0:\n                data += [[subjects[trial_index], list_index, \n                          'recall', recall_index+1, recall_event]]\n\n    data = pd.DataFrame(data, columns=[\n        'subject', 'list', 'trial_type', 'position', 'item'])\n    merged = fr.merge_free_recall(data)\n    return trials, merged, list_length\n\nmurd_trials, murd_events, murd_length = prepare_murdock1970_data('../../data/mo1970.txt')\nmurd_events.head()\n\n\n\n\n  \n    \n      \n      subject\n      list\n      item\n      input\n      output\n      study\n      recall\n      repeat\n      intrusion\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      1\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      1\n      1\n      1\n      2\n      2\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      2\n      1\n      1\n      3\n      3\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      3\n      1\n      1\n      4\n      4\n      NaN\n      True\n      False\n      0\n      False\n    \n    \n      4\n      1\n      1\n      5\n      5\n      NaN\n      True\n      False\n      0\n      False"
  },
  {
    "objectID": "library\\datasets\\PEERS.html",
    "href": "library\\datasets\\PEERS.html",
    "title": "compmemlearn",
    "section": "",
    "text": "# default_exp datasets\n\nPEERS Dataset\nThe Penn Electrophysiology of Encoding and Retrieval Study (PEERS) is a multi-session experiment looking at scalp EEG during free recall and recognition. It recruits both younger adults (16-30) and older adults (60-90). For now, we exclusively use the free recall trials performed by younger adult participants.\nimport os\nfrom glob import glob\n\n\ndef prepare_peers_data(path):\n    \n    # build list of subject directories that excludes older subjects\n    with open(os.path.join(path, 'PEERS_older_adult_subject_list.txt')) as f:\n        older_subjects = [each for each in f.read().split('\\n')[:-1]]\n        \n    subject_dirs = [each for each in os.listdir(path) \n                    if each[:3] == 'LTP' and each[-3:] not in older_subjects]\n        \n    # loop through subjects\n    for subject_index, subject_dir in enumerate(subject_dirs):\n    \n        # loop through sessions\n        session_dirs = [\n            each for each in os.listdir(os.path.join(path, subject_dir)) \n            if each[:7] == 'session']\n        \n        for session_index, session_dir in enumerate(session_dirs):\n            \n            pass\n            # identify study events in session lag\n            \n            # for each trial, also track recall events\n            \n    return subject_dirs\n\nprepare_peers_data('../../data/ltpFR')\n\n['LTP063',\n 'LTP064',\n 'LTP065',\n 'LTP066',\n 'LTP067',\n 'LTP068',\n 'LTP069',\n 'LTP070',\n 'LTP071',\n 'LTP072',\n 'LTP073',\n 'LTP074',\n 'LTP075',\n 'LTP076',\n 'LTP077',\n 'LTP078',\n 'LTP079',\n 'LTP080',\n 'LTP081',\n 'LTP082',\n 'LTP083',\n 'LTP084',\n 'LTP085',\n 'LTP086',\n 'LTP087',\n 'LTP088',\n 'LTP089',\n 'LTP090',\n 'LTP091',\n 'LTP092',\n 'LTP093',\n 'LTP094',\n 'LTP095',\n 'LTP096',\n 'LTP097',\n 'LTP098',\n 'LTP099',\n 'LTP100',\n 'LTP101',\n 'LTP102',\n 'LTP103',\n 'LTP104',\n 'LTP105',\n 'LTP106',\n 'LTP107',\n 'LTP108',\n 'LTP109',\n 'LTP110',\n 'LTP111',\n 'LTP112',\n 'LTP113',\n 'LTP114',\n 'LTP115',\n 'LTP116',\n 'LTP117',\n 'LTP118',\n 'LTP119',\n 'LTP120',\n 'LTP121',\n 'LTP122',\n 'LTP123',\n 'LTP124',\n 'LTP125',\n 'LTP126',\n 'LTP127',\n 'LTP128',\n 'LTP129',\n 'LTP130',\n 'LTP131',\n 'LTP132',\n 'LTP133',\n 'LTP134',\n 'LTP135',\n 'LTP136',\n 'LTP137',\n 'LTP138',\n 'LTP139',\n 'LTP140',\n 'LTP141',\n 'LTP142',\n 'LTP143',\n 'LTP144',\n 'LTP145',\n 'LTP146',\n 'LTP147',\n 'LTP148',\n 'LTP149',\n 'LTP150',\n 'LTP151',\n 'LTP152',\n 'LTP153',\n 'LTP155',\n 'LTP158',\n 'LTP159',\n 'LTP161',\n 'LTP166',\n 'LTP167',\n 'LTP168',\n 'LTP172',\n 'LTP174',\n 'LTP181',\n 'LTP184',\n 'LTP185',\n 'LTP186',\n 'LTP187',\n 'LTP188',\n 'LTP189',\n 'LTP190',\n 'LTP191',\n 'LTP192',\n 'LTP193',\n 'LTP194',\n 'LTP195',\n 'LTP196',\n 'LTP197',\n 'LTP198',\n 'LTP199',\n 'LTP200',\n 'LTP201',\n 'LTP202',\n 'LTP207',\n 'LTP209',\n 'LTP210',\n 'LTP211',\n 'LTP212',\n 'LTP214',\n 'LTP215',\n 'LTP221',\n 'LTP224',\n 'LTP227',\n 'LTP228',\n 'LTP229',\n 'LTP230',\n 'LTP231',\n 'LTP232',\n 'LTP233',\n 'LTP234',\n 'LTP235',\n 'LTP236',\n 'LTP237',\n 'LTP238',\n 'LTP239',\n 'LTP240',\n 'LTP241',\n 'LTP242',\n 'LTP243',\n 'LTP244',\n 'LTP245',\n 'LTP246',\n 'LTP247',\n 'LTP248',\n 'LTP249',\n 'LTP250',\n 'LTP251',\n 'LTP252',\n 'LTP253',\n 'LTP254',\n 'LTP255',\n 'LTP256',\n 'LTP258',\n 'LTP259',\n 'LTP260',\n 'LTP261',\n 'LTP263',\n 'LTP264',\n 'LTP265',\n 'LTP267',\n 'LTP268',\n 'LTP269',\n 'LTP270',\n 'LTP271',\n 'LTP272',\n 'LTP273',\n 'LTP274',\n 'LTP275',\n 'LTP276',\n 'LTP277',\n 'LTP278',\n 'LTP279',\n 'LTP280',\n 'LTP281',\n 'LTP282',\n 'LTP283',\n 'LTP284',\n 'LTP285',\n 'LTP286',\n 'LTP287',\n 'LTP288',\n 'LTP289',\n 'LTP290',\n 'LTP291',\n 'LTP292',\n 'LTP293',\n 'LTP294',\n 'LTP295',\n 'LTP296',\n 'LTP297',\n 'LTP298',\n 'LTP299',\n 'LTP300']\n\n\ntrials, events, list_length = prepare_peers_data('../../data/ltpFR')\nevents.head()"
  },
  {
    "objectID": "library\\models\\Classic_CMR.html",
    "href": "library\\models\\Classic_CMR.html",
    "title": "compmemlearn",
    "section": "",
    "text": "# default_exp models\n\nClassic CMR\nThe Context Maintenance and Retrieval (CMR) as specified by Morton and Polyn (2016) takes the form of a simplified neural network with two interacting representations, a feature-based representation of the studied item and a contextual representation (the context layer, \\(C\\)). The two layers communicate with one another through two sets of associative connections represented by matrices \\(M^{FC}\\) and \\(M^{CF}\\). Each of these weight matrices contains both pre-experimental associations and new associations learned during the experiment.\n# export\n\nimport numpy as np\nfrom numba import float64, int32, boolean\nfrom numba.experimental import jitclass\nfrom compmemlearn.familiarity import familiarity_weighting\n\ncmr_spec = [\n    ('item_count', int32), \n    ('encoding_drift_rate', float64),\n    ('delay_drift_rate', float64),\n    ('start_drift_rate', float64),\n    ('recall_drift_rate', float64),\n    ('shared_support', float64),\n    ('item_support', float64),\n    ('learning_rate', float64),\n    ('primacy_scale', float64),\n    ('primacy_decay', float64),\n    ('stop_probability_scale', float64),\n    ('stop_probability_growth', float64),\n    ('choice_sensitivity', float64),\n    ('context', float64[::1]),\n    ('start_context_input', float64[::1]),\n    ('delay_context_input', float64[::1]),\n    ('preretrieval_context', float64[::1]),\n    ('recall', int32[::1]),\n    ('retrieving', boolean),\n    ('recall_total', int32),\n    ('primacy_weighting', float64[::1]),\n    ('probabilities', float64[::1]),\n    ('mfc', float64[:,::1]),\n    ('mcf', float64[:,::1]),\n    ('encoding_index', int32),\n    ('items', float64[:,::1]),\n    ('drift_familiarity_scale', float64),\n    ('mfc_familiarity_scale', float64),\n    ('mcf_familiarity_scale', float64),\n    ('sampling_rule', int32)\n]\n# export\n@jitclass(cmr_spec)\nclass Classic_CMR:\n\n    def __init__(self, item_count, presentation_count, parameters):\n\n        # store initial parameters\n        self.item_count = item_count\n        self.encoding_drift_rate = parameters['encoding_drift_rate']\n        self.delay_drift_rate = parameters['delay_drift_rate']\n        self.start_drift_rate = parameters['start_drift_rate']\n        self.recall_drift_rate = parameters['recall_drift_rate']\n        self.shared_support = parameters['shared_support']\n        self.item_support = parameters['item_support']\n        self.learning_rate = parameters['learning_rate']\n        self.primacy_scale = parameters['primacy_scale']\n        self.primacy_decay = parameters['primacy_decay']\n        self.stop_probability_scale = parameters['stop_probability_scale']\n        self.stop_probability_growth = parameters['stop_probability_growth']\n        self.choice_sensitivity = parameters['choice_sensitivity']\n        self.drift_familiarity_scale = parameters['drift_familiarity_scale']\n        self.mfc_familiarity_scale = parameters['mfc_familiarity_scale']\n        self.mcf_familiarity_scale = parameters['mcf_familiarity_scale']\n        self.sampling_rule = parameters['sampling_rule']\n        \n        # at the start of the list context is initialized with a state \n        # orthogonal to the pre-experimental context\n        # associated with the set of items\n        self.context = np.zeros(item_count + 2)\n        self.context[0] = 1\n        self.preretrieval_context = self.context\n        self.recall = np.zeros(item_count, int32) # recalls has at most `item_count` entries\n        self.retrieving = False\n        self.recall_total = 0\n\n        # predefine primacy weighting vectors\n        self.primacy_weighting = parameters['primacy_scale'] * np.exp(\n            -parameters['primacy_decay'] * np.arange(presentation_count)) + 1\n\n        # preallocate for outcome_probabilities\n        self.probabilities = np.zeros((item_count + 1))\n\n        # predefine contextual input vectors relevant for delay_drift_rate and start_drift_rate parameters\n        self.start_context_input = np.zeros((self.item_count+2))\n        self.start_context_input[0] = 1\n        self.delay_context_input = np.zeros((self.item_count+2))\n        self.delay_context_input[-1] = 1\n\n        # The two layers communicate with one another through two sets of \n        # associative connections represented by matrices Mfc and Mcf. \n        # Pre-experimental Mfc is 1-learning_rate and pre-experimental Mcf is\n        # item_support for i=j. For i!=j, Mcf is shared_support.\n        self.mfc = np.eye(item_count, item_count+2, 1) * (1-self.learning_rate)\n        self.mcf = np.ones((item_count, item_count)) * self.shared_support\n        for i in range(item_count):\n            self.mcf[i, i] = self.item_support\n        self.mcf =  np.vstack((np.zeros((1, item_count)), self.mcf, np.zeros((1, item_count))))\n        self.encoding_index = 0\n        self.items = np.eye(item_count, item_count)\n\n    def experience(self, experiences):\n        \n        for i in range(len(experiences)):\n            \n            mfc_familiarity = 1\n            mcf_familiarity = 1\n            if np.any(self.context[1:-1] > 0) and ((self.mfc_familiarity_scale != 0) or (self.mcf_familiarity_scale != 0)):\n                \n                similarity = np.dot(self.context[1:-1], experiences[i]) / (\n                    np.sqrt(np.dot(self.context[1:-1], self.context[1:-1])) * np.sqrt(\n                        np.dot(experiences[i],experiences[i])))\n            \n                mfc_familiarity = familiarity_weighting(self.mfc_familiarity_scale, similarity)\n                mcf_familiarity = familiarity_weighting(self.mcf_familiarity_scale, similarity)\n\n            self.update_context(self.encoding_drift_rate, experiences[i])\n            self.mfc += mfc_familiarity * self.learning_rate * np.outer(self.context, experiences[i]).T\n            self.mcf += mcf_familiarity * self.primacy_weighting[self.encoding_index] * np.outer(\n                self.context, experiences[i])\n            self.encoding_index += 1\n\n    def update_context(self, drift_rate, experience):\n\n        # first pre-experimental or initial context is retrieved\n        familiarity = 1\n        if len(experience) == len(self.mfc):\n            \n            if np.any(self.context[1:-1] > 0) and self.drift_familiarity_scale != 0:\n                \n                similarity = np.dot(self.context[1:-1], experience) / (\n                     np.sqrt(np.dot(self.context[1:-1], self.context[1:-1])) * np.sqrt(\n                          np.dot(experience,experience)))\n                familiarity = familiarity_weighting(self.drift_familiarity_scale, similarity)\n\n            context_input = np.dot(experience, self.mfc)\n            context_input = np.power(context_input, familiarity)\n\n            # if the context is not pre-experimental, the context is retrieved\n            context_input = context_input / np.sqrt(np.sum(np.square(context_input))) # norm to length 1\n            \n        else:\n            context_input = experience\n  \n        # updated context is sum of context and input, modulated by rho to have len 1 and some drift_rate\n        rho = np.sqrt(1 + np.square(min(drift_rate, 1.0)) * (\n            np.square(self.context * context_input) - 1)) - (\n                min(drift_rate, 1.0) * (self.context * context_input))\n        self.context = (rho * self.context) + (min(drift_rate, 1.0) * context_input)\n\n    def activations(self, probe, use_mfc=False):\n\n        if use_mfc:\n            return np.dot(probe, self.mfc) + 10e-7\n        else:\n            return np.dot(probe, self.mcf) + 10e-7\n\n    def outcome_probabilities(self):\n\n        self.probabilities[0] = min(self.stop_probability_scale * np.exp(\n            self.recall_total * self.stop_probability_growth), 1.0 - (\n                 (self.item_count-self.recall_total) * 10e-7))\n        self.probabilities[1:] = 10e-7\n\n        if self.probabilities[0] < (1.0 - ((self.item_count-self.recall_total) * 10e-7)):\n\n            # measure the activation for each item\n            activation = self.activations(self.context)\n\n            # already recalled items have zero activation\n            activation[self.recall[:self.recall_total]] = 0\n\n            if np.sum(activation) > 0:\n\n                # power sampling rule vs modified exponential sampling rule\n                if self.sampling_rule == 0:\n                    activation = np.power(activation, self.choice_sensitivity)\n                else:\n                    pre_activation = (2 * activation)/ self.choice_sensitivity\n                    activation = np.exp(pre_activation - np.max(pre_activation))\n                \n                # normalized result downweighted by stop prob is probability of choosing each item\n                self.probabilities[1:] = (1-self.probabilities[0]) * activation / np.sum(activation)\n            \n        return self.probabilities\n\n    def free_recall(self, steps=None):\n\n        # some amount of the pre-list context is reinstated before initiating recall\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        # number of items to retrieve is # of items left to recall if steps is unspecified\n        if steps is None:\n            steps = self.item_count - self.recall_total\n        steps = self.recall_total + steps\n        \n        # at each recall attempt\n        while self.recall_total < steps:\n\n            # the current state of context is used as a retrieval cue to attempt recall of a studied item\n            # compute outcome probabilities and make choice based on distribution\n            outcome_probabilities = self.outcome_probabilities()\n            if np.any(outcome_probabilities[1:]):\n                choice = np.sum(np.cumsum(outcome_probabilities) < np.random.rand(), dtype=int32)\n            else:\n                choice = 0\n\n            # resolve and maybe store outcome\n            # we stop recall if no choice is made (0)\n            if choice == 0:\n                self.retrieving = False\n                self.context = self.preretrieval_context\n                break\n                \n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.items[choice - 1])\n        return self.recall[:self.recall_total]\n\n    def force_recall(self, choice=None):\n\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        if choice is None:\n            pass\n        elif choice > 0:\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.items[choice - 1])\n        else:\n            self.retrieving = False\n            self.context = self.preretrieval_context\n        return self.recall[:self.recall_total]"
  },
  {
    "objectID": "library\\models\\Instance_CMR.html#context-maintenance-and-retrieval-within-an-instance-based-architecture",
    "href": "library\\models\\Instance_CMR.html#context-maintenance-and-retrieval-within-an-instance-based-architecture",
    "title": "compmemlearn",
    "section": "Context Maintenance and Retrieval within an Instance-Based Architecture",
    "text": "# export \n\nimport numpy as np\nfrom numba import float64, int32, boolean\nfrom numba.experimental import jitclass\n\nicmr_spec = [\n    ('item_count', int32), \n    ('encoding_drift_rate', float64),\n    ('start_drift_rate', float64),\n    ('recall_drift_rate', float64),\n    ('delay_drift_rate', float64),\n    ('shared_support', float64),\n    ('item_support', float64),\n    ('learning_rate', float64),\n    ('primacy_scale', float64),\n    ('primacy_decay', float64),\n    ('stop_probability_scale', float64),\n    ('stop_probability_growth', float64),\n    ('choice_sensitivity', float64),\n    ('context_sensitivity', float64),\n    ('feature_sensitivity', float64),\n    ('context', float64[::1]),\n    ('start_context_input', float64[::1]),\n    ('delay_context_input', float64[::1]),\n    ('preretrieval_context', float64[::1]),\n    ('recall', int32[::1]),\n    ('retrieving', boolean),\n    ('recall_total', int32),\n    ('item_weighting', float64[::1]),\n    ('context_weighting', float64[::1]),\n    ('all_weighting', float64[::1]),\n    ('probabilities', float64[::1]),\n    ('memory', float64[:,::1]),\n    ('encoding_index', int32),\n    ('items', float64[:,::1]),\n    ('norm', float64[::1]),\n]\n# export\n\n\n@jitclass(icmr_spec)\nclass Instance_CMR:\n\n    def __init__(self, item_count, presentation_count, parameters):\n\n        # store initial parameters\n        self.item_count = item_count\n        self.encoding_drift_rate = parameters['encoding_drift_rate']\n        self.delay_drift_rate = parameters['delay_drift_rate']\n        self.start_drift_rate = parameters['start_drift_rate']\n        self.recall_drift_rate = parameters['recall_drift_rate']\n        self.shared_support = parameters['shared_support']\n        self.item_support = parameters['item_support']\n        self.learning_rate = parameters['learning_rate']\n        self.primacy_scale = parameters['primacy_scale']\n        self.primacy_decay = parameters['primacy_decay']\n        self.stop_probability_scale = parameters['stop_probability_scale']\n        self.stop_probability_growth = parameters['stop_probability_growth']\n        self.choice_sensitivity = parameters['choice_sensitivity']\n        self.context_sensitivity = parameters['context_sensitivity']\n        self.feature_sensitivity = parameters['feature_sensitivity']\n        \n        # at the start of the list context is initialized with a state \n        # orthogonal to the pre-experimental context associated with the set of items\n        self.context = np.zeros(item_count + 2)\n        self.context[0] = 1\n        self.preretrieval_context = self.context\n        self.recall = np.zeros(item_count, int32) # recalls has at most `item_count` entries\n        self.retrieving = False\n        self.recall_total = 0\n\n        # predefine activation weighting vectors\n        self.item_weighting = np.ones(item_count+presentation_count)\n        self.context_weighting = np.ones(item_count+presentation_count)\n        self.item_weighting[item_count:] = self.learning_rate\n        self.context_weighting[item_count:] = \\\n            self.primacy_scale * np.exp(-self.primacy_decay * np.arange(presentation_count)) + 1\n        self.all_weighting = self.item_weighting * self.context_weighting\n\n        # preallocate for outcome_probabilities\n        self.probabilities = np.zeros((item_count + 1))\n\n        # predefine contextual input vectors relevant for delay_drift_rate and start_drift_rate parameters\n        self.start_context_input = np.zeros((self.item_count+2))\n        self.start_context_input[0] = 1\n        self.delay_context_input = np.zeros((self.item_count+2))\n        self.delay_context_input[-1] = 1\n\n        # initialize memory\n        # we now conceptualize it as a pairing of two stores Mfc and Mcf respectively\n        # representing feature-to-context and context-to-feature associations\n        mfc = np.eye(item_count, item_count + 2, 1) * (1 - self.learning_rate)\n        mcf = np.ones((item_count, item_count)) * self.shared_support\n        for i in range(item_count):\n            mcf[i, i] = self.item_support\n        mcf = np.hstack((np.zeros((item_count, 1)), mcf,  np.zeros((item_count, 1))))\n        self.memory = np.zeros((item_count + presentation_count, item_count * 2 + 4))\n        self.memory[:item_count,] = np.hstack((mfc, mcf))\n\n        self.norm = np.zeros(item_count + presentation_count)\n        self.norm[:item_count] = np.sqrt(np.sum(np.square(self.memory[0])))\n        self.norm[item_count:] = np.sqrt(2)\n        self.encoding_index = item_count\n        self.items = np.hstack((np.eye(item_count, item_count + 2, 1), np.zeros((item_count, item_count+2))))\n        self.items = self.items.astype(int32)\n\n    def experience(self, experiences):\n\n        for i in range(len(experiences)):\n            self.memory[self.encoding_index] = experiences[i]\n            self.update_context(self.encoding_drift_rate, self.memory[self.encoding_index])\n            self.memory[self.encoding_index, self.item_count+2:] = self.context\n            self.encoding_index += 1\n\n    def update_context(self, drift_rate, experience):\n\n        # first pre-experimental or initial context is retrieved\n        if len(experience) == self.item_count * 2 + 4:\n            context_input = self.echo(experience)[self.item_count + 2:]\n            context_input = context_input / np.sqrt(np.sum(np.square(context_input))) # norm to length 1\n        else:\n            context_input = experience\n\n        # updated context is sum of context and input, modulated by rho to have len 1 and some drift_rate\n        rho = np.sqrt(1 + np.square(drift_rate) * (np.square(self.context * context_input) - 1)) - (\n                drift_rate * (self.context * context_input))\n        self.context = (rho * self.context) + (drift_rate * context_input)\n        self.context = self.context / np.sqrt(np.sum(np.square(self.context)))\n\n    def echo(self, probe):\n\n        return np.dot(self.activations(probe), self.memory[:self.encoding_index])\n\n    def activations(self, probe, probe_norm=1.0):\n\n        activation = np.dot(self.memory[:self.encoding_index], probe) / (\n             self.norm[:self.encoding_index] * probe_norm)\n\n        # weight activations based on whether probe contains item or contextual features or both\n        if np.any(probe[:self.item_count + 2]): # if probe is an item feature cue as during contextual retrieval\n            if np.any(probe[self.item_count + 2:]): # if probe is (also) a contextual cue as during item retrieval\n                # both mfc and mcf weightings, see below\n                activation *= self.all_weighting[:self.encoding_index]\n            else:\n                # mfc weightings - scale by gamma for each experimental trace\n                activation *= self.item_weighting[:self.encoding_index]\n            activation = np.power(activation, self.context_sensitivity)\n        else:\n            # mcf weightings - scale by primacy/attention function based on experience position\n            activation *= self.context_weighting[:self.encoding_index]\n            if self.feature_sensitivity != 1.0:\n                activation = np.power(activation, self.feature_sensitivity)\n            else:\n                activation = np.power(activation, self.context_sensitivity)\n            \n        return activation\n\n    def outcome_probabilities(self):\n        \n        self.probabilities[0] = min(self.stop_probability_scale * np.exp(\n            self.recall_total * self.stop_probability_growth), 1.0 - (\n                 (self.item_count-self.recall_total) * 10e-7))\n        self.probabilities[1:] = 10e-7\n\n        if self.probabilities[0] < (1.0 - ((self.item_count-self.recall_total) * 10e-7)):\n\n            # measure activation for each item\n            activation_cue = np.hstack((np.zeros(self.item_count + 2), self.context))\n            activation = self.echo(activation_cue)[1:self.item_count+1]\n\n            # already recalled items have zero activation\n            activation[self.recall[:self.recall_total]] = 0\n            \n            # recall probability is a function of activation\n            if np.sum(activation) > 0:\n                activation = np.power(activation, self.choice_sensitivity)\n                self.probabilities[1:] = (1-self.probabilities[0]) * activation / np.sum(activation)\n        \n        return self.probabilities\n\n    def free_recall(self, steps=None):\n\n        # some pre-list context is reinstated before initiating recall\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n            \n        # number of items to retrieve is infinite if steps is unspecified\n        if steps is None:\n            steps = self.item_count - self.recall_total\n        steps = self.recall_total + steps\n\n        # at each recall attempt\n        while self.recall_total < steps:\n\n            # the current state of context is used as a retrieval cue to \n            # attempt recall of a studied item compute outcome probabilities \n            # and make choice based on distribution\n            outcome_probabilities = self.outcome_probabilities()\n            if np.any(outcome_probabilities[1:]):\n                choice = np.sum(\n                    np.cumsum(outcome_probabilities) < np.random.rand(), dtype=int32)\n            else:\n                choice = 0\n\n            # resolve and maybe store outcome\n            # we stop recall if no choice is made (0)\n            if choice == 0:\n                self.retrieving = False\n                self.context = self.preretrieval_context\n                break\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.items[choice - 1])\n        return self.recall[:self.recall_total]\n    \n    def force_recall(self, choice=None):\n\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        if choice is None:\n            pass\n        elif choice > 0:\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(\n                self.recall_drift_rate, self.items[choice - 1])\n        else:\n            self.retrieving = False\n            self.context = self.preretrieval_context\n        return self.recall[:self.recall_total]"
  },
  {
    "objectID": "library\\models\\Noisy_CMR.html#fitting-murdock-1970-dataset",
    "href": "library\\models\\Noisy_CMR.html#fitting-murdock-1970-dataset",
    "title": "compmemlearn",
    "section": "Fitting Murdock 1970 Dataset",
    "text": "Must update to reflect shift to instance model. But in practice I’ll probably move to a different notebook.\nfrom compmemlearn.datasets import prepare_murdock1970_data\n#from compmemlearn.fitting import murdock_objective_function\nfrom scipy.optimize import differential_evolution\nfrom numba import njit\nimport numpy as np\nfrom compmemlearn.fitting import apply_and_concatenate\nimport seaborn as sns\nfrom numba.typed import Dict\nfrom numba.core import types\nfrom compmemlearn.datasets import simulate_df\nimport matplotlib.pyplot as plt\nfrom psifr import fr\ndef murdock_data_likelihood(data_to_fit, item_counts, model_class, parameters):\n\n    result = 0.0\n    for i in range(len(item_counts)):\n        item_count = item_counts[i]\n        trials = data_to_fit[i]\n        likelihood = np.ones((len(trials), item_count))\n\n        model = model_class(item_count, item_count, parameters)\n        model.experience(model.items)\n\n        for trial_index in range(len(trials)):\n            trial = trials[trial_index]\n\n            model.force_recall()\n            for recall_index in range(len(trial) + 1):\n\n                # identify index of item recalled; if zero then recall is over\n                if recall_index == len(trial) and len(trial) < item_count:\n                    recall = 0\n                else:\n                    recall = trial[recall_index]\n\n                # store probability of and simulate recall of indexed item \n                likelihood[trial_index, recall_index] = \\\n                    model.outcome_probabilities()[recall] + 10e-7\n                \n                if recall == 0:\n                    break\n                model.force_recall(recall)\n\n            # reset model to its pre-retrieval (but post-encoding) state\n            model.force_recall(0)\n        \n        result -= np.sum(np.log(likelihood))\n\n    return result\n\ndef murdock_objective_function(data_to_fit, item_counts, model_class, fixed_parameters, free_parameters):\n    \"\"\"\n    Configures cmr_likelihood for search over specified free/fixed parameters.\n    \"\"\"\n\n    parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\n    for name, value in fixed_parameters.items():\n        parameters[name] = value\n    \n    def objective_function(x):\n        for i in range(len(free_parameters)):\n            parameters[free_parameters[i]] = x[i]\n        return murdock_data_likelihood(data_to_fit, item_counts, model_class, parameters)\n\n    return objective_function\ntrials, events, list_length = prepare_murdock1970_data('../../data/mo1970.txt')\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n)\n\n#@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Noise_CMR(item_count, presentation_count, parameters)\n    \ncost_function = murdock_objective_function(\n    (trials, ),  \n    (list_length, ),\n    init_cmr,\n    {'sampling_rule': 0}, \n    free_parameters)\n\nmurdock_result = differential_evolution(cost_function, bounds, disp=True)\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(murdock_result.x)):\n    fitted_parameters[free_parameters[i]] = murdock_result.x[i]\nfitted_parameters['sampling_rule'] = 0\n\nmodel0 = Noise_CMR(20, 20, fitted_parameters)\n\nsim_df0 = simulate_df(model0, 5000)\ntrue_df0 = events\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['ScalarCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['ScalarCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['ScalarCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)"
  },
  {
    "objectID": "library\\models\\Noisy_CMR.html#section",
    "href": "library\\models\\Noisy_CMR.html#section",
    "title": "compmemlearn",
    "section": "",
    "text": ""
  },
  {
    "objectID": "library\\models\\Scalar_CMR.html#fitting-murdock-1970-dataset",
    "href": "library\\models\\Scalar_CMR.html#fitting-murdock-1970-dataset",
    "title": "compmemlearn",
    "section": "Fitting Murdock 1970 Dataset",
    "text": "from compmemlearn.datasets import prepare_murdock1970_data\nfrom compmemlearn.fitting import murdock_objective_function\nfrom scipy.optimize import differential_evolution\nfrom numba import int32\nfrom numba import njit\nimport numpy as np\nfrom compmemlearn.fitting import apply_and_concatenate\nimport seaborn as sns\nfrom numba.typed import Dict\nfrom numba.core import types\nfrom compmemlearn.datasets import simulate_df\nimport matplotlib.pyplot as plt\nfrom psifr import fr\ntrials, events, list_length = prepare_murdock1970_data('../../data/mo1970.txt')\n\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'increment',\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub)\n)\n\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Scalar_CMR(item_count, presentation_count, parameters)\n    \ncost_function = murdock_objective_function(\n    (trials, ),  \n    (list_length, ),\n    init_cmr,\n    {'sampling_rule': 0}, \n    free_parameters)\n\nmurdock_result = differential_evolution(cost_function, bounds, disp=True)\n\ndifferential_evolution step 1: f(x)= 28456.6\ndifferential_evolution step 2: f(x)= 28025.4\ndifferential_evolution step 3: f(x)= 27778\ndifferential_evolution step 4: f(x)= 27474.9\ndifferential_evolution step 5: f(x)= 27474.9\ndifferential_evolution step 6: f(x)= 27474.9\ndifferential_evolution step 7: f(x)= 27189.7\ndifferential_evolution step 8: f(x)= 27189.7\ndifferential_evolution step 9: f(x)= 26938.8\ndifferential_evolution step 10: f(x)= 26938.8\ndifferential_evolution step 11: f(x)= 26681.4\ndifferential_evolution step 12: f(x)= 26510.4\ndifferential_evolution step 13: f(x)= 26083.6\ndifferential_evolution step 14: f(x)= 26083.6\ndifferential_evolution step 15: f(x)= 26083.6\ndifferential_evolution step 16: f(x)= 26083.6\ndifferential_evolution step 17: f(x)= 26019.2\ndifferential_evolution step 18: f(x)= 25866.7\ndifferential_evolution step 19: f(x)= 25866.7\ndifferential_evolution step 20: f(x)= 25866.7\ndifferential_evolution step 21: f(x)= 25866.7\ndifferential_evolution step 22: f(x)= 25866.7\ndifferential_evolution step 23: f(x)= 25866.7\ndifferential_evolution step 24: f(x)= 25866.7\ndifferential_evolution step 25: f(x)= 25866.7\ndifferential_evolution step 26: f(x)= 25866.7\ndifferential_evolution step 27: f(x)= 25866.7\ndifferential_evolution step 28: f(x)= 25866.7\ndifferential_evolution step 29: f(x)= 25866.7\ndifferential_evolution step 30: f(x)= 25866.7\ndifferential_evolution step 31: f(x)= 25866.7\ndifferential_evolution step 32: f(x)= 25866.7\ndifferential_evolution step 33: f(x)= 25221.9\ndifferential_evolution step 34: f(x)= 25221.9\ndifferential_evolution step 35: f(x)= 25221.9\ndifferential_evolution step 36: f(x)= 25221.9\ndifferential_evolution step 37: f(x)= 25221.9\ndifferential_evolution step 38: f(x)= 25221.9\ndifferential_evolution step 39: f(x)= 25221.9\ndifferential_evolution step 40: f(x)= 25221.9\ndifferential_evolution step 41: f(x)= 25221.9\ndifferential_evolution step 42: f(x)= 25221.9\ndifferential_evolution step 43: f(x)= 25221.9\ndifferential_evolution step 44: f(x)= 25205.6\ndifferential_evolution step 45: f(x)= 25205.6\ndifferential_evolution step 46: f(x)= 25157.5\ndifferential_evolution step 47: f(x)= 25157.5\ndifferential_evolution step 48: f(x)= 25157.5\ndifferential_evolution step 49: f(x)= 25057.5\ndifferential_evolution step 50: f(x)= 25057.5\ndifferential_evolution step 51: f(x)= 25057.5\ndifferential_evolution step 52: f(x)= 25057.5\ndifferential_evolution step 53: f(x)= 25057.5\ndifferential_evolution step 54: f(x)= 25057.5\ndifferential_evolution step 55: f(x)= 25057.5\ndifferential_evolution step 56: f(x)= 25057.5\ndifferential_evolution step 57: f(x)= 25057.5\ndifferential_evolution step 58: f(x)= 25057.5\ndifferential_evolution step 59: f(x)= 25044.6\ndifferential_evolution step 60: f(x)= 25033.3\ndifferential_evolution step 61: f(x)= 25033.3\ndifferential_evolution step 62: f(x)= 25033.3\ndifferential_evolution step 63: f(x)= 25033.3\ndifferential_evolution step 64: f(x)= 25033.3\ndifferential_evolution step 65: f(x)= 25033.3\ndifferential_evolution step 66: f(x)= 25033.3\ndifferential_evolution step 67: f(x)= 25033.3\ndifferential_evolution step 68: f(x)= 25033.3\ndifferential_evolution step 69: f(x)= 25016.7\ndifferential_evolution step 70: f(x)= 25016.7\ndifferential_evolution step 71: f(x)= 25010\ndifferential_evolution step 72: f(x)= 24976.5\ndifferential_evolution step 73: f(x)= 24957.2\n\n\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(murdock_result.x)):\n    fitted_parameters[free_parameters[i]] = murdock_result.x[i]\nfitted_parameters['sampling_rule'] = 0\n\nmodel0 = Scalar_CMR(20, 20, fitted_parameters)\n\nsim_df0 = simulate_df(model0, 5000)\ntrue_df0 = events\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['ScalarCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['ScalarCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['ScalarCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)"
  },
  {
    "objectID": "library\\models\\Trace_Reinstatement_CMR.html#trace-based-reinstatement",
    "href": "library\\models\\Trace_Reinstatement_CMR.html#trace-based-reinstatement",
    "title": "compmemlearn",
    "section": "Trace-Based Reinstatement",
    "text": "A variant of InstanceCMR that slightly breaks PrototypeCMR’s assumption that feature-to-context associations drive the evolution of context across a list-learning experiment. Instead, we’ll suppose that a composite cue containing activation of an item’s item-feature unit AND main contextual unit determines contextual input at each model step.\n# export \n\nimport numpy as np\nfrom numba import float64, int32, boolean\nfrom numba.experimental import jitclass\n\ntrcmr_spec = [\n    ('item_count', int32), \n    ('encoding_drift_rate', float64),\n    ('start_drift_rate', float64),\n    ('recall_drift_rate', float64),\n    ('delay_drift_rate', float64),\n    ('shared_support', float64),\n    ('item_support', float64),\n    ('learning_rate', float64),\n    ('primacy_scale', float64),\n    ('primacy_decay', float64),\n    ('stop_probability_scale', float64),\n    ('stop_probability_growth', float64),\n    ('choice_sensitivity', float64),\n    ('context_sensitivity', float64),\n    ('feature_sensitivity', float64),\n    ('context', float64[::1]),\n    ('start_context_input', float64[::1]),\n    ('delay_context_input', float64[::1]),\n    ('preretrieval_context', float64[::1]),\n    ('recall', int32[::1]),\n    ('retrieving', boolean),\n    ('recall_total', int32),\n    ('item_weighting', float64[::1]),\n    ('context_weighting', float64[::1]),\n    ('all_weighting', float64[::1]),\n    ('probabilities', float64[::1]),\n    ('memory', float64[:,::1]),\n    ('encoding_index', int32),\n    ('items', float64[:,::1]),\n    ('recall_items', float64[:,::1]),\n    ('norm', float64[::1]),\n    ('context_reinstatement', float64),\n    ('feature_drift_rate', float64),\n    ('features', float64[::1]),\n]\n# export\n\n@jitclass(trcmr_spec)\nclass Trace_Reinstatement_CMR:\n\n    def __init__(self, item_count, presentation_count, parameters):\n\n        # store initial parameters\n        self.item_count = item_count\n        self.encoding_drift_rate = parameters['encoding_drift_rate']\n        self.delay_drift_rate = parameters['delay_drift_rate']\n        self.start_drift_rate = parameters['start_drift_rate']\n        self.recall_drift_rate = parameters['recall_drift_rate']\n        self.shared_support = parameters['shared_support']\n        self.item_support = parameters['item_support']\n        self.learning_rate = parameters['learning_rate']\n        self.primacy_scale = parameters['primacy_scale']\n        self.primacy_decay = parameters['primacy_decay']\n        self.stop_probability_scale = parameters['stop_probability_scale']\n        self.stop_probability_growth = parameters['stop_probability_growth']\n        self.choice_sensitivity = parameters['choice_sensitivity']\n        self.context_sensitivity = parameters['context_sensitivity']\n        self.feature_sensitivity = parameters['feature_sensitivity']\n        self.context_reinstatement = parameters['context_reinstatement']\n        self.feature_drift_rate = parameters['feature_drift_rate']\n        \n        # at the start of the list context is initialized with a state \n        # orthogonal to the pre-experimental context associated with the set of items\n        self.context = np.zeros(item_count + 2)\n        self.context[0] = 1\n        self.preretrieval_context = self.context\n        self.recall = np.zeros(item_count, dtype=int32) # recalls has at most `item_count` entries\n        self.retrieving = False\n        self.recall_total = 0\n\n        # predefine activation weighting vectors\n        self.item_weighting = np.ones(item_count+presentation_count)\n        self.context_weighting = np.ones(item_count+presentation_count)\n        self.item_weighting[item_count:] = self.learning_rate\n        self.context_weighting[item_count:] = \\\n            self.primacy_scale * np.exp(-self.primacy_decay * np.arange(presentation_count)) + 1\n        self.all_weighting = self.item_weighting * self.context_weighting\n\n        # preallocate for outcome_probabilities\n        self.probabilities = np.zeros((item_count + 1))\n\n        # predefine contextual input vectors relevant for delay_drift_rate and start_drift_rate parameters\n        self.start_context_input = np.zeros((self.item_count+2))\n        self.start_context_input[0] = 1\n        self.delay_context_input = np.zeros((self.item_count+2))\n        self.delay_context_input[-1] = 1\n\n        # initialize memory\n        # we now conceptualize it as a pairing of two stores Mfc and Mcf respectively\n        # representing feature-to-context and context-to-feature associations\n        mfc = np.eye(item_count, item_count + 2, 1) * (1 - self.learning_rate)\n        mcf = np.ones((item_count, item_count)) * self.shared_support\n        for i in range(item_count):\n            mcf[i, i] = self.item_support\n        mcf = np.hstack((np.zeros((item_count, 1)), mcf,  np.zeros((item_count, 1))))\n        self.memory = np.zeros((item_count + presentation_count, item_count * 2 + 4))\n        self.memory[:item_count,] = np.hstack((mfc, mcf))\n\n        self.norm = np.zeros(item_count + presentation_count)\n        self.norm[:item_count] = np.sqrt(np.sum(np.square(self.memory[0])))\n        self.norm[item_count:] = np.sqrt(2)\n        self.encoding_index = item_count\n\n        # base\n        self.features = np.zeros((self.item_count+2))\n        self.recall_items = np.hstack((np.eye(item_count, item_count + 2, 1), np.zeros((item_count, item_count+2))))\n        \n        # mixed cue\n        self.items = np.hstack((np.eye(item_count, item_count + 2, 1), np.eye(item_count, item_count + 2, 1)))\n        \n        # contextual unit cue\n        #self.items = np.hstack((np.zeros((item_count, item_count+2)), np.eye(item_count, item_count + 2, 1)))\n\n        # parametrized mixed cue\n        #self.items = np.hstack(\n        #    (np.eye(item_count, item_count + 2, 1), self.context_reinstatement*np.eye(item_count,item_count + 2, 1)))\n        \n        #self.items /= np.sqrt(np.sum(np.square(self.items[0])))\n\n    def experience(self, experiences):\n\n        for i in range(len(experiences)):\n\n            # configure contextual representation for trace\n            self.update_context(self.encoding_drift_rate, experiences[i])\n            self.memory[self.encoding_index, self.item_count+2:] = self.context\n\n            # configure feature representation for trace\n            self.update_features(self.feature_drift_rate, experiences[i])\n            self.memory[self.encoding_index, :self.item_count+2] = self.features\n\n            self.encoding_index += 1\n\n    def update_features(self, drift_rate, experience):\n\n        probe = experience.copy() \n        probe[:self.item_count+2] *= 0 #TODO: exclude if I'm including C information in cue\n        feature_input = self.echo(probe)[:self.item_count + 2]\n        feature_input = feature_input / np.sqrt(np.sum(np.square(feature_input))) # norm to length 1\n\n        self.features = experience[:self.item_count+2]\n        rho = np.sqrt(1 + np.square(drift_rate) * (np.square(self.features * feature_input) - 1)) - (\n                drift_rate * (self.features * feature_input))\n        self.features = (rho * self.features) + (drift_rate * feature_input)\n        self.features = self.features / np.sqrt(np.sum(np.square(self.features)))\n\n    def update_context(self, drift_rate, experience):\n\n        # first pre-experimental or initial context is retrieved\n        if len(experience) == self.item_count * 2 + 4:\n            probe = experience.copy() \n            probe[self.item_count+2:] *= self.context_reinstatement\n            context_input = self.echo(probe)[self.item_count + 2:]\n            context_input = context_input / np.sqrt(np.sum(np.square(context_input))) # norm to length 1\n        else:\n            context_input = experience\n\n        # updated context is sum of context and input, modulated by rho to have len 1 and some drift_rate\n        rho = np.sqrt(1 + np.square(drift_rate) * (np.square(self.context * context_input) - 1)) - (\n                drift_rate * (self.context * context_input))\n        self.context = (rho * self.context) + (drift_rate * context_input)\n        self.context = self.context / np.sqrt(np.sum(np.square(self.context)))\n\n    def echo(self, probe):\n        return np.dot(self.activations(probe), self.memory[:self.encoding_index])\n\n    def activations(self, probe, probe_norm=1.0):\n\n        activation = np.dot(self.memory[:self.encoding_index], probe) / (\n             self.norm[:self.encoding_index] * probe_norm)\n\n        # weight activations based on whether probe contains item or contextual features or both\n        if np.any(probe[:self.item_count + 2]): # if probe is an item feature cue as during contextual retrieval\n            if np.any(probe[self.item_count + 2:]): # if probe is (also) a contextual cue as during item retrieval\n                # both mfc and mcf weightings, see below\n                activation *= self.all_weighting[:self.encoding_index]\n            else:\n                # mfc weightings - scale by gamma for each experimental trace\n                activation *= self.item_weighting[:self.encoding_index]\n            activation = np.power(activation, self.context_sensitivity)\n        else:\n            # mcf weightings - scale by primacy/attention function based on experience position\n            activation *= self.context_weighting[:self.encoding_index]\n            if self.feature_sensitivity != 1.0:\n                activation = np.power(activation, self.feature_sensitivity)\n            else:\n                activation = np.power(activation, self.context_sensitivity)\n            \n        return activation\n\n    def outcome_probabilities(self):\n        \n        self.probabilities[0] = min(self.stop_probability_scale * np.exp(\n            self.recall_total * self.stop_probability_growth), 1.0 - (\n                 (self.item_count-self.recall_total) * 10e-7))\n        self.probabilities[1:] = 10e-7\n\n        if self.probabilities[0] < (1.0 - ((self.item_count-self.recall_total) * 10e-7)):\n\n            # measure activation for each item\n            activation_cue = np.hstack((np.zeros(self.item_count + 2), self.context))\n            activation = self.echo(activation_cue)[1:self.item_count+1]\n\n            # already recalled items have zero activation\n            activation[self.recall[:self.recall_total]] = 0\n            \n            # recall probability is a function of activation\n            if np.sum(activation) > 0:\n                activation = np.power(activation, self.choice_sensitivity)\n                self.probabilities[1:] = (1-self.probabilities[0]) * activation / np.sum(activation)\n        \n        return self.probabilities\n\n    def free_recall(self, steps=None):\n\n        # some pre-list context is reinstated before initiating recall\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, dtype=int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n            \n        # number of items to retrieve is infinite if steps is unspecified\n        if steps is None:\n            steps = self.item_count - self.recall_total\n        steps = self.recall_total + steps\n\n        # at each recall attempt\n        while self.recall_total < steps:\n\n            # the current state of context is used as a retrieval cue to \n            # attempt recall of a studied item compute outcome probabilities \n            # and make choice based on distribution\n            outcome_probabilities = self.outcome_probabilities()\n            if np.any(outcome_probabilities[1:]):\n                choice = np.sum(\n                    np.cumsum(outcome_probabilities) < np.random.rand(), dtype=int32)\n            else:\n                choice = 0\n\n            # resolve and maybe store outcome\n            # we stop recall if no choice is made (0)\n            if choice == 0:\n                self.retrieving = False\n                self.context = self.preretrieval_context\n                break\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(self.recall_drift_rate, self.recall_items[choice - 1])\n        return self.recall[:self.recall_total]\n    \n    def force_recall(self, choice=None):\n\n        if not self.retrieving:\n            self.recall = np.zeros(self.item_count, dtype=int32)\n            self.recall_total = 0\n            self.preretrieval_context = self.context\n            self.update_context(self.delay_drift_rate, self.delay_context_input)\n            self.update_context(self.start_drift_rate, self.start_context_input)\n            self.retrieving = True\n\n        if choice is None:\n            pass\n        elif choice > 0:\n            self.recall[self.recall_total] = choice - 1\n            self.recall_total += 1\n            self.update_context(\n                self.recall_drift_rate, self.recall_items[choice - 1])\n        else:\n            self.retrieving = False\n            self.context = self.preretrieval_context\n        return self.recall[:self.recall_total]"
  },
  {
    "objectID": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#alternative-contiguity-tracing",
    "href": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#alternative-contiguity-tracing",
    "title": "compmemlearn",
    "section": "Alternative Contiguity Tracing",
    "text": "Fit CMR and InstanceCMR models to condition 4 of lohnas dataset and plot key summary statistics, including repetition effects. (We already do this in another pair of notebooks!). Visualize corresponding memory representations in the models, focusing on lag-connectivity under parameter shifting.\nLay out understanding of how item repetition codetermines model representations and behavior in the canonical and presented trials and why that means no accounting for alternative contiguity (and maybe hopefully also poor accounting for spacing effect).\nTo “prove” understanding, implement transformation of model weights that results in simulation of the alternative contiguity effect.\nRelate these results to CMR’s limitations and potential mechanisms for the transformation that might be implemented within a model.\nfrom compmemlearn.analyses import flex_mixed_spc, flex_mixed_crp, flex_mixed_pfr, fast_csp, alternative_contiguity, fast_rpl\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_array_from_presentations\nfrom compmemlearn.fitting import lohnas_objective_function\nfrom compmemlearn.models import Classic_CMR, Instance_CMR, Trace_Reinstatement_CMR\nfrom compmemlearn.model_analysis import matrix_heatmap, icmr_memory_heatmap, latent_mfc_mcf, connectivity_by_lag, mfc_heatmap\n\nimport numpy as np\nfrom numpy import matlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import differential_evolution\n\nfrom numba import njit\nfrom numba import int32\nfrom numba.typed import Dict\nfrom numba.core import types\n\n\nconditions = ['Control', 'Massed', 'Spaced', 'Mixed']\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')"
  },
  {
    "objectID": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#initial-fitting",
    "href": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#initial-fitting",
    "title": "compmemlearn",
    "section": "Initial Fitting",
    "text": "PrototypeCMR\n\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\ncondition = 4\nselection = list_types == condition\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\ncmr_result = differential_evolution(cost_function, cmr_bounds, disp=True)\n\n\ncmr_result\n\n     fun: 17271.488868401062\n     jac: array([ 4.36193657e-01, -2.87764124e-01,  5.14046403e-01, -3.44880391e-01,\n        2.74860213e+01,  5.78802429e-01,  4.72937248e-03,  0.00000000e+00,\n        2.95767677e-01, -6.92671165e-01, -5.31144909e-02, -7.47240843e-01])\n message: 'Optimization terminated successfully.'\n    nfev: 8861\n     nit: 34\n success: True\n       x: array([8.53866588e-01, 9.01233935e-02, 9.64653634e-01, 3.64986965e-02,\n       2.22044605e-16, 4.60969130e-01, 3.38252088e+00, 3.05443417e+01,\n       2.13921114e-02, 1.06822773e-01, 1.17216059e+00, 9.84763294e-01])\n\n\nexperiment_count = 1000\n\ncmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(cmr_result.x)):\n    cmr_fitted_parameters[cmr_free_parameters[j]] = cmr_result.x[j]\n    \ncmr_fitted_parameters['sampling_rule'] = 0\ncmr_fitted_parameters['mfc_familiarity_scale'] = 0\ncmr_fitted_parameters['mcf_familiarity_scale'] = 0\ncmr_fitted_parameters['drift_familiarity_scale'] = 0\n\ncmr_trials = simulate_array_from_presentations(\n    init_cmr, cmr_fitted_parameters, presentations[list_types==condition], experiment_count)\ncmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\nmodel_name = 'PrototypeCMR'\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\ncmr_spc = flex_mixed_spc(cmr_trials, cmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(cmr_spc)), cmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\ncmr_pfr = flex_mixed_pfr(cmr_trials, cmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(cmr_pfr)), cmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\ncmr_crp = flex_mixed_crp(cmr_trials, cmr_presentations)\ncmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(cmr_crp)), cmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\ncmr_rpl = fast_rpl(\n    cmr_trials, cmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = cmr_rpl[0]\nbinned[1] = cmr_rpl[1]\nbinned[2] = (cmr_rpl[2] + cmr_rpl[3])/2\nbinned[3] = (cmr_rpl[4] + cmr_rpl[5] + cmr_rpl[6])/3\nbinned[4] = (cmr_rpl[7] + cmr_rpl[8] + cmr_rpl[9])/3\ncmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(cmr_rpl)), cmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\ncmr_altcrp = alternative_contiguity(\n    cmr_trials, cmr_presentations, 6, 2)\ncmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), cmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), cmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n\n\n\n\nmodel_name = 'PrototypeCMR'\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10), sharey='row')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\ncmr_altcrp = alternative_contiguity(\n    cmr_trials, cmr_presentations, 6, 2)\ncmr_altcrp[:, lag_range] = np.nan\n\naxes.plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4]-data_altcrp[1][lag_range-3:lag_range+4], label='Data')\naxes.plot(np.arange(7), cmr_altcrp[0][lag_range-3:lag_range+4]-cmr_altcrp[1][lag_range-3:lag_range+4], label=model_name)\naxes.set_xticks(np.arange(7))\naxes.set_xticklabels(np.arange(7) - 3)\naxes.set_title('Positional Contiguity Difference')\n\naxes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x21749473ee0>\n\n\n\n\n\n\n\nInstanceCMR\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n)\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\ncondition = 4\nselection = list_types == condition\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\n\ndifferential_evolution step 1: f(x)= 46459.2\ndifferential_evolution step 2: f(x)= 28269\ndifferential_evolution step 3: f(x)= 26437.4\ndifferential_evolution step 4: f(x)= 24436.2\ndifferential_evolution step 5: f(x)= 24436.2\ndifferential_evolution step 6: f(x)= 20687.5\ndifferential_evolution step 7: f(x)= 20080.2\ndifferential_evolution step 8: f(x)= 20065.4\ndifferential_evolution step 9: f(x)= 19693.7\ndifferential_evolution step 10: f(x)= 19693.7\ndifferential_evolution step 11: f(x)= 19620.8\ndifferential_evolution step 12: f(x)= 19620.8\ndifferential_evolution step 13: f(x)= 19620.8\ndifferential_evolution step 14: f(x)= 19620.8\ndifferential_evolution step 15: f(x)= 19578.4\ndifferential_evolution step 16: f(x)= 18942.2\ndifferential_evolution step 17: f(x)= 18942.2\ndifferential_evolution step 18: f(x)= 18942.2\ndifferential_evolution step 19: f(x)= 18207\ndifferential_evolution step 20: f(x)= 18207\ndifferential_evolution step 21: f(x)= 18207\ndifferential_evolution step 22: f(x)= 18207\ndifferential_evolution step 23: f(x)= 18207\ndifferential_evolution step 24: f(x)= 18207\ndifferential_evolution step 25: f(x)= 18066.3\ndifferential_evolution step 26: f(x)= 18066.3\ndifferential_evolution step 27: f(x)= 18066.3\ndifferential_evolution step 28: f(x)= 17823.5\ndifferential_evolution step 29: f(x)= 17823.5\ndifferential_evolution step 30: f(x)= 17823.5\ndifferential_evolution step 31: f(x)= 17675.6\ndifferential_evolution step 32: f(x)= 17675.6\ndifferential_evolution step 33: f(x)= 17675.6\ndifferential_evolution step 34: f(x)= 17675.6\ndifferential_evolution step 35: f(x)= 17675.6\ndifferential_evolution step 36: f(x)= 17675.6\ndifferential_evolution step 37: f(x)= 17675.6\ndifferential_evolution step 38: f(x)= 17517.9\ndifferential_evolution step 39: f(x)= 17517.9\ndifferential_evolution step 40: f(x)= 17517.9\ndifferential_evolution step 41: f(x)= 17517.9\ndifferential_evolution step 42: f(x)= 17517.9\ndifferential_evolution step 43: f(x)= 17471.5\ndifferential_evolution step 44: f(x)= 17471.5\ndifferential_evolution step 45: f(x)= 17471.5\ndifferential_evolution step 46: f(x)= 17454.3\ndifferential_evolution step 47: f(x)= 17454.3\ndifferential_evolution step 48: f(x)= 17416\ndifferential_evolution step 49: f(x)= 17369.9\ndifferential_evolution step 50: f(x)= 17369.9\ndifferential_evolution step 51: f(x)= 17369.9\ndifferential_evolution step 52: f(x)= 17361.7\ndifferential_evolution step 53: f(x)= 17361.7\ndifferential_evolution step 54: f(x)= 17330.1\ndifferential_evolution step 55: f(x)= 17330.1\ndifferential_evolution step 56: f(x)= 17280.6\ndifferential_evolution step 57: f(x)= 17280.6\ndifferential_evolution step 58: f(x)= 17280.6\ndifferential_evolution step 59: f(x)= 17280.6\ndifferential_evolution step 60: f(x)= 17280.6\n\n\n\nicmr_result\n\n     fun: 17221.459000970448\n     jac: array([-34.70595385,  15.63421393,  22.91963017, 306.74091249,\n       -33.19582901,   9.75414878, -11.35013015,   0.        ,\n       -16.64921001,  71.7092917 , -10.55632317, -15.95180939])\n message: 'Optimization terminated successfully.'\n    nfev: 12969\n     nit: 60\n success: True\n       x: array([8.32901225e-01, 2.08116235e-01, 9.60915690e-01, 3.05257500e-04,\n       3.18582949e-02, 1.02883666e-01, 2.14975563e+00, 6.66001243e+01,\n       2.05812661e-02, 1.09385203e-01, 1.35697460e+00, 9.42731805e-01])\n\n\nexperiment_count = 1000\n\nicmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(icmr_result.x)):\n    icmr_fitted_parameters[icmr_free_parameters[j]] = icmr_result.x[j]\n    \nicmr_fitted_parameters['choice_sensitivity'] = 1\nicmr_fitted_parameters['feature_sensitivity'] = 1\n\nicmr_trials = simulate_array_from_presentations(\n    init_icmr, icmr_fitted_parameters, presentations[list_types==condition], experiment_count)\nicmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\nmodel_name = 'InstanceCMR'\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\nicmr_spc = flex_mixed_spc(icmr_trials, icmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(icmr_spc)), icmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\nicmr_pfr = flex_mixed_pfr(icmr_trials, icmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(icmr_pfr)), icmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\nicmr_crp = flex_mixed_crp(icmr_trials, icmr_presentations)\nicmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(icmr_crp)), icmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\nicmr_rpl = fast_rpl(\n    icmr_trials, icmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = icmr_rpl[0]\nbinned[1] = icmr_rpl[1]\nbinned[2] = (icmr_rpl[2] + icmr_rpl[3])/2\nbinned[3] = (icmr_rpl[4] + icmr_rpl[5] + icmr_rpl[6])/3\nbinned[4] = (icmr_rpl[7] + icmr_rpl[8] + icmr_rpl[9])/3\nicmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(icmr_rpl)), icmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\nicmr_altcrp = alternative_contiguity(\n    icmr_trials, icmr_presentations, 6, 2)\nicmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), icmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), icmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n\n\n\n\n\nTrace_Reinstatement_CMR\n# remember to run nbdev_build_lib first! \n\nimport compmemlearn\nimport importlib\nimportlib.reload(compmemlearn.models)\nfrom compmemlearn.models import Trace_Reinstatement_CMR\n\ntrcmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n    'context_reinstatement'\n)\n\ntrcmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n    (1, 10)\n]\n\n@njit(fastmath=True, nogil=True)\ndef init_trcmr(item_count, presentation_count, parameters):\n    return Trace_Reinstatement_CMR(item_count, presentation_count, parameters)\n\ncondition = 4\nselection = list_types == condition\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_trcmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    trcmr_free_parameters)\n\n# will throw error if i haven't already fitted\nprint(init_trcmr(20, 20, trcmr_fitted_parameters).items[0]) # to maybe avoid weird fitting failure -- we'll see if it works\n\n\n[0.         0.02499219 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.99968765\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]\n\n\n\ntrcmr_result = differential_evolution(cost_function, trcmr_bounds, disp=True)\ntrcmr_result\n\ndifferential_evolution step 1: f(x)= 38770.8\ndifferential_evolution step 2: f(x)= 31970\ndifferential_evolution step 3: f(x)= 28471.3\ndifferential_evolution step 4: f(x)= 20224.9\ndifferential_evolution step 5: f(x)= 20053.9\ndifferential_evolution step 6: f(x)= 19901.2\ndifferential_evolution step 7: f(x)= 19746.1\ndifferential_evolution step 8: f(x)= 19671.3\ndifferential_evolution step 9: f(x)= 19671.3\ndifferential_evolution step 10: f(x)= 19671.3\ndifferential_evolution step 11: f(x)= 19671.3\ndifferential_evolution step 12: f(x)= 19671.3\ndifferential_evolution step 13: f(x)= 19654.2\ndifferential_evolution step 14: f(x)= 19625.7\ndifferential_evolution step 15: f(x)= 19574.9\ndifferential_evolution step 16: f(x)= 19574.9\ndifferential_evolution step 17: f(x)= 19574.9\ndifferential_evolution step 18: f(x)= 19574.9\ndifferential_evolution step 19: f(x)= 19574.9\ndifferential_evolution step 20: f(x)= 19373.8\ndifferential_evolution step 21: f(x)= 19373.8\ndifferential_evolution step 22: f(x)= 19373.8\ndifferential_evolution step 23: f(x)= 19373.8\ndifferential_evolution step 24: f(x)= 19373.8\ndifferential_evolution step 25: f(x)= 19373.8\ndifferential_evolution step 26: f(x)= 19373.8\ndifferential_evolution step 27: f(x)= 19373.8\ndifferential_evolution step 28: f(x)= 19373.8\ndifferential_evolution step 29: f(x)= 19373.8\ndifferential_evolution step 30: f(x)= 18841.3\n\n\n     fun: 17059.651290065947\n     jac: array([-3.95208188e+01, -4.22016456e+01, -1.21831908e+02,  5.78427716e+01,\n       -1.35005394e+00,  1.87246768e+00, -1.51703717e-01,  0.00000000e+00,\n       -8.20444256e+01,  1.97843110e+02,  1.71508874e+01, -6.52773448e+02,\n       -6.14309105e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 12317\n     nit: 30\n success: True\n       x: array([8.32287621e-01, 6.19383237e-03, 9.14802390e-01, 6.03601368e-03,\n       3.49635680e-01, 7.39360206e-01, 2.72092798e+00, 4.64522241e+01,\n       1.91378214e-02, 1.14005522e-01, 2.29533351e+00, 9.99988823e-01,\n       5.73518735e+00])\n\n\n\nWith Maxed Out Context Reinstatement\n\nexperiment_count = 1000\n\ntrcmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(trcmr_result.x)):\n    trcmr_fitted_parameters[trcmr_free_parameters[j]] = trcmr_result.x[j]\n    \ntrcmr_fitted_parameters['choice_sensitivity'] = 1\ntrcmr_fitted_parameters['feature_sensitivity'] = 1\ntrcmr_fitted_parameters['context_reinstatement'] = 40 # hack to drive alternative contiguity effect\n\n# will throw error if i haven't already fitted\nprint(init_trcmr(20, 20, trcmr_fitted_parameters).items[0]) # to maybe avoid weird fitting failure -- we'll see if it works\n\ntrcmr_trials = simulate_array_from_presentations(\n    init_trcmr, trcmr_fitted_parameters, presentations[list_types==condition], experiment_count)\ntrcmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\nmodel_name = 'Trace Reinstatement CMR'\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_spc = flex_mixed_spc(trcmr_trials, trcmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(trcmr_spc)), trcmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_pfr = flex_mixed_pfr(trcmr_trials, trcmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(trcmr_pfr)), trcmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\ntrcmr_crp = flex_mixed_crp(trcmr_trials, trcmr_presentations)\ntrcmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(trcmr_crp)), trcmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\ntrcmr_rpl = fast_rpl(\n    trcmr_trials, trcmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = trcmr_rpl[0]\nbinned[1] = trcmr_rpl[1]\nbinned[2] = (trcmr_rpl[2] + trcmr_rpl[3])/2\nbinned[3] = (trcmr_rpl[4] + trcmr_rpl[5] + trcmr_rpl[6])/3\nbinned[4] = (trcmr_rpl[7] + trcmr_rpl[8] + trcmr_rpl[9])/3\ntrcmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(trcmr_rpl)), trcmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\ntrcmr_altcrp = alternative_contiguity(\n    trcmr_trials, trcmr_presentations, 6, 2)\ntrcmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n[0.         0.02499219 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.99968765\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]\n\n\n\n\n\n\n\nWith Parametrized Amount\n\nexperiment_count = 1000\n\ntrcmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(trcmr_result.x)):\n    trcmr_fitted_parameters[trcmr_free_parameters[j]] = trcmr_result.x[j]\n    \ntrcmr_fitted_parameters['choice_sensitivity'] = 1\ntrcmr_fitted_parameters['feature_sensitivity'] = 1\n\n# will throw error if i haven't already fitted\nprint(init_trcmr(20, 20, trcmr_fitted_parameters).items[0]) # to maybe avoid weird fitting failure -- we'll see if it works\n\ntrcmr_trials = simulate_array_from_presentations(\n    init_trcmr, trcmr_fitted_parameters, presentations[list_types==condition], experiment_count)\ntrcmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\n[0.         0.17177067 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.98513696\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]\n\n\n\nmodel_name = 'Trace Reinstatement CMR'\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_spc = flex_mixed_spc(trcmr_trials, trcmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(trcmr_spc)), trcmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_pfr = flex_mixed_pfr(trcmr_trials, trcmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(trcmr_pfr)), trcmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\ntrcmr_crp = flex_mixed_crp(trcmr_trials, trcmr_presentations)\ntrcmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(trcmr_crp)), trcmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\ntrcmr_rpl = fast_rpl(\n    trcmr_trials, trcmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = trcmr_rpl[0]\nbinned[1] = trcmr_rpl[1]\nbinned[2] = (trcmr_rpl[2] + trcmr_rpl[3])/2\nbinned[3] = (trcmr_rpl[4] + trcmr_rpl[5] + trcmr_rpl[6])/3\nbinned[4] = (trcmr_rpl[7] + trcmr_rpl[8] + trcmr_rpl[9])/3\ntrcmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(trcmr_rpl)), trcmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\ntrcmr_altcrp = alternative_contiguity(\n    trcmr_trials, trcmr_presentations, 6, 2)\ntrcmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);"
  },
  {
    "objectID": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#lag-connectivity",
    "href": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#lag-connectivity",
    "title": "compmemlearn",
    "section": "Lag-Connectivity",
    "text": "To begin our analysis of model representations underlying this effect, we’ll develop an extension of the lag-connectivity analysis that tracks whether our memory representations similarly disfavor the repetition contiguity effect.\nWe’ll simulate each study sequence presented in the study. After each simulation, we’ll identify items that were presented repeatedly with some minimum spacing between repetitions (usually 6). Then we’ll track for lags in range [-3, 3] from each study position the mean connection weight between the focused item and item at the corresponding lag.\n\nFunctions\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef matrix_heatmap(matrix, title='', figsize=(15, 15), savefig=False, axis=None):\n    \"\"\"\n\n    **Arguments**:  \n    - matrix: an array of model states, ideally with columns representing unique feature indices and rows\n        representing unique update indices  \n    - title: a title for the generated plot, ideally conveying what array values represent at each entry  \n    - savefig: boolean deciding whether generated figure is saved (True if Yes)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(matrix, annot=True, linewidths=.5, ax=axis)\n    plt.title(title)\n    plt.xlabel('Feature Index')\n    plt.ylabel('Update Index')\n    if savefig:\n        plt.savefig('figures/{}.jpeg'.format(title).replace(' ', '_').lower(), bbox_inches='tight')\n    plt.show()\n\ndef icmr_memory_heatmap(model, just_experimental=False, just_context=False):\n    memory_shape = np.shape(model.memory)\n    fig_size = list(reversed(memory_shape))\n    plotted_memory = model.memory.copy()\n    title = \"Memory Traces\"\n\n    if just_context:\n        fig_size[0] /= 2\n        plotted_memory = plotted_memory[:, int(memory_shape[1]/2):]\n        title = \"Contextual \" + title\n\n    if just_experimental:\n        fig_size[1] -= model.item_count\n        plotted_memory = plotted_memory[model.item_count:]\n        title = \"Experimental \" + title\n\n\n    matrix_heatmap(plotted_memory, title, figsize=fig_size)\n    \ndef mixed_connectivity_by_lag(item_connections, presentation):\n    item_count = np.max(presentation)+1\n    lag_range = len(presentation) - 1\n    total_connectivity = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    item_positions = np.arange(len(presentation), dtype=int)\n\n    for item in range(item_count):\n\n        # only consider items that are repeated\n        current_positions = np.nonzero(presentation == item)[0]\n\n        # we consider each study position of repeated items separately\n        for position_index in range(len(current_positions)):\n\n            # lag of each item from current item is item_positions - current_position, \n            # and will always be in range [-lag_range, lag_range] so we keep position by adding lag_range\n            item_lags = item_positions - current_positions[position_index] + lag_range\n            total_connectivity[item_lags[presentation != item]] += item_connections[item, presentation[presentation != item]]\n            total_possible_lags[item_lags[presentation != item]] += 1\n\n    # divide by possible lags to get average connectivity\n    total_possible_lags[total_connectivity == 0] += 1\n    connectivity = total_connectivity / total_possible_lags\n    return connectivity\n\ndef alternative_connectivity_by_lag(item_connections, presentation, minimum_lag=6, max_repeats=2):\n\n    item_count = np.max(presentation)+1\n    lag_range = len(presentation) - 1\n    total_connectivity = np.zeros((max_repeats, lag_range * 2 + 1))\n    total_possible_lags = np.zeros((max_repeats, lag_range * 2 + 1))\n    item_positions = np.arange(len(presentation), dtype=int)\n\n    for item in range(item_count):\n\n        # only consider items that are repeated\n        current_positions = np.nonzero(presentation == item)[0]\n        if len(current_positions) < max_repeats:\n            continue\n\n        # only consider items with repeats of lag >= minimum_lag\n        assert(current_positions[1] > current_positions[0])\n        if current_positions[1] - current_positions[0] < minimum_lag:\n            continue\n\n        # we consider each study position of repeated items separately\n        for position_index in range(max_repeats):\n\n            # lag of each item from current item is item_positions - current_position, \n            # and will always be in range [-lag_range, lag_range] so we keep position by adding lag_range\n            item_lags = item_positions - current_positions[position_index] + lag_range\n            total_connectivity[position_index, item_lags] += item_connections[item, presentation]\n            total_possible_lags[position_index, item_lags] += 1\n\n    # divide by possible lags to get average connectivity\n    total_possible_lags[total_connectivity == 0] += 1\n    connectivity = total_connectivity / total_possible_lags\n    return connectivity\n\n\nInstanceCMR\n\n# configure parameters\nmodel_class = Instance_CMR\nmodel_parameters = icmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [1, 2, 3, 4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n        lag_range = item_count+1\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities[0] += mixed_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities[0] += mixed_connectivity_by_lag(mcf_connections, presentation)\n        #mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)[:, lag_range-3:lag_range+4]\n        #mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)[:, lag_range-3:lag_range+4]\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=True)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    #axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    #axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[1].set_title('MFC')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassic CMR\n\n# configure parameters\nmodel_class = Classic_CMR\nmodel_parameters = cmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [1, 2, 3, 4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n        lag_range = item_count+1\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities[0] += mixed_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities[0] += mixed_connectivity_by_lag(mcf_connections, presentation)\n        #mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)[:, lag_range-3:lag_range+4]\n        #mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)[:, lag_range-3:lag_range+4]\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=True)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    #axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    #axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[1].set_title('MFC')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTR-CMR\n\n# configure parameters\nmodel_class = Trace_Reinstatement_CMR\nmodel_parameters = trcmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [1, 2, 3, 4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n        lag_range = item_count+1\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities[0] += mixed_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities[0] += mixed_connectivity_by_lag(mcf_connections, presentation)\n        #mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)[:, lag_range-3:lag_range+4]\n        #mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)[:, lag_range-3:lag_range+4]\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=True)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    #axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    #axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[1].set_title('MFC')"
  },
  {
    "objectID": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#alternative-lag-connectivity",
    "href": "library\\model_analysis\\Alternative_Contiguity_Tracing.html#alternative-lag-connectivity",
    "title": "compmemlearn",
    "section": "Alternative Lag-Connectivity",
    "text": "InstanceCMR\n\n# configure parameters\nmodel_class = Instance_CMR\nmodel_parameters = icmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n            if trial_index == 0:\n                mfc_heatmap(model.mfc)\n                mfc_heatmap(model.mcf)\n                print(presentation)\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n            if trial_index == 0:\n                icmr_memory_heatmap(model, just_experimental=True, just_context=True)\n                print(presentation)\n\n        # track alternative connectivity\n        mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # focus on +/- 3 lags\n    plotting_lag_range = 5\n    mfc_alternative_connectivity = mfc_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mcf_alternative_connectivity = mcf_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=False)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[1].set_title('MFC')\n\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 11 12 13 14 15 16  9 17 18 19 18 20\n 21 22 19 23 24 25 21 26 27 23 28 29 30 31 32 33]\n\n\n\n\n\n\n\nClassic CMR\n\n# configure parameters\nmodel_class = Classic_CMR\nmodel_parameters = cmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n            if trial_index == 0:\n                mfc_heatmap(model.mfc)\n                mfc_heatmap(model.mcf)\n                print(presentation)\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n            if trial_index == 0:\n                icmr_memory_heatmap(model, just_experimental=True, just_context=True)\n                print(presentation)\n\n        # track alternative connectivity\n        mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # focus on +/- 3 lags\n    plotting_lag_range = 5\n    mfc_alternative_connectivity = mfc_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mcf_alternative_connectivity = mcf_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=False)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[1].set_title('MFC')\n\n\n\n\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 11 12 13 14 15 16  9 17 18 19 18 20\n 21 22 19 23 24 25 21 26 27 23 28 29 30 31 32 33]\n\n\n\n\n\n\n\nTR-CMR\n\n# configure parameters\nmodel_class = Trace_Reinstatement_CMR\nmodel_parameters = trcmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n            if trial_index == 0:\n                mfc_heatmap(model.mfc)\n                mfc_heatmap(model.mcf)\n                print(presentation)\n        else:\n            latent_mfc, latent_mcf = latent_mfc_mcf(model)\n            mfc_connections = latent_mfc[:, 1:-1]\n            mcf_connections = latent_mcf[:, 1:-1]\n            if trial_index == 0:\n                icmr_memory_heatmap(model, just_experimental=True, just_context=True)\n                print(presentation)\n\n        # track alternative connectivity\n        mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # focus on +/- 3 lags\n    plotting_lag_range = 5\n    mfc_alternative_connectivity = mfc_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mcf_alternative_connectivity = mcf_alternative_connectivity[:, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 20/2), sharey=False)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[1].set_title('MFC')\n\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 11 12 13 14 15 16  9 17 18 19 18 20\n 21 22 19 23 24 25 21 26 27 23 28 29 30 31 32 33]"
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#contiguity-tracing",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#contiguity-tracing",
    "title": "compmemlearn",
    "section": "Contiguity Tracing",
    "text": "Let’s try an analysis where I simulate a regular trial using the model and compute a representational CRP where for each serial position, I tabulate the corresponding item’s connectivity to items with graduating serial lag from the item."
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#background-of-this-notebook",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#background-of-this-notebook",
    "title": "compmemlearn",
    "section": "Background of This Notebook",
    "text": "from compmemlearn.fitting import murdock_objective_function, apply_and_concatenate\nfrom compmemlearn.datasets import prepare_lohnas2014_data, simulate_array, simulate_df\nfrom compmemlearn.models import Classic_CMR, Instance_CMR\nfrom scipy.optimize import differential_evolution\nfrom numba.typed import List, Dict\nimport seaborn as sns\nimport numpy as np\nfrom numba import njit, types\nimport matplotlib.pyplot as plt\nfrom psifr import fr\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')"
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#initial-fitting",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#initial-fitting",
    "title": "compmemlearn",
    "section": "Initial Fitting",
    "text": "Instance CMR\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n)\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n    \ncost_function = murdock_objective_function(\n    (trials[list_types == 1], ), \n    (list_length, ), \n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1},\n    icmr_free_parameters)\n\n#result = differential_evolution(cost_function, icmr_bounds, disp=True)\n\nicmr_result = np.array([7.66940287e-01, 3.40249111e-01, 9.48122676e-01, 2.14378788e-03,\n       1.42758234e-01, 5.66283526e-01, 2.20949851e+00, 7.60228147e-01,\n       2.40663552e-02, 9.01763767e-02, 1.82284478e+00, 4.93091746e-01])\n\nicmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(icmr_result)):\n    icmr_fitted_parameters[icmr_free_parameters[i]] = icmr_result[i]\nicmr_fitted_parameters['choice_sensitivity'] = 1\nicmr_fitted_parameters['feature_sensitivity'] = 1\n\nmodel0 = Instance_CMR(list_length, list_length, icmr_fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\ntrue_df0 = events.loc[events.condition==1]\n\nicmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['InstanceCMR', 'data'])\nicmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['InstanceCMR', 'data'])\nicmr_pfr0 = icmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=icmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=icmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=icmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=icmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\n\n\n\n\n\n\nClassic CMR\n\ncmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ncmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n]\n\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n    \ncost_function = murdock_objective_function(\n    (trials[list_types == 1], ), \n    (list_length, ), \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, \n         'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    cmr_free_parameters)\n\nresult = differential_evolution(cost_function, cmr_bounds, disp=True)\n\ndifferential_evolution step 1: f(x)= 38886.5\ndifferential_evolution step 2: f(x)= 28748.4\ndifferential_evolution step 3: f(x)= 25562.1\ndifferential_evolution step 4: f(x)= 25562.1\ndifferential_evolution step 5: f(x)= 19949.3\ndifferential_evolution step 6: f(x)= 19949.3\ndifferential_evolution step 7: f(x)= 19949.3\ndifferential_evolution step 8: f(x)= 19949.3\ndifferential_evolution step 9: f(x)= 19949.3\ndifferential_evolution step 10: f(x)= 19949.3\ndifferential_evolution step 11: f(x)= 19949.3\ndifferential_evolution step 12: f(x)= 19562.1\ndifferential_evolution step 13: f(x)= 19562.1\ndifferential_evolution step 14: f(x)= 19349.4\ndifferential_evolution step 15: f(x)= 19311.6\ndifferential_evolution step 16: f(x)= 19196.3\ndifferential_evolution step 17: f(x)= 19196.3\ndifferential_evolution step 18: f(x)= 19106.7\ndifferential_evolution step 19: f(x)= 19106.7\ndifferential_evolution step 20: f(x)= 19106.7\ndifferential_evolution step 21: f(x)= 19106.7\ndifferential_evolution step 22: f(x)= 19106.7\ndifferential_evolution step 23: f(x)= 19106.7\ndifferential_evolution step 24: f(x)= 19106.7\ndifferential_evolution step 25: f(x)= 18975.8\ndifferential_evolution step 26: f(x)= 18975.8\ndifferential_evolution step 27: f(x)= 18975.8\ndifferential_evolution step 28: f(x)= 18912.9\ndifferential_evolution step 29: f(x)= 18912.9\ndifferential_evolution step 30: f(x)= 18895.5\ndifferential_evolution step 31: f(x)= 18895.5\ndifferential_evolution step 32: f(x)= 18895.5\ndifferential_evolution step 33: f(x)= 18895.5\n\n\nFor some reason, printing result.x and and directly using the array as cmr_result doesn’t work. So whatever.\n\ncmr_result = result.x\n\ncmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(cmr_result)):\n    cmr_fitted_parameters[cmr_free_parameters[i]] = cmr_result[i]\ncmr_fitted_parameters['sampling_rule'] = 0\ncmr_fitted_parameters['mfc_familiarity_scale'] = 0\ncmr_fitted_parameters['mcf_familiarity_scale'] = 0\ncmr_fitted_parameters['drift_familiarity_scale'] = 0\nmodel0 = Classic_CMR(list_length, list_length, cmr_fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\ntrue_df0 = events.loc[events.condition==1]\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\n\n\n\n\n\n\nNoisy_CMR\n# remember to run nbdev_build_lib first! \n\nimport compmemlearn\nimport importlib\nimportlib.reload(compmemlearn.models)\nfrom compmemlearn.models import Noisy_CMR\ndef slow_murdock_data_likelihood(data_to_fit, item_counts, model_class, parameters):\n\n    result = 0.0\n    for i in range(len(item_counts)):\n        item_count = item_counts[i]\n        trials = data_to_fit[i]\n        likelihood = np.ones((len(trials), item_count))\n\n        model = model_class(item_count, item_count, parameters)\n        model.experience(model.items)\n\n        for trial_index in range(len(trials)):\n            trial = trials[trial_index]\n\n            model.force_recall()\n            for recall_index in range(len(trial) + 1):\n\n                # identify index of item recalled; if zero then recall is over\n                if recall_index == len(trial) and len(trial) < item_count:\n                    recall = 0\n                else:\n                    recall = trial[recall_index]\n\n                # store probability of and simulate recall of indexed item \n                likelihood[trial_index, recall_index] = \\\n                    model.outcome_probabilities()[recall] + 10e-7\n                \n                if recall == 0:\n                    break\n                model.force_recall(recall)\n\n            # reset model to its pre-retrieval (but post-encoding) state\n            model.force_recall(0)\n        \n        result -= np.sum(np.log(likelihood))\n\n    return result\n\ndef slow_murdock_objective_function(data_to_fit, item_counts, model_class, fixed_parameters, free_parameters):\n    \"\"\"\n    Configures cmr_likelihood for search over specified free/fixed parameters.\n    \"\"\"\n\n    parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\n    for name, value in fixed_parameters.items():\n        parameters[name] = value\n    \n    def objective_function(x):\n        for i in range(len(free_parameters)):\n            parameters[free_parameters[i]] = x[i]\n        return slow_murdock_data_likelihood(data_to_fit, item_counts, model_class, parameters)\n\n    return objective_function\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n)\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\n@njit(fastmath=True, nogil=True)\ndef init_ncmr(item_count, presentation_count, parameters):\n    return Noisy_CMR(item_count, presentation_count, parameters)\n    \ncost_function = murdock_objective_function(\n    (trials[list_types == 1], ), \n    (list_length, ), \n    init_ncmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1},\n    icmr_free_parameters)\n\nresult = differential_evolution(cost_function, icmr_bounds, disp=True)\nresult\n\ndifferential_evolution step 1: f(x)= 34339.7\ndifferential_evolution step 2: f(x)= 23934.4\ndifferential_evolution step 3: f(x)= 22650.8\ndifferential_evolution step 4: f(x)= 22238.9\ndifferential_evolution step 5: f(x)= 21860.8\ndifferential_evolution step 6: f(x)= 21860.8\ndifferential_evolution step 7: f(x)= 21860.8\ndifferential_evolution step 8: f(x)= 21786.2\ndifferential_evolution step 9: f(x)= 21728.2\ndifferential_evolution step 10: f(x)= 21728.2\ndifferential_evolution step 11: f(x)= 21728.2\ndifferential_evolution step 12: f(x)= 21728.2\ndifferential_evolution step 13: f(x)= 21633.9\ndifferential_evolution step 14: f(x)= 21633.9\ndifferential_evolution step 15: f(x)= 21633.9\ndifferential_evolution step 16: f(x)= 21633.9\ndifferential_evolution step 17: f(x)= 21633.9\ndifferential_evolution step 18: f(x)= 21633.9\ndifferential_evolution step 19: f(x)= 21633.9\ndifferential_evolution step 20: f(x)= 21633.9\ndifferential_evolution step 21: f(x)= 21629.2\ndifferential_evolution step 22: f(x)= 21629.2\ndifferential_evolution step 23: f(x)= 21629\ndifferential_evolution step 24: f(x)= 21595.1\n\n\n\nncmr_result = result.x\n\nncmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(ncmr_result)):\n    ncmr_fitted_parameters[icmr_free_parameters[i]] = ncmr_result[i]\nncmr_fitted_parameters['choice_sensitivity'] = 1\nncmr_fitted_parameters['feature_sensitivity'] = 1\n\nmodel0 = Noisy_CMR(list_length, list_length, ncmr_fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\ntrue_df0 = events.loc[events.condition==1]\n\nncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['NoisyCMR', 'data'])\nncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['NoisyCMR', 'data'])\nncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['NoisyCMR', 'data'])\nncmr_pfr0 = ncmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=ncmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=ncmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=ncmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=ncmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)"
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#trace-visualization",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#trace-visualization",
    "title": "compmemlearn",
    "section": "Trace Visualization",
    "text": "# export\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef matrix_heatmap(matrix, title='', figsize=(15, 15), savefig=False, axis=None):\n    \"\"\"\n    Plots an array of model states as a value-annotated heatmap with an arbitrary title.\n\n    **Arguments**:  \n    - matrix: an array of model states, ideally with columns representing unique feature indices and rows\n        representing unique update indices  \n    - title: a title for the generated plot, ideally conveying what array values represent at each entry  \n    - savefig: boolean deciding whether generated figure is saved (True if Yes)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(matrix, annot=True, linewidths=.5, ax=axis)\n    plt.title(title)\n    plt.xlabel('Feature Index')\n    plt.ylabel('Update Index')\n    if savefig:\n        plt.savefig('figures/{}.jpeg'.format(title).replace(' ', '_').lower(), bbox_inches='tight')\n    plt.show()\n\ndef icmr_memory_heatmap(model, just_experimental=False, just_context=False):\n    memory_shape = np.shape(model.memory)\n    fig_size = list(reversed(memory_shape))\n    plotted_memory = model.memory.copy()\n    title = \"Memory Traces\"\n\n    if just_context:\n        fig_size[0] /= 2\n        plotted_memory = plotted_memory[:, int(memory_shape[1]/2):]\n        title = \"Contextual \" + title\n\n    if just_experimental:\n        fig_size[1] -= model.item_count\n        plotted_memory = plotted_memory[int(memory_shape[0]/2):]\n        title = \"Experimental \" + title\n\n    matrix_heatmap(plotted_memory, title, figsize=fig_size)\n\nicmr_result = np.array([7.66940287e-01, 3.40249111e-01, 9.48122676e-01, 2.14378788e-03,\n       1.42758234e-01, 5.66283526e-01, 2.20949851e+00, 7.60228147e-01,\n       2.40663552e-02, 9.01763767e-02, 1.82284478e+00, 4.93091746e-01])\n\nicmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(icmr_result)):\n    icmr_fitted_parameters[icmr_free_parameters[i]] = icmr_result[i]\nicmr_fitted_parameters['choice_sensitivity'] = 1\nicmr_fitted_parameters['feature_sensitivity'] = 1\n\nmodel = Instance_CMR(list_length, list_length, icmr_fitted_parameters)\nsim_df = simulate_df(model, 1)\n\nicmr_memory_heatmap(model, just_experimental=True, just_context=True)\n\n\n\n\n\nmodel = Noisy_CMR(list_length, list_length, ncmr_fitted_parameters)\nsim_df = simulate_df(model, 1)\n\nicmr_memory_heatmap(model, just_experimental=False, just_context=False)"
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#latent-mfc-and-mcf",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#latent-mfc-and-mcf",
    "title": "compmemlearn",
    "section": "Latent MFC and MCF",
    "text": "# export\n\nimport numpy as np\n\ndef latent_mfc_mcf(model):\n    \n    \"\"\"\n    Generates the latent $M^{FC}$ and $M^{CF}$ in the specified ICMR instance.\n    For exploring and demonstrating model equivalence, we can calculate for any state of ICMR's dual-store memory \n    array $M$ a corresponding $M^{FC}$ (or $M^{CF}$) by computing for each orthogonal $f_i$ (or $c_i$) the model's \n    corresponding echo representation. \n    \"\"\"\n    \n    # start by finding latent mfc: the contextual representation cued when each orthogonal $f_i$ is cued\n    latent_mfc = np.zeros((model.item_count, model.item_count+2))\n    for i in range(model.item_count):\n        latent_mfc[i] = model.echo(model.items[i])[model.item_count + 2:]\n\n    latent_mcf = np.zeros((model.item_count, model.item_count+2))\n    context_units = np.hstack(\n        (np.zeros((model.item_count, model.item_count+2)), \n         np.eye(model.item_count, model.item_count + 2, 1))\n         )\n    for i in range(model.item_count):\n        latent_mcf[i] = model.echo(context_units[i])[:model.item_count+2]\n\n    return latent_mfc, latent_mcf\n\ndef mfc_heatmap(mfc):\n    matrix_heatmap(mfc, title='', figsize=list(reversed(np.shape(mfc))))\n\nicmr_result = np.array([7.66940287e-01, 3.40249111e-01, 9.48122676e-01, 2.14378788e-03,\n       1.42758234e-01, 5.66283526e-01, 2.20949851e+00, 7.60228147e-01,\n       2.40663552e-02, 9.01763767e-02, 1.82284478e+00, 4.93091746e-01])\n\nicmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(icmr_result)):\n    icmr_fitted_parameters[icmr_free_parameters[i]] = icmr_result[i]\nicmr_fitted_parameters['choice_sensitivity'] = 1\nicmr_fitted_parameters['feature_sensitivity'] = 1\n\nmodel = Instance_CMR(list_length, list_length, icmr_fitted_parameters)\nsim_df = simulate_df(model, 1)\n\nlatent_mfc, latent_mcf = latent_mfc_mcf(model)\n\nmfc_heatmap(latent_mfc)\nmfc_heatmap(latent_mcf)\n\n\n\n\n\n\n\n\nmodel = Noisy_CMR(list_length, list_length, ncmr_fitted_parameters)\nsim_df = simulate_df(model, 1)\n\nnoisy_latent_mfc, noisy_latent_mcf = latent_mfc_mcf(model)\n\nmfc_heatmap(noisy_latent_mfc)\nmfc_heatmap(noisy_latent_mcf)\n\n\n\n\n\n\n\n\ncmr_fitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(cmr_result)):\n    cmr_fitted_parameters[cmr_free_parameters[i]] = cmr_result[i]\ncmr_fitted_parameters['sampling_rule'] = 0\ncmr_fitted_parameters['mfc_familiarity_scale'] = 0\ncmr_fitted_parameters['mcf_familiarity_scale'] = 0\ncmr_fitted_parameters['drift_familiarity_scale'] = 0\ncmr_model = Classic_CMR(list_length, list_length, cmr_fitted_parameters)\nsimulate_df(cmr_model, 1);\n\nmfc_heatmap(cmr_model.mfc)\nmfc_heatmap(cmr_model.mcf)"
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#memory-connectivity-by-lag",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#memory-connectivity-by-lag",
    "title": "compmemlearn",
    "section": "Memory Connectivity by Lag",
    "text": "Connectivity here refers to “how much an item’s corresponding feature or contextual unit gets activated when another considered item’s corresponding contextual or feature unit is activated. memory argument should always be a 2D matrix mapping item connections, usually a subset of the corresponding memory matrix.\n# export\n\ndef connectivity_by_lag(item_connections, item_count):\n    \n    lag_range = item_count - 1\n    total_connectivity = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    item_positions = np.arange(item_count, dtype=int)\n    \n    # tabulate bin totals for actual and possible lags\n    # this time instead of looping through trials and recall indices, we only loop once through each item index\n    for i in range(item_count):\n\n        # lag of each item from current item is item position - i, \n        # and will always be in range [-lag_range, lag_range] so we keep position by adding lag_range\n        item_lags = item_positions - i + lag_range\n        total_connectivity[item_lags] += item_connections[i]\n        total_possible_lags[item_lags] += 1\n\n    # divide by possible lags to get average connectivity\n    connectivity = total_connectivity / total_possible_lags\n    return connectivity\ndef mixed_connectivity_by_lag(item_connections, presentation):\n    item_count = np.max(presentation)+1\n    lag_range = len(presentation) - 1\n    total_connectivity = np.zeros(lag_range * 2 + 1)\n    total_possible_lags = np.zeros(lag_range * 2 + 1)\n    item_positions = np.arange(len(presentation), dtype=int)\n\n    for item in range(item_count):\n\n        # only consider items that are repeated\n        current_positions = np.nonzero(presentation == item)[0]\n\n        # we consider each study position of repeated items separately\n        for position_index in range(len(current_positions)):\n\n            # lag of each item from current item is item_positions - current_position, \n            # and will always be in range [-lag_range, lag_range] so we keep position by adding lag_range\n            item_lags = item_positions - current_positions[position_index] + lag_range\n            total_connectivity[item_lags] += item_connections[item, presentation]\n            total_possible_lags[item_lags] += 1\n\n    # divide by possible lags to get average connectivity\n    total_possible_lags[total_connectivity == 0] += 1\n    connectivity = total_connectivity / total_possible_lags\n    return connectivity\n\nPrototypeCMR\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15/2), sharey=True)\n#test_crp = connectivity_by_lag(cmr_model.mcf[1:-1, :], cmr_model.item_count)\ntest_crp = mixed_connectivity_by_lag(cmr_model.mcf[1:-1, :], np.arange(cmr_model.item_count, dtype=int))\ntest_crp[cmr_model.item_count-1] = np.nan\naxes[0].plot(np.arange(len(test_crp)), test_crp)\naxes[0].set_xticks(np.arange(0, len(test_crp), 4))\naxes[0].set_xticklabels(np.arange(0, len(test_crp), 4) - (cmr_model.item_count - 1))\naxes[0].set_title('MCF')\n\n# same for MFC\n#test_crp = connectivity_by_lag(cmr_model.mfc[:, 1:-1], cmr_model.item_count)\ntest_crp = mixed_connectivity_by_lag(cmr_model.mfc[:, 1:-1], np.arange(cmr_model.item_count, dtype=int))\ntest_crp[cmr_model.item_count-1] = np.nan\naxes[1].plot(np.arange(len(test_crp)), test_crp)\naxes[1].set_xticks(np.arange(0, len(test_crp), 4))\naxes[1].set_xticklabels(np.arange(0, len(test_crp), 4) - (cmr_model.item_count - 1))\naxes[1].set_title('MFC')\nfig.suptitle('PrototypeCMR Item Connectivity By Lag')\n\n\nText(0.5, 0.98, 'PrototypeCMR Item Connectivity By Lag')\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5), sharey=True)\ntest_crp = (connectivity_by_lag(cmr_model.mcf[1:-1, :], cmr_model.item_count) + connectivity_by_lag(cmr_model.mfc[:, 1:-1], cmr_model.item_count))/2\ntest_crp[cmr_model.item_count-1] = np.nan\naxes.plot(np.arange(len(test_crp)), test_crp)\naxes.set_xticks(np.arange(0, len(test_crp), 4))\naxes.set_xticklabels(np.arange(0, len(test_crp), 4) - (cmr_model.item_count - 1))\naxes.set_title('Mean')\n\ntest_crp\n\narray([0.19768762, 0.19768767, 0.19768774, 0.19768784, 0.19768799,\n       0.19768822, 0.19768855, 0.19768903, 0.19768974, 0.1976908 ,\n       0.19769235, 0.19769463, 0.197698  , 0.19770296, 0.19771026,\n       0.19772103, 0.19773689, 0.19776027, 0.19779471, 0.19784545,\n       0.19792021, 0.19803037, 0.19819267, 0.19843182, 0.19878417,\n       0.19930334, 0.20006828, 0.20119534, 0.20285597, 0.20530276,\n       0.20890787, 0.21421967, 0.22204611, 0.23357766, 0.25056832,\n       0.27560249, 0.31248804, 0.36683547, 0.4469114 ,        nan,\n       0.4469114 , 0.36683547, 0.31248804, 0.27560249, 0.25056832,\n       0.23357766, 0.22204611, 0.21421967, 0.20890787, 0.20530276,\n       0.20285597, 0.20119534, 0.20006828, 0.19930334, 0.19878417,\n       0.19843182, 0.19819267, 0.19803037, 0.19792021, 0.19784545,\n       0.19779471, 0.19776027, 0.19773689, 0.19772103, 0.19771026,\n       0.19770296, 0.197698  , 0.19769463, 0.19769235, 0.1976908 ,\n       0.19768974, 0.19768903, 0.19768855, 0.19768822, 0.19768799,\n       0.19768784, 0.19768774, 0.19768767, 0.19768762])\n\n\n\n\n\n\n\nInstanceCMR\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n\ntest_crp = connectivity_by_lag(latent_mcf[:, 1:-1], model.item_count)\ntest_crp[model.item_count-1] = np.nan\naxes[0].plot(np.arange(len(test_crp)), test_crp)\naxes[0].set_xticks(np.arange(0, len(test_crp), 4))\naxes[0].set_xticklabels(np.arange(0, len(test_crp), 4) - (model.item_count - 1))\naxes[0].set_title('Context to Feature Associations (MCF)')\n\n# same for MFC\ntest_crp = connectivity_by_lag(latent_mfc[:, 1:-1], model.item_count)\ntest_crp[model.item_count-1] = np.nan\naxes[1].plot(np.arange(len(test_crp)), test_crp)\naxes[1].set_xticks(np.arange(0, len(test_crp), 4))\naxes[1].set_xticklabels(np.arange(0, len(test_crp), 4) - (model.item_count - 1))\naxes[1].set_title('Feature to Context Associations (MFC)')\nfig.suptitle('InstanceCMR Item Connectivity By Lag')\n\nText(0.5, 0.98, 'InstanceCMR Item Connectivity By Lag')\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5), sharey=True)\n\ntest_crp = (connectivity_by_lag(latent_mcf[:, 1:-1], model.item_count) + connectivity_by_lag(latent_mfc[:, 1:-1], model.item_count))/2\ntest_crp[model.item_count-1] = np.nan\naxes.plot(np.arange(len(test_crp)), test_crp)\naxes.set_xticks(np.arange(0, len(test_crp), 4))\naxes.set_xticklabels(np.arange(0, len(test_crp), 4) - (model.item_count - 1))\naxes.set_title('InstanceCMR Mean')\ntest_crp\n\narray([0.0044123 , 0.00442927, 0.00439535, 0.00435875, 0.0043293 ,\n       0.00430716, 0.00429065, 0.00427818, 0.00426855, 0.00426094,\n       0.00425479, 0.00424974, 0.00424552, 0.00424197, 0.00423898,\n       0.0042365 , 0.00423451, 0.00423307, 0.0042323 , 0.00423243,\n       0.00423386, 0.00423727, 0.00424375, 0.00425509, 0.00427414,\n       0.00430555, 0.00435678, 0.00443984, 0.00457406, 0.00479056,\n       0.00513945, 0.00570143, 0.00660656, 0.00806467, 0.01041486,\n       0.01420667, 0.0203346 , 0.03026503, 0.04642974,        nan,\n       0.08096561, 0.03590036, 0.01818106, 0.01067097, 0.00731709,\n       0.00574602, 0.00497271, 0.004572  , 0.00435357, 0.00422872,\n       0.00415411, 0.00410753, 0.004077  , 0.00405579, 0.00403998,\n       0.00402722, 0.00401606, 0.00400556, 0.00399511, 0.00398428,\n       0.00397273, 0.00396015, 0.00394627, 0.00393077, 0.00391328,\n       0.00389336, 0.00387047, 0.00384388, 0.00381267, 0.00377558,\n       0.00373095, 0.00367647, 0.00360897, 0.00352402, 0.00341535,\n       0.00327409, 0.00308763, 0.00283816, 0.00250086])\n\n\n\n\n\n\n\nNoisyCMR\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n\ntest_crp = connectivity_by_lag(noisy_latent_mcf[:, 1:-1], model.item_count)\ntest_crp[model.item_count-1] = np.nan\naxes[0].plot(np.arange(len(test_crp)), test_crp)\naxes[0].set_xticks(np.arange(0, len(test_crp), 4))\naxes[0].set_xticklabels(np.arange(0, len(test_crp), 4) - (model.item_count - 1))\naxes[0].set_title('MCF')\n\n# same for MFC\ntest_crp = connectivity_by_lag(noisy_latent_mfc[:, 1:-1], model.item_count)\ntest_crp[model.item_count-1] = np.nan\naxes[1].plot(np.arange(len(test_crp)), test_crp)\naxes[1].set_xticks(np.arange(0, len(test_crp), 4))\naxes[1].set_xticklabels(np.arange(0, len(test_crp), 4) - (model.item_count - 1))\naxes[1].set_title('MFC')\nfig.suptitle('InstanceCMR Item Connectivity By Lag')\n\nText(0.5, 0.98, 'InstanceCMR Item Connectivity By Lag')\n\n\n\n\n\n\n\nNotes\nCMR seems to fit toward stronger MCF weights overall relative to MFC. But when you turn sharey off during plotting, it’s clear that the effect is more of a translation up the y-axis than a meaningful change in the pattern of connectivities (aside from the reflection along y-axis thing).\nInstanceCMR though seems to do no such translation, instead making its latent MCF “steeper” than its latent MFC.\nWhat if I take an average? CMR only gets assymetry if I do a weighted average that downweights MFC importance. InstanceCMR needs no such transformation. Neither of these CRPs exactly match what I see in the data, but that makes some sense. It’s unclear whether these observations imply more fundamental differences between model architectures."
  },
  {
    "objectID": "library\\model_analysis\\Contiguity_Tracing.html#parameter-shifting",
    "href": "library\\model_analysis\\Contiguity_Tracing.html#parameter-shifting",
    "title": "compmemlearn",
    "section": "Parameter Shifting",
    "text": "Add experiments to this notebook tracing how changing relevant model parameters simulataneously shapes CRP structure and memory connections, focusing on learning and drift rates parameter-wise and contiguity and assymetry outcome-wise. I’m done when I know high-level (parametric) ways to control contiguity.\n\nfrom tqdm import tqdm\nfrom compmemlearn.analyses import fast_spc, fast_crp, fast_pfr\nimport pandas as pd\n\nmax_lag = 13\n\nscore_ranges = {\n    'encoding_drift_rate': np.arange(.001, .99, .1),\n    'recall_drift_rate': np.arange(.001, .99, .1),\n    'shared_support': np.arange(.001, .005, .001),\n    'item_support': np.arange(.001, .01, .001),\n    'learning_rate': np.arange(.001, .99, .1),\n    'choice_sensitivity': np.arange(.001, 5, .5),\n}\n\nfor model_class in [Classic_CMR]:\n\n    if model_class.__name__ == 'Classic_CMR':\n        parameters = cmr_fitted_parameters\n    else:\n        parameters = icmr_fitted_parameters\n\n    for varied_parameter in score_ranges.keys():\n        print(varied_parameter, parameters[varied_parameter])\n        crps = []\n        spcs = []\n        pfrs = []\n        mfc_connectivities = []\n        mcf_connectivities = []\n\n        for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n\n            # simulate data with this parameter value modified\n            sub_params = parameters.copy()\n            sub_params[varied_parameter] = parameter_value\n            model = model_class(list_length, list_length, sub_params)\n            simulation = simulate_array(model, 10000)\n\n            # accumulate spcs, crps, pfrs\n            spc = fast_spc(simulation, list_length)\n            spc = pd.DataFrame(\n                {'Study Position': np.arange(len(spc)), 'Recall Rate': spc, varied_parameter: parameter_value})\n            spcs.append(spc)\n\n            crp = fast_crp(simulation, list_length)\n            crp[list_length-1] = np.nan\n            crp = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': crp[list_length-max_lag-1:list_length+max_lag], varied_parameter: parameter_value})\n            crps.append(crp)\n\n            pfr = fast_pfr(simulation, list_length)\n            pfr = pd.DataFrame(\n                {'Study Position': np.arange(len(pfr)), 'First Recall Rate': pfr, varied_parameter: parameter_value})\n            pfrs.append(pfr)\n            \n            if model_class.__name__ == 'Classic_CMR':\n                mfc_connectivity = connectivity_by_lag(model.mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(model.mcf[1:-1, :], model.item_count)\n            else:\n                latent_mfc, latent_mcf = latent_mfc_mcf(model)\n                mfc_connectivity = connectivity_by_lag(latent_mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(latent_mcf[: , 1:-1], model.item_count)\n\n            mfc_connectivity[model.item_count-1] = np.nan\n            mcf_connectivity[model.item_count-1] = np.nan\n            mfc_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mfc_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mcf_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mcf_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mfc_connectivities.append(mfc_connectivity)\n            mcf_connectivities.append(mcf_connectivity)\n\n        # concatenate result into a single table\n        spc = pd.concat(spcs).reset_index()\n        crp = pd.concat(crps).reset_index()\n        pfr = pd.concat(pfrs).reset_index()\n        mfc_connectivity = pd.concat(mfc_connectivities).reset_index()\n        mcf_connectivity = pd.concat(mcf_connectivities).reset_index()\n\n        sns.set(style='darkgrid')\n        fig, axes = plt.subplots(5, 1, figsize=(10, 20), sharey=False)\n\n        sns.lineplot(ax=axes[0], data=spc, x='Study Position', y='Recall Rate', hue=varied_parameter, ci=None)\n        #axes[0].set_xlabel('Study Position')\n        #axes[0].set_ylabel('Probability Recall')\n        axes[0].set_title('SPC')\n        axes[0].legend(np.round(score_ranges[varied_parameter], 5), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n        sns.lineplot(ax=axes[1], data=pfr, x='Study Position', y='First Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[1].set_title('PFR')\n\n        filt_neg = f'{-max_lag} <= Lag < 0'\n        filt_pos = f'0 < Lag <= {max_lag}'\n        sns.lineplot(ax=axes[2], data=crp.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[2], data=crp.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[2].set_title('Lag-CRP')\n\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[3].set_title('MFC Lag-Connectivity')\n        \n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[4].set_title('MCF Lag-Connectivity')\n\n        plt.tight_layout(pad=2)\n\n        fig.suptitle(varied_parameter.replace('_', ' ').upper())\n        #plt.savefig('results/{}_{}.svg'.format(model.__name__, varied_parameter))\n        plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nencoding_drift_rate 0.7344167192532168\n\n\n100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nrecall_drift_rate 0.9322928178785643\n\n\n100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n\n\n\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n\n\nshared_support 0.39537504326174394\n\n\n100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n\n\n\n\n\n  0%|          | 0/9 [00:00<?, ?it/s]\n\n\nitem_support 2.220446049250313e-16\n\n\n100%|██████████| 9/9 [00:06<00:00,  1.46it/s]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nlearning_rate 0.9999999999999998\n\n\n100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nchoice_sensitivity 5.760897798064937\n\n\n100%|██████████| 10/10 [00:07<00:00,  1.42it/s]\n\n\n\n\n\n\nNotes\n\nICMR\n\nEncoding drift rate messes with SPC height at high value for some reason. CRP sharpness grows with drift rate. With sharpness comes greater assymetry. Can I control one without the other?\nRecall drift rate has similar effect as encoding drift rate, except my high encoding drift rate seems protective of contiguity even with low recall drift rate. With low recall drift rate, contiguity is not so assymetric even though it’s there, so that’s a neat differentiation. Still, sharpness and assymetry are tied here.\nshared_support breaks the model unless configured to low value. Must return to this. Oh, as this increases, contiguity is suppressed. Assymetry largely untouched.\nitem support configures initial mfc weights and seems to mainly control sharpness w/o affecting assymetry as much, idk.\nLearning rate specifically controls contiguity. High values balance CRP while low values make assymetry super strong. No direct control of mean CRP “height”, I think. This is maybe really valuable, but I haven’t had much success manipulating it in CMR-DE.\nchoice sensitivity seems to control height of CRP w/o disrupting assymetry much except at very low values.\n\nSo we have a lot of parameters that suppress contiguity in a way that also erases apparent assymetry at different rates. And we have a parameter that directly controls assymetry w/o controlling height. Any of these might prove useful tools for getting me toward the suppressed relative contiguity I’m looking for.\n\n\nCMR\n\nEncoding drift rate behavior is same as ICMR’s. Seems clearer that low values suppress assymetry and overall contiguity.\nRecall drift rate too. Clearer than before that recall drift rate is an avenue to impact assymetry w/o erasing contiguity as quickly as\nRelevant range for shared_support seems wildly different. Must return.\nItem support has a weird experiment I wanna return to but overall seems to drive greater negative assymetry at higher values rather than positive. Must compare with ICMR again. I need to consider a smaller range of values for CMR too since the parameter fits very close to 0. I wonder if I have the model implemented wrong?\nLearning rate works similarly. Fits to as sharp assymetry as possible, suggesting the model wishes it could go further.\nModel very noisy with choice sensitivity values below 1.0 apparently.\n\n\n\nLag_Connectivity\nEncoding drift rate affects both MFC and MCF lag-connectivity. Low values make flat memories. As value increases, memory remains flat but grows in height. Then after some threshold, contiguity sharpness rather than memory strength is modified.\nRecall drift rate doesn’t affect memory, as it shouldn’t.\nShared support especially impacts MFC lag-connecitivity, mainly more distal lags, effectively suppressing lag-contiguity at higher values.\nItem support affects both MFC and MCF lag-connectivity. Opposite transitions is a flat line that translate down as parameter value increases. Main contiguity starts in wrong direction at low values and increases to produce contiguity. Definitely not a parameter to mess with, this shit is weird.\nLearning rate exclusively impacts MFC. Which makes sense because MFC doesn’t affect learning the way MCF does. Wait, that’s not true. Why does higher learning rate suppress forward transitions if it doesn’t affect MCF? Guess because context gets more support for negative transitions, okay.\nChoice sensitivity similarly affects CRP without affecting memory representations.\nSo we get an interesting divide! Recall-based mechanisms can constrain CRP"
  },
  {
    "objectID": "library\\model_analysis\\Data_Likelihood_Under_Model.html#testing-the-likelihood-functions",
    "href": "library\\model_analysis\\Data_Likelihood_Under_Model.html#testing-the-likelihood-functions",
    "title": "compmemlearn",
    "section": "Testing the Likelihood Functions",
    "text": "We’ll make sure the likelihood functions still return expected values depending on pre-fitted parameters from our Murdock, 1962 and Lohnas & Kahana, 2014 dataset analyses.\nfrom compmemlearn.models import Classic_CMR\nfrom numba.typed import List, Dict\nfrom compmemlearn.datasets import prepare_murdock1962_data, prepare_lohnas2014_data\n\nmurdock_data_likelihood\n\nmurd_trials0, murd_events0, murd_length0 = prepare_murdock1962_data('../../data/MurdData_clean.mat', 0)\nmurd_trials1, murd_events1, murd_length1 = prepare_murdock1962_data('../../data/MurdData_clean.mat', 1)\nmurd_trials2, murd_events2, murd_length2 = prepare_murdock1962_data('../../data/MurdData_clean.mat', 2)\n\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nfit_values = np.array([5.88304182e-01, 3.76144942e-02, 7.51294302e-01, 2.91680115e-01,\n       1.00000000e+00, 1.39633721e-01, 5.62625588e+00, 4.28789782e+01,\n       2.40537436e-02, 2.61824232e-01, 5.32941045e+00, 9.34036191e-01])\n\ncmr_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(fit_values)):\n    cmr_parameters[free_parameters[i]] = fit_values[i]\ncmr_parameters['sampling_rule'] = 0\ncmr_parameters['mfc_familiarity_scale'] = 0\ncmr_parameters['mcf_familiarity_scale'] = 0\ncmr_parameters['drift_familiarity_scale'] = 0\n\n@njit(fastmath=True, nogil=True)\ndef init_cmr(item_count, presentation_count, parameters):\n    return Classic_CMR(item_count, presentation_count, parameters)\n\nprint('murdock, 1962 == 80052.6888735279')\nprint(murdock_data_likelihood(\n    (murd_trials0, murd_trials1, murd_trials2), \n    (murd_length0,murd_length1, murd_length2), \n    init_cmr, cmr_parameters))\n\n%timeit murdock_data_likelihood(List([murd_trials0, murd_trials1, murd_trials2]), List([murd_length0,murd_length1, murd_length2]), init_cmr, cmr_parameters)\n\nmurdock, 1962 == 80052.6888735279\n80052.6888735276\n59.4 ms ± 961 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\nlohnas_data_likelihood\n\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nfit_values = np.array([8.52979921e-01, 9.68207643e-02, 9.64470531e-01, 3.73838044e-02,\n       2.22044605e-16, 4.62850531e-01, 3.18792422e+00, 8.62357681e+01,\n       2.13805131e-02, 1.06861707e-01, 1.18381379e+00, 9.83789369e-01])\n\nfor i in range(len(fit_values)):\n    cmr_parameters[free_parameters[i]] = fit_values[i]\n\nprint('lohnas, 2014 == 17271.524963186363')\nprint(lohnas_data_likelihood(\n    trials[list_types == 4], \n    presentations[list_types == 4], \n    init_cmr, cmr_parameters))\n\n%timeit lohnas_data_likelihood(trials[list_types == 4], presentations[list_types == 4], init_cmr, cmr_parameters)\n\nlohnas, 2014 == 17271.524963186363\n17271.524963184707\n39.4 ms ± 2.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-pure-lists-with-murdock_data_likelihood",
    "href": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-pure-lists-with-murdock_data_likelihood",
    "title": "compmemlearn",
    "section": "Fitting Pure Lists with murdock_data_likelihood",
    "text": "With the runtime of the cost function established, we’ll now use it with scipy’s differential_evolution function to find the best fit parameters for the model to the single subject we’re examining.\n\nLoading and Optimizing Along Cost Function\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = murdock_objective_function(\n    (murd_trials0, murd_trials1, murd_trials2), \n    (murd_length0,murd_length1, murd_length2), \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    free_parameters)\n\nresult = differential_evolution(cost_function, bounds, disp=True)\nprint(result)\n\ndifferential_evolution step 1: f(x)= 97308.5\ndifferential_evolution step 2: f(x)= 91013.4\ndifferential_evolution step 3: f(x)= 91013.4\ndifferential_evolution step 4: f(x)= 91013.4\ndifferential_evolution step 5: f(x)= 91013.4\ndifferential_evolution step 6: f(x)= 88452\ndifferential_evolution step 7: f(x)= 86792\ndifferential_evolution step 8: f(x)= 84293.4\ndifferential_evolution step 9: f(x)= 84293.4\ndifferential_evolution step 10: f(x)= 84293.4\ndifferential_evolution step 11: f(x)= 82942.1\ndifferential_evolution step 12: f(x)= 82942.1\ndifferential_evolution step 13: f(x)= 82942.1\ndifferential_evolution step 14: f(x)= 82518\ndifferential_evolution step 15: f(x)= 82311.4\ndifferential_evolution step 16: f(x)= 81474.7\ndifferential_evolution step 17: f(x)= 81474.7\ndifferential_evolution step 18: f(x)= 81474.7\ndifferential_evolution step 19: f(x)= 81474.7\ndifferential_evolution step 20: f(x)= 81474.7\ndifferential_evolution step 21: f(x)= 81474.7\ndifferential_evolution step 22: f(x)= 81474.7\ndifferential_evolution step 23: f(x)= 81474.7\ndifferential_evolution step 24: f(x)= 81474.7\ndifferential_evolution step 25: f(x)= 81474.7\ndifferential_evolution step 26: f(x)= 81474.7\ndifferential_evolution step 27: f(x)= 81474.7\ndifferential_evolution step 28: f(x)= 81036.4\ndifferential_evolution step 29: f(x)= 81036.4\ndifferential_evolution step 30: f(x)= 81036.4\ndifferential_evolution step 31: f(x)= 81036.4\ndifferential_evolution step 32: f(x)= 81036.4\ndifferential_evolution step 33: f(x)= 81036.4\ndifferential_evolution step 34: f(x)= 81036.4\ndifferential_evolution step 35: f(x)= 81036.4\ndifferential_evolution step 36: f(x)= 81036.4\ndifferential_evolution step 37: f(x)= 81036.4\ndifferential_evolution step 38: f(x)= 80813.8\ndifferential_evolution step 39: f(x)= 80813.8\ndifferential_evolution step 40: f(x)= 80591.5\ndifferential_evolution step 41: f(x)= 80591.5\n     fun: 80047.41028350465\n     jac: array([-1.34896253e+00,  4.80213202e-01,  1.85682437e+00, -5.99829946e+00,\n       -1.40143674e+02,  3.74129741e+00,  1.29512046e-01,  0.00000000e+00,\n       -1.22934580e+01, -8.96397979e-01,  4.49654183e-01, -2.86818248e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 9419\n     nit: 41\n success: True\n       x: array([6.00473229e-01, 3.32875358e-02, 7.50604203e-01, 2.27164284e-01,\n       1.00000000e+00, 1.30584257e-01, 6.45835896e+00, 6.06522276e+01,\n       2.40310219e-02, 2.61880252e-01, 4.39924093e+00, 9.34438836e-01])\n\n\nWe expect these results:\n     fun: 80052.68887354505\n     jac: array([  10.6170773 ,   17.17708073,   -2.77505022,   -2.50875019,\n       -123.24599022,   17.89885574,   -2.93221094,    0.        ,\n        118.13972376,   26.6605639 ,    7.70960473,    7.07950672])\n message: 'Optimization terminated successfully.'\n    nfev: 7882\n     nit: 36\n success: True\n       x: array([5.88304182e-01, 3.76144942e-02, 7.51294302e-01, 2.91680115e-01,\n       1.00000000e+00, 1.39633721e-01, 5.62625588e+00, 4.28789782e+01,\n       2.40537436e-02, 2.61824232e-01, 5.32941045e+00, 9.34036191e-01])\nThe x attribute of the result object contains the best parameter configuration found, while the fun attribute represents the overall cost of the configuration as computed with our specified cost function.\n\n\nVisualizing Fit\nNext we’ll visualize the fit of the model and its parameters to the data. We’ll do this by simulating a dataset using the models and our parameters found above and plotting its simulated benchmark recall phenomena (serial position curve, lag-CRP, and probability of first recall) against the actual data. A new helper function called apply_and_concatenate helps streamline the process of setting up tables for comparison of an analysis outcome between simulated and real data.\n# export\nimport pandas as pd\n\ndef apply_and_concatenate(function, df1, df2, contrast_name='contrast', labels='AB'):\n    \"\"\"\n    Concatenates the results of a function applied to two dataframes and creates a new column identifying the contrast.\n    \"\"\"\n    return pd.concat([function(df1), function(df2)], keys=labels, names=[contrast_name]).reset_index()\nNow let’s create some simulated data with our model and fitted parameters.\nfrom compmemlearn.datasets import simulate_df\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(result.x)):\n    fitted_parameters[free_parameters[i]] = result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nmodel0 = Classic_CMR(murd_length0, murd_length0, fitted_parameters)\nmodel1 = Classic_CMR(murd_length1, murd_length1, fitted_parameters)\nmodel2 = Classic_CMR(murd_length2, murd_length2, fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\nsim_df1 = simulate_df(model1, 1000)\nsim_df2 = simulate_df(model2, 1000)\ntrue_df0 = murd_events0.copy()\ntrue_df1 = murd_events1.copy()\ntrue_df2 = murd_events2.copy()\nThen extract summary statistics…\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\ncmr_spc1 = apply_and_concatenate(fr.spc, sim_df1, true_df1, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp1 = apply_and_concatenate(fr.lag_crp, sim_df1, true_df1, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr1 = apply_and_concatenate(fr.pnr, sim_df1, true_df1, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr1 = cmr_pfr1.query('output <= 1')\n\ncmr_spc2 = apply_and_concatenate(fr.spc, sim_df2, true_df2, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp2 = apply_and_concatenate(fr.lag_crp, sim_df2, true_df2, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr2 = apply_and_concatenate(fr.pnr, sim_df2, true_df2, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr2 = cmr_pfr2.query('output <= 1')\nAnd plot the result…\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12/1.5), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0, 0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[0, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 1], data=cmr_spc1, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0, 1].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[0, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[0, 2], data=cmr_spc2, x='input', y='recall', err_style='bars', hue='source', legend=True)\naxes[0, 2].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[0, 2].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1, 0], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 0], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 0].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 0].set_xticks(np.arange(-5, 6, 1))\naxes[1, 0].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp1.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 1], data=cmr_lag_crp1.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 1].set(xlabel=\"Lag From Last Recalled Item\", ylabel='Conditional Recall Rate')\naxes[1, 1].set_xticks(np.arange(-5, 6, 1))\naxes[1, 1].set_ylim((0, 1))\n\nsns.lineplot(ax=axes[1, 2], data=cmr_lag_crp2.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1, 2], data=cmr_lag_crp2.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1, 2].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1, 2].set_xticks(np.arange(-5, 6, 1))\naxes[1, 2].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2, 0], hue='source', legend=False)\naxes[2, 0].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 0].set_xticks(np.arange(1, murd_length0+1, 2))\naxes[2, 0].set_ylim((0, 1))\n\nsns.lineplot(data=cmr_pfr1, x='input', y='prob', err_style='bars', ax=axes[2, 1], hue='source', legend=False)\naxes[2, 1].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 1].set_xticks(np.arange(1, murd_length1+1, 3))\naxes[2, 1].set_ylim((0, 1))\n\nsns.lineplot(data=cmr_pfr2, x='input', y='prob', err_style='bars', ax=axes[2, 2], hue='source', legend=False)\naxes[2, 2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2, 2].set_xticks(np.arange(1, murd_length2+1, 4))\naxes[2, 2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\n\n\n\n\n\n\nWhy aren’t the fits better?\nThe model doesn’t actually do a good job of capturing some of the quirks in the data – namely the graded probability of first recall curve in that last plot. CMR predicts a much sharper curve than the actual data and its struggle to account for something different results in worse apparent fits to other benchmark recall phenomena visualized here too. At least, that’s our best guess about what’s behind the failure."
  },
  {
    "objectID": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-pure-lists-with-lohnas_data_likelihood",
    "href": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-pure-lists-with-lohnas_data_likelihood",
    "title": "compmemlearn",
    "section": "Fitting Pure Lists with lohnas_data_likelihood",
    "text": "lohnas_data_likelihood is almost always slower than murdock_data_likelihood when it comes to fitting pure lists, but it’s useful to confirm that the functions work similarly in the use cases under which they overlap.\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate'\n)\n\nLoading and Optimizing Along Cost Function\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = lohnas_objective_function(\n    trials[list_types == 1], \n    presentations[list_types == 1], \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    free_parameters)\n\nlohnas_result = differential_evolution(cost_function, bounds, disp=True)\nprint(lohnas_result)\n\ndifferential_evolution step 1: f(x)= 22456.8\ndifferential_evolution step 2: f(x)= 21794.3\ndifferential_evolution step 3: f(x)= 21794.3\ndifferential_evolution step 4: f(x)= 21794.3\ndifferential_evolution step 5: f(x)= 21794.3\ndifferential_evolution step 6: f(x)= 21794.3\ndifferential_evolution step 7: f(x)= 21794.3\ndifferential_evolution step 8: f(x)= 20789\ndifferential_evolution step 9: f(x)= 20711.1\ndifferential_evolution step 10: f(x)= 20552.2\ndifferential_evolution step 11: f(x)= 20552.2\ndifferential_evolution step 12: f(x)= 19784.7\ndifferential_evolution step 13: f(x)= 19524.3\ndifferential_evolution step 14: f(x)= 19524.3\ndifferential_evolution step 15: f(x)= 19524.3\ndifferential_evolution step 16: f(x)= 19262.8\ndifferential_evolution step 17: f(x)= 19194.9\ndifferential_evolution step 18: f(x)= 19194.9\ndifferential_evolution step 19: f(x)= 19194.9\ndifferential_evolution step 20: f(x)= 19194.9\ndifferential_evolution step 21: f(x)= 19194.9\ndifferential_evolution step 22: f(x)= 19043.8\ndifferential_evolution step 23: f(x)= 19043.8\ndifferential_evolution step 24: f(x)= 19038.3\ndifferential_evolution step 25: f(x)= 19038.3\ndifferential_evolution step 26: f(x)= 19038.3\ndifferential_evolution step 27: f(x)= 19038.3\ndifferential_evolution step 28: f(x)= 18939.9\ndifferential_evolution step 29: f(x)= 18939.9\ndifferential_evolution step 30: f(x)= 18939.9\ndifferential_evolution step 31: f(x)= 18939.9\ndifferential_evolution step 32: f(x)= 18878.9\ndifferential_evolution step 33: f(x)= 18878.9\ndifferential_evolution step 34: f(x)= 18878.9\ndifferential_evolution step 35: f(x)= 18878.9\ndifferential_evolution step 36: f(x)= 18878.9\ndifferential_evolution step 37: f(x)= 18878.9\ndifferential_evolution step 38: f(x)= 18878.9\ndifferential_evolution step 39: f(x)= 18878.9\ndifferential_evolution step 40: f(x)= 18878.9\n     fun: 18810.058985583455\n     jac: array([ 2.22415109e+01, -6.99197695e+01,  2.76399076e+01,  2.91838660e+01,\n        1.67485268e+01, -4.66279741e+01, -1.05916114e+01,  0.00000000e+00,\n        4.38329152e+01, -1.48027539e+02, -2.56040950e+00, -3.63797881e-04])\n message: 'Optimization terminated successfully.'\n    nfev: 8524\n     nit: 40\n success: True\n       x: array([7.33635729e-01, 5.46844102e-01, 9.32757477e-01, 4.54648984e-01,\n       5.28712166e-02, 1.00000000e+00, 5.75890108e-01, 8.17565974e+01,\n       2.52220993e-02, 8.70307405e-02, 6.42673177e+00, 2.22044605e-16])\n\n\n\nfrom compmemlearn.datasets import simulate_df\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(lohnas_result.x)):\n    fitted_parameters[free_parameters[i]] = lohnas_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nmodel0 = Classic_CMR(40, 40, fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\ntrue_df0 = events.loc[events.condition==1]\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)\n\n\n\n\n\n\nLet’s compare this result with what we’d obtain if we fit using murdock_data_likelihood.\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = murdock_objective_function(\n    (trials[list_types==1], ), \n    (40, ), \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    free_parameters)\n\nmurdock_result = differential_evolution(cost_function, bounds, disp=True)\nprint(murdock_result)\n\ndifferential_evolution step 1: f(x)= 25374.1\ndifferential_evolution step 2: f(x)= 22149.1\ndifferential_evolution step 3: f(x)= 22149.1\ndifferential_evolution step 4: f(x)= 22149.1\ndifferential_evolution step 5: f(x)= 22149.1\ndifferential_evolution step 6: f(x)= 21726.2\ndifferential_evolution step 7: f(x)= 21186.5\ndifferential_evolution step 8: f(x)= 20212.4\ndifferential_evolution step 9: f(x)= 20212.4\ndifferential_evolution step 10: f(x)= 20212.4\ndifferential_evolution step 11: f(x)= 20212.4\ndifferential_evolution step 12: f(x)= 19736.4\ndifferential_evolution step 13: f(x)= 19736.4\ndifferential_evolution step 14: f(x)= 19315.3\ndifferential_evolution step 15: f(x)= 19315.3\ndifferential_evolution step 16: f(x)= 19310.9\ndifferential_evolution step 17: f(x)= 19166.4\ndifferential_evolution step 18: f(x)= 19166.4\ndifferential_evolution step 19: f(x)= 19166.4\ndifferential_evolution step 20: f(x)= 19166.4\ndifferential_evolution step 21: f(x)= 19166.4\ndifferential_evolution step 22: f(x)= 19161.6\ndifferential_evolution step 23: f(x)= 19161.6\ndifferential_evolution step 24: f(x)= 19136.2\ndifferential_evolution step 25: f(x)= 19113.5\ndifferential_evolution step 26: f(x)= 18970.4\ndifferential_evolution step 27: f(x)= 18931.5\ndifferential_evolution step 28: f(x)= 18931.5\ndifferential_evolution step 29: f(x)= 18931.5\ndifferential_evolution step 30: f(x)= 18925.5\ndifferential_evolution step 31: f(x)= 18925.5\ndifferential_evolution step 32: f(x)= 18925.5\ndifferential_evolution step 33: f(x)= 18925.5\ndifferential_evolution step 34: f(x)= 18925.5\ndifferential_evolution step 35: f(x)= 18899.1\ndifferential_evolution step 36: f(x)= 18899.1\ndifferential_evolution step 37: f(x)= 18899.1\ndifferential_evolution step 38: f(x)= 18899.1\n     fun: 18807.983211993796\n     jac: array([ 1.09866959e-01, -1.17142917e-01, -9.85892252e-02, -1.12049747e-01,\n        1.20442564e+01, -4.45139447e+01, -2.14640749e-02,  0.00000000e+00,\n        1.42608769e-01, -3.45607987e-02,  9.45874496e-03,  0.00000000e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 8385\n     nit: 38\n success: True\n       x: array([7.34343133e-01, 5.77169827e-01, 9.32322680e-01, 3.95403975e-01,\n       2.22044605e-16, 1.00000000e+00, 5.52537331e-01, 5.21590857e+01,\n       2.36174418e-02, 9.12284879e-02, 5.76055376e+00, 2.22044605e-16])\n\n\n\nfrom compmemlearn.datasets import simulate_df\n\nfitted_parameters = Dict.empty(key_type=types.unicode_type, value_type=types.float64)\nfor i in range(len(murdock_result.x)):\n    fitted_parameters[free_parameters[i]] = murdock_result.x[i]\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nmodel0 = Classic_CMR(40, 40, fitted_parameters)\n\nsim_df0 = simulate_df(model0, 1000)\ntrue_df0 = events.loc[events.condition==1]\n\ncmr_spc0 = apply_and_concatenate(fr.spc, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_lag_crp0 = apply_and_concatenate(fr.lag_crp, sim_df0, true_df0, 'source', ['PrototypeCMR', 'data'])\ncmr_pfr0 = apply_and_concatenate(fr.pnr, sim_df0, true_df0, contrast_name='source', labels=['PrototypeCMR', 'data'])\ncmr_pfr0 = cmr_pfr0.query('output <= 1')\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 12/3), sharey=False)\n\n# serial position curve\nsns.lineplot(ax=axes[0], data=cmr_spc0, x='input', y='recall', err_style='bars', hue='source', legend=False)\naxes[0].set(xlabel='Study Position', ylabel='Recall Rate')\naxes[0].set_xticks(np.arange(1, list_length+1, 3))\naxes[0].set_ylim((0, 1))\n\n# lag CRP\nmax_lag = 5\nfilt_neg = f'{-max_lag} <= lag < 0'\nfilt_pos = f'0 < lag <= {max_lag}'\n\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_neg), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\nsns.lineplot(ax=axes[1], data=cmr_lag_crp0.query(filt_pos), x='lag', y='prob', \n             err_style='bars', hue='source', legend=False)\naxes[1].set(xlabel='Lag From Last Recalled Item', ylabel='Conditional Recall Rate')\naxes[1].set_xticks(np.arange(-5, 6, 1))\naxes[1].set_ylim((0, 1))\n\n# pfr\nsns.lineplot(data=cmr_pfr0, x='input', y='prob', err_style='bars', ax=axes[2], hue='source', legend=True)\naxes[2].set(xlabel='Study Position', ylabel='Probability of First Recall')\naxes[2].set_xticks(np.arange(1, list_length+1, 3))\naxes[2].set_ylim((0, 1))\n\n# set legend of axis 2 outside the plot, to the right\naxes[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.tight_layout(pad=2)"
  },
  {
    "objectID": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-mixed-lists-with-lohnas_data_likelihood",
    "href": "library\\model_analysis\\Data_Likelihood_Under_Model.html#fitting-mixed-lists-with-lohnas_data_likelihood",
    "title": "compmemlearn",
    "section": "Fitting Mixed Lists with lohnas_data_likelihood",
    "text": "trials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate',\n)\n\nLoading and Optimizing Along Cost Function\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = lohnas_objective_function(\n    trials[list_types == 4], \n    presentations[list_types == 4], \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    free_parameters)\n\nlohnas_result = differential_evolution(cost_function, bounds, disp=True)\nprint(lohnas_result)\n\ndifferential_evolution step 1: f(x)= 21960.7\ndifferential_evolution step 2: f(x)= 19692\ndifferential_evolution step 3: f(x)= 19692\ndifferential_evolution step 4: f(x)= 19692\ndifferential_evolution step 5: f(x)= 19692\ndifferential_evolution step 6: f(x)= 19692\ndifferential_evolution step 7: f(x)= 19692\ndifferential_evolution step 8: f(x)= 19692\ndifferential_evolution step 9: f(x)= 19106\ndifferential_evolution step 10: f(x)= 19106\ndifferential_evolution step 11: f(x)= 19106\ndifferential_evolution step 12: f(x)= 19012.4\ndifferential_evolution step 13: f(x)= 19012.4\ndifferential_evolution step 14: f(x)= 18122.3\ndifferential_evolution step 15: f(x)= 18122.3\ndifferential_evolution step 16: f(x)= 18122.3\ndifferential_evolution step 17: f(x)= 18122.3\ndifferential_evolution step 18: f(x)= 18035.6\ndifferential_evolution step 19: f(x)= 18035.6\ndifferential_evolution step 20: f(x)= 17991.2\ndifferential_evolution step 21: f(x)= 17770.7\ndifferential_evolution step 22: f(x)= 17711\ndifferential_evolution step 23: f(x)= 17711\ndifferential_evolution step 24: f(x)= 17657.4\ndifferential_evolution step 25: f(x)= 17433.7\ndifferential_evolution step 26: f(x)= 17433.7\ndifferential_evolution step 27: f(x)= 17433.7\ndifferential_evolution step 28: f(x)= 17433.7\ndifferential_evolution step 29: f(x)= 17433.7\ndifferential_evolution step 30: f(x)= 17433.7\ndifferential_evolution step 31: f(x)= 17433.7\ndifferential_evolution step 32: f(x)= 17433.7\ndifferential_evolution step 33: f(x)= 17433.7\ndifferential_evolution step 34: f(x)= 17433.7\ndifferential_evolution step 35: f(x)= 17433.7\ndifferential_evolution step 36: f(x)= 17433.7\ndifferential_evolution step 37: f(x)= 17433.7\ndifferential_evolution step 38: f(x)= 17433.7\ndifferential_evolution step 39: f(x)= 17360\n     fun: 17170.906872565276\n     jac: array([-3.17231750e-01,  4.29281499e-02,  2.78305377e-01, -1.91357685e-01,\n        2.77457730e+01, -2.11002771e-02,  4.72937248e-03,  8.58562999e-02,\n       -4.09636414e+00, -7.53061613e-01,  1.41881174e-02,  0.00000000e+00])\n message: 'Optimization terminated successfully.'\n    nfev: 9553\n     nit: 39\n success: True\n       x: array([8.37481840e-01, 3.33421728e-01, 9.66834627e-01, 5.95371489e-02,\n       2.22044605e-16, 4.45280033e-01, 4.47215803e+00, 4.07037005e-01,\n       2.13742105e-02, 1.06866782e-01, 1.31660740e+00, 8.61694026e-09])\n\n\nfrom compmemlearn.datasets import simulate_array_from_presentations\nfrom compmemlearn.analyses import fast_rpl\n\nfitted_parameters = Dict.empty(\n    key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(lohnas_result.x)):\n    fitted_parameters[free_parameters[j]] = lohnas_result.x[j]\n\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nexperiment_count = 1000\nnew_sim_array = simulate_array_from_presentations(init_cmr, fitted_parameters, presentations[list_types==4], experiment_count)\n\nimport numpy.matlib\n\nresult = fast_rpl(np.matlib.repmat(presentations[list_types==4], experiment_count, 1), new_sim_array)\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\n[0.37778061 0.45604643 0.48747679 0.50164762 0.50651667]\n\n\n\nimport matplotlib.pyplot as plt\n\nfit_sources = ['lohnas_4']\nfit_rpls = [binned]\n\nfor i in range(len(fit_sources)):\n    plt.plot(fit_rpls[i], label=fit_sources[i])\n\nresult = fast_rpl(presentations[list_types==4], trials[list_types==4])\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nplt.plot(binned, label='data')\nlags = ['N/A', '0', '1-2', '3-5', '6-8']\nplt.xticks(np.arange(len(lags)), lags)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x125811f0d90>\n\n\n\n\n\n\n\nAll Conditions\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = lohnas_objective_function(\n    trials[list_types >= 1], \n    presentations[list_types >= 1], \n    init_cmr,\n    {'sampling_rule': 0, 'mfc_familiarity_scale': 0, 'mcf_familiarity_scale': 0, 'drift_familiarity_scale': 0}, \n    free_parameters)\n\nlohnas_result = differential_evolution(cost_function, bounds, disp=True)\nprint(lohnas_result)\n\ndifferential_evolution step 1: f(x)= 94631.7\ndifferential_evolution step 2: f(x)= 79692.9\ndifferential_evolution step 3: f(x)= 70885.6\ndifferential_evolution step 4: f(x)= 69762\ndifferential_evolution step 5: f(x)= 67176.3\ndifferential_evolution step 6: f(x)= 67176.3\ndifferential_evolution step 7: f(x)= 67176.3\ndifferential_evolution step 8: f(x)= 66556.1\ndifferential_evolution step 9: f(x)= 66556.1\ndifferential_evolution step 10: f(x)= 65542\ndifferential_evolution step 11: f(x)= 65542\ndifferential_evolution step 12: f(x)= 64378.7\ndifferential_evolution step 13: f(x)= 64378.7\ndifferential_evolution step 14: f(x)= 63249.4\ndifferential_evolution step 15: f(x)= 62687.4\ndifferential_evolution step 16: f(x)= 62687.4\ndifferential_evolution step 17: f(x)= 62365.9\ndifferential_evolution step 18: f(x)= 62365.9\ndifferential_evolution step 19: f(x)= 62302.4\ndifferential_evolution step 20: f(x)= 62004.3\ndifferential_evolution step 21: f(x)= 62004.3\ndifferential_evolution step 22: f(x)= 62004.3\ndifferential_evolution step 23: f(x)= 62004.3\ndifferential_evolution step 24: f(x)= 61902.9\ndifferential_evolution step 25: f(x)= 61902.9\ndifferential_evolution step 26: f(x)= 61849.5\ndifferential_evolution step 27: f(x)= 61722.5\ndifferential_evolution step 28: f(x)= 61620.5\ndifferential_evolution step 29: f(x)= 61620.5\ndifferential_evolution step 30: f(x)= 61541.9\ndifferential_evolution step 31: f(x)= 61541.9\ndifferential_evolution step 32: f(x)= 61490\ndifferential_evolution step 33: f(x)= 61490\ndifferential_evolution step 34: f(x)= 61490\n     fun: 60797.2210129435\n     jac: array([-2.640445  , -0.65556378,  2.33558238, -2.81143002, -0.37325663,\n        0.38999133, -0.0174623 ,  0.        ,  3.73474904,  5.01822797,\n       -0.2757588 , -3.82788128])\n message: 'Optimization terminated successfully.'\n    nfev: 8913\n     nit: 34\n success: True\n       x: array([8.65844790e-01, 2.26172120e-01, 9.52083253e-01, 2.63516673e-02,\n       5.43072905e-07, 4.31711566e-01, 2.64505857e+00, 4.38402542e+01,\n       2.51585067e-02, 1.01418425e-01, 1.02244712e+00, 9.80136664e-01])\n\n\nfrom compmemlearn.datasets import simulate_array_from_presentations\nfrom compmemlearn.analyses import fast_rpl\n\nfitted_parameters = Dict.empty(\n    key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(lohnas_result.x)):\n    fitted_parameters[free_parameters[j]] = lohnas_result.x[j]\n\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nexperiment_count = 1000\nnew_sim_array = simulate_array_from_presentations(init_cmr, fitted_parameters, presentations[list_types>=1], experiment_count)\n\nimport numpy.matlib\n\nresult = fast_rpl(np.matlib.repmat(presentations[list_types>=1], experiment_count, 1), new_sim_array)\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\n[0.3343702  0.60605876 0.58105782 0.58587598 0.58480437]\n\n\n\nimport matplotlib.pyplot as plt\n\nfit_sources = ['fit_to_full_dataset']\nfit_rpls = [binned]\n\nfor i in range(len(fit_sources)):\n    plt.plot(fit_rpls[i], label=fit_sources[i])\n\nresult = fast_rpl(presentations[list_types>=1], trials[list_types>=1])\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nplt.plot(binned, label='data')\nlags = ['N/A', '0', '1-2', '3-5', '6-8']\nplt.xticks(np.arange(len(lags)), lags)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x12580e20ac0>\n\n\n\n\n\n\n\nDifferential Encoding\ntrials, events, list_length, presentations, list_types, rep_data, subjects = prepare_lohnas2014_data(\n    '../../data/repFR.mat')\n\nfree_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n    'choice_sensitivity',\n    'delay_drift_rate',\n    'mfc_familiarity_scale',\n    'mcf_familiarity_scale',\n    'drift_familiarity_scale',\n)\n\nfrom scipy.optimize import differential_evolution\nimport numpy as np\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nbounds = (\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub)\n)\n\n# cost function to be minimized\n# ours scales inversely with the probability that the data could have been \n# generated using the specified parameters and our model\ncost_function = lohnas_objective_function(\n    trials[list_types == 4], \n    presentations[list_types == 4], \n    init_cmr,\n    {'sampling_rule': 0}, \n    free_parameters)\n\nlohnas_result = differential_evolution(cost_function, bounds, disp=True)\nprint(lohnas_result)\n\ndifferential_evolution step 1: f(x)= 22294.2\ndifferential_evolution step 2: f(x)= 22294.2\ndifferential_evolution step 3: f(x)= 19887.7\ndifferential_evolution step 4: f(x)= 19618.8\ndifferential_evolution step 5: f(x)= 19618.8\ndifferential_evolution step 6: f(x)= 19618.8\ndifferential_evolution step 7: f(x)= 19618.8\ndifferential_evolution step 8: f(x)= 19347.4\ndifferential_evolution step 9: f(x)= 18814.7\ndifferential_evolution step 10: f(x)= 18814.7\ndifferential_evolution step 11: f(x)= 18139.3\ndifferential_evolution step 12: f(x)= 18139.3\ndifferential_evolution step 13: f(x)= 18139.3\ndifferential_evolution step 14: f(x)= 18139.3\ndifferential_evolution step 15: f(x)= 17855.5\ndifferential_evolution step 16: f(x)= 17827.9\ndifferential_evolution step 17: f(x)= 17827.9\ndifferential_evolution step 18: f(x)= 17655\ndifferential_evolution step 19: f(x)= 17655\ndifferential_evolution step 20: f(x)= 17655\ndifferential_evolution step 21: f(x)= 17538.4\ndifferential_evolution step 22: f(x)= 17538.4\ndifferential_evolution step 23: f(x)= 17538.4\ndifferential_evolution step 24: f(x)= 17538.4\ndifferential_evolution step 25: f(x)= 17538.4\ndifferential_evolution step 26: f(x)= 17520.4\ndifferential_evolution step 27: f(x)= 17520.4\ndifferential_evolution step 28: f(x)= 17471.6\ndifferential_evolution step 29: f(x)= 17471.6\ndifferential_evolution step 30: f(x)= 17465.5\ndifferential_evolution step 31: f(x)= 17462.6\ndifferential_evolution step 32: f(x)= 17462.6\ndifferential_evolution step 33: f(x)= 17429.5\ndifferential_evolution step 34: f(x)= 17429.5\ndifferential_evolution step 35: f(x)= 17429.5\ndifferential_evolution step 36: f(x)= 17429.5\n     fun: 17152.738785919926\n     jac: array([ 8.36007526e-01,  2.67027645e-01, -1.85973476e+00,  6.17073965e+00,\n        4.68331564e+01, -1.02118065e+00,  2.87400328e-02,  5.11499821e-01,\n        8.13815859e-01, -1.51376298e+00, -1.09866961e-01,  4.61295710e-01,\n       -6.80302037e-01, -7.06495485e-01, -1.57826434e+01])\n message: 'Optimization terminated successfully.'\n    nfev: 14677\n     nit: 36\n success: True\n       x: array([8.45399329e-01, 8.94309837e-02, 9.63793630e-01, 2.43252487e-02,\n       2.22044605e-16, 5.95712778e-01, 3.25658220e+00, 2.53201152e-01,\n       2.14029202e-02, 1.06789857e-01, 1.22711302e+00, 9.29194808e-01,\n       4.90150172e-01, 4.58480657e-01, 1.00000000e+00])\n\n\nfrom compmemlearn.datasets import simulate_array_from_presentations\nfrom compmemlearn.analyses import fast_rpl\n\nfitted_parameters = Dict.empty(\n    key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(lohnas_result.x)):\n    fitted_parameters[free_parameters[j]] = lohnas_result.x[j]\n\nfitted_parameters['sampling_rule'] = 0\nfitted_parameters['mfc_familiarity_scale'] = 0\nfitted_parameters['mcf_familiarity_scale'] = 0\nfitted_parameters['drift_familiarity_scale'] = 0\n\nexperiment_count = 1000\nnew_sim_array = simulate_array_from_presentations(init_cmr, fitted_parameters, presentations[list_types==4], experiment_count)\n\nimport numpy.matlib\n\nresult = fast_rpl(np.matlib.repmat(presentations[list_types==4], experiment_count, 1), new_sim_array)\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nprint(binned)\n\n[0.37035927 0.48302143 0.52116429 0.54002976 0.54700595]\n\n\n\nimport matplotlib.pyplot as plt\n\nfit_sources = ['lohnas_4']\nfit_rpls = [binned]\n\nfor i in range(len(fit_sources)):\n    plt.plot(fit_rpls[i], label=fit_sources[i])\n\nresult = fast_rpl(presentations[list_types==4], trials[list_types==4])\nbinned = np.zeros(5)\nbinned[0] = result[0]\nbinned[1] = result[1]\nbinned[2] = (result[2] + result[3])/2\nbinned[3] = (result[4] + result[5] + result[6])/3\nbinned[4] = (result[7] + result[8] + result[9])/3\nplt.plot(binned, label='data')\nlags = ['N/A', '0', '1-2', '3-5', '6-8']\nplt.xticks(np.arange(len(lags)), lags)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x1258219d0d0>"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#initial-fitting-and-memory-visualization",
    "href": "library\\model_analysis\\Model_Characterization.html#initial-fitting-and-memory-visualization",
    "title": "compmemlearn",
    "section": "Initial Fitting and Memory Visualization",
    "text": "InstanceCMR\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\nicmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n)\n\nicmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n]\n\n@njit(fastmath=True, nogil=True)\ndef init_icmr(item_count, presentation_count, parameters):\n    return Instance_CMR(item_count, presentation_count, parameters)\n\ncondition = 4\nselection = list_types == condition\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_icmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1}, \n    icmr_free_parameters)\n\nicmr_result = differential_evolution(cost_function, icmr_bounds, disp=True)\nicmr_result\n\ndifferential_evolution step 1: f(x)= 33720.1\ndifferential_evolution step 2: f(x)= 30743.7\ndifferential_evolution step 3: f(x)= 30743.7\ndifferential_evolution step 4: f(x)= 23444.6\ndifferential_evolution step 5: f(x)= 19831.3\ndifferential_evolution step 6: f(x)= 19400.9\ndifferential_evolution step 7: f(x)= 19400.9\ndifferential_evolution step 8: f(x)= 18531.7\ndifferential_evolution step 9: f(x)= 18285.1\ndifferential_evolution step 10: f(x)= 18274.3\ndifferential_evolution step 11: f(x)= 18194.7\ndifferential_evolution step 12: f(x)= 18125.7\ndifferential_evolution step 13: f(x)= 18125.7\ndifferential_evolution step 14: f(x)= 17241\ndifferential_evolution step 15: f(x)= 17241\ndifferential_evolution step 16: f(x)= 17241\ndifferential_evolution step 17: f(x)= 17241\ndifferential_evolution step 18: f(x)= 16361\ndifferential_evolution step 19: f(x)= 16361\ndifferential_evolution step 20: f(x)= 16361\ndifferential_evolution step 21: f(x)= 16361\ndifferential_evolution step 22: f(x)= 16361\ndifferential_evolution step 23: f(x)= 16361\ndifferential_evolution step 24: f(x)= 16361\ndifferential_evolution step 25: f(x)= 16361\ndifferential_evolution step 26: f(x)= 16361\ndifferential_evolution step 27: f(x)= 16361\ndifferential_evolution step 28: f(x)= 16200.5\ndifferential_evolution step 29: f(x)= 15919.8\ndifferential_evolution step 30: f(x)= 15919.8\ndifferential_evolution step 31: f(x)= 15919.8\ndifferential_evolution step 32: f(x)= 15919.8\ndifferential_evolution step 33: f(x)= 15919.8\ndifferential_evolution step 34: f(x)= 15919.8\ndifferential_evolution step 35: f(x)= 15919.8\ndifferential_evolution step 36: f(x)= 15919.8\ndifferential_evolution step 37: f(x)= 15906.9\ndifferential_evolution step 38: f(x)= 15906.9\ndifferential_evolution step 39: f(x)= 15873.1\ndifferential_evolution step 40: f(x)= 15873.1\ndifferential_evolution step 41: f(x)= 15873.1\ndifferential_evolution step 42: f(x)= 15855\ndifferential_evolution step 43: f(x)= 15855\ndifferential_evolution step 44: f(x)= 15840\ndifferential_evolution step 45: f(x)= 15840\ndifferential_evolution step 46: f(x)= 15840\ndifferential_evolution step 47: f(x)= 15840\ndifferential_evolution step 48: f(x)= 15840\ndifferential_evolution step 49: f(x)= 15840\ndifferential_evolution step 50: f(x)= 15840\ndifferential_evolution step 51: f(x)= 15790.8\ndifferential_evolution step 52: f(x)= 15790.8\ndifferential_evolution step 53: f(x)= 15790.8\ndifferential_evolution step 54: f(x)= 15790.8\ndifferential_evolution step 55: f(x)= 15790.8\ndifferential_evolution step 56: f(x)= 15790.8\ndifferential_evolution step 57: f(x)= 15790.8\ndifferential_evolution step 58: f(x)= 15790.8\ndifferential_evolution step 59: f(x)= 15790.8\ndifferential_evolution step 60: f(x)= 15790.8\ndifferential_evolution step 61: f(x)= 15789.2\n\n\n     fun: 15639.52630931897\n     jac: array([-4.40741130e-01, -2.41743692e-01, -7.29232848e-01, -5.62577043e+00,\n       -6.25277604e+00,  1.09394022e+00, -5.82076613e-03, -2.20279618e-01,\n       -2.04709067e+00, -3.06499715e-01, -4.07453629e-02, -1.16415321e-02])\n message: 'Optimization terminated successfully.'\n    nfev: 15112\n     nit: 61\n success: True\n       x: array([0.81354362, 0.13186679, 0.92156226, 0.01638355, 1.        ,\n       0.51789751, 2.51054862, 1.07486145, 0.02249255, 0.10414492,\n       2.10982446, 0.96461678])\n\n\n\nexperiment_count = 1000\n\n# static_icmr_fit_result = np.array([8.32809463e-01, 4.85458609e-02, 9.61140398e-01, 1.19908530e-03,\n#        1.31621512e-01, 2.45449928e-01, 3.33484776e+00, 2.09892365e+01,\n#        2.15080187e-02, 1.06720819e-01, 1.32169006e+00, 9.94120769e-01])\n\n# icmr_fitted_parameters = Dict.empty(\n#         key_type=types.unicode_type, value_type=types.float64)\n# for j in range(len(static_icmr_fit_result)):\n#     icmr_fitted_parameters[icmr_free_parameters[j]] =static_icmr_fit_result.x[j]\n\nicmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(icmr_result.x)):\n    icmr_fitted_parameters[icmr_free_parameters[j]] =icmr_result.x[j]\n    \nicmr_fitted_parameters['choice_sensitivity'] = 1\nicmr_fitted_parameters['feature_sensitivity'] = 1\n\nmodel = Instance_CMR(list_length, list_length, icmr_fitted_parameters)\nsimulate_df(model, 1)\n\nicmr_memory_heatmap(model, just_experimental=True, just_context=False)\n\nitem_count = np.max(presentations[0])+1\nmodel = Instance_CMR(item_count, list_length, icmr_fitted_parameters)\nmodel.experience(model.items[presentations[0]])\nicmr_memory_heatmap(model, just_experimental=True, just_context=False)\nprint(presentations[0])\n\n\n\n\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 11 12 13 14 15 16  9 17 18 19 18 20\n 21 22 19 23 24 25 21 26 27 23 28 29 30 31 32 33]\n\n\n\n\nTR-CMR\nA cell to reload the model in case I’ve updated the specification:\n\n# remember to run nbdev_build_lib first! \n\n!nbdev_build_lib\nimport compmemlearn\nimport importlib\nimportlib.reload(compmemlearn.models)\nfrom compmemlearn.models import Trace_Reinstatement_CMR, Instance_CMR\n\nConverted Alternative_Contiguity.ipynb.\nConverted Conditional_Stop_Probability.ipynb.\nConverted Lag_Contiguity_Effect.ipynb.\nConverted Lag_Contiguity_with_Repetition_Data.ipynb.\nConverted Measuring_Repetition_Effects.ipynb.\nConverted Probability_of_First_Recall.ipynb.\nConverted Probability_of_First_Recall_in_Repetition_Data.ipynb.\nConverted Recall_Probability_by_Spacing.ipynb.\nConverted Serial_Position_Effect.ipynb.\nConverted Serial_Position_Effect_in_Repetition_Datasets.ipynb.\nConverted Shared_Contiguity.ipynb.\nConverted ClairExpt6.ipynb.\nConverted Data_Simulation.ipynb.\nConverted HowaKaha05.ipynb.\nConverted Lohnas2014.ipynb.\nConverted Murdock1962.ipynb.\nConverted MurdockOkada1970.ipynb.\nConverted PEERS.ipynb.\nConverted index.ipynb.\nConverted Alternative_Contiguity_Tracing.ipynb.\nConverted Contiguity_Tracing.ipynb.\nConverted Data_Likelihood_Under_Model.ipynb.\nConverted Model_Characterization.ipynb.\nConverted Model_Visualization.ipynb.\nConverted Classic_CMR.ipynb.\nConverted Instance_CMR.ipynb.\nConverted Noisy_CMR.ipynb.\nConverted Scalar_CMR.ipynb.\nConverted Trace_Reinstatement_CMR.ipynb.\n\n\nlb = np.finfo(float).eps\nub = 1-np.finfo(float).eps\n\ntrcmr_free_parameters = (\n    'encoding_drift_rate',\n    'start_drift_rate',\n    'recall_drift_rate',\n    'shared_support',\n    'item_support',\n    'learning_rate',\n    'primacy_scale',\n    'primacy_decay',\n    'stop_probability_scale',\n    'stop_probability_growth',\n#    'choice_sensitivity',\n    'context_sensitivity',\n#    'feature_sensitivity'\n    'delay_drift_rate',\n    'feature_drift_rate',\n#    'context_reinstatement'\n)\n\ntrcmr_bounds = [\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, ub),\n    (lb, 100),\n    (lb, 100),\n    (lb, ub),\n    (lb, 10),\n    (lb, 10),\n#    (lb, 10),\n#    (lb, 10)\n    (lb, ub),\n    (lb, ub),\n#    (lb, 10)\n]\n\n@njit(fastmath=True, nogil=True)\ndef init_trcmr(item_count, presentation_count, parameters):\n    return Trace_Reinstatement_CMR(item_count, presentation_count, parameters)\n\ncondition = 4\nselection = list_types == condition\ncost_function = lohnas_objective_function(\n    trials[selection], \n    presentations[selection],\n    init_trcmr,\n    {'choice_sensitivity': 1, 'feature_sensitivity': 1, 'context_reinstatement': 0}, \n    trcmr_free_parameters)\n\n# will throw error if i haven't already fitted\n#print(init_trcmr(20, 20, trcmr_fitted_parameters).items[0])\n\n#TODO: When holding variable constant, 1) comment out the variable in free parameters, 2) comment out its bounds, and 3) add constant variable to the dict argument in cost_function, and 4) make sure initial simulation code also sets constant parameter's value (or you won't get far)\n\ntrcmr_result = differential_evolution(cost_function, trcmr_bounds, disp=True)\ntrcmr_result\n\ndifferential_evolution step 1: f(x)= 20113.2\ndifferential_evolution step 2: f(x)= 20113.2\ndifferential_evolution step 3: f(x)= 18436.3\ndifferential_evolution step 4: f(x)= 18436.3\ndifferential_evolution step 5: f(x)= 18436.3\ndifferential_evolution step 6: f(x)= 18436.3\ndifferential_evolution step 7: f(x)= 18287.8\ndifferential_evolution step 8: f(x)= 18287.8\ndifferential_evolution step 9: f(x)= 18006.6\ndifferential_evolution step 10: f(x)= 18006.6\ndifferential_evolution step 11: f(x)= 18006.6\ndifferential_evolution step 12: f(x)= 17808.6\ndifferential_evolution step 13: f(x)= 17808.6\ndifferential_evolution step 14: f(x)= 17808.6\ndifferential_evolution step 15: f(x)= 17808.6\ndifferential_evolution step 16: f(x)= 16763.6\ndifferential_evolution step 17: f(x)= 16763.6\ndifferential_evolution step 18: f(x)= 16658.6\ndifferential_evolution step 19: f(x)= 16658.6\ndifferential_evolution step 20: f(x)= 16658.6\ndifferential_evolution step 21: f(x)= 16658.6\ndifferential_evolution step 22: f(x)= 16658.6\ndifferential_evolution step 23: f(x)= 16658.6\ndifferential_evolution step 24: f(x)= 16658.6\ndifferential_evolution step 25: f(x)= 16658.6\ndifferential_evolution step 26: f(x)= 16658.6\ndifferential_evolution step 27: f(x)= 16600.2\ndifferential_evolution step 28: f(x)= 16600.2\ndifferential_evolution step 29: f(x)= 16162.4\ndifferential_evolution step 30: f(x)= 16162.4\ndifferential_evolution step 31: f(x)= 16162.4\ndifferential_evolution step 32: f(x)= 16162.4\ndifferential_evolution step 33: f(x)= 16067.9\ndifferential_evolution step 34: f(x)= 15965.1\ndifferential_evolution step 35: f(x)= 15965.1\ndifferential_evolution step 36: f(x)= 15965.1\ndifferential_evolution step 37: f(x)= 15933.2\ndifferential_evolution step 38: f(x)= 15933.2\ndifferential_evolution step 39: f(x)= 15933.2\ndifferential_evolution step 40: f(x)= 15867.8\ndifferential_evolution step 41: f(x)= 15867.8\ndifferential_evolution step 42: f(x)= 15867.8\ndifferential_evolution step 43: f(x)= 15867.8\ndifferential_evolution step 44: f(x)= 15867.8\ndifferential_evolution step 45: f(x)= 15867.8\ndifferential_evolution step 46: f(x)= 15867.8\ndifferential_evolution step 47: f(x)= 15834.9\ndifferential_evolution step 48: f(x)= 15834.9\ndifferential_evolution step 49: f(x)= 15834.9\ndifferential_evolution step 50: f(x)= 15834.9\ndifferential_evolution step 51: f(x)= 15781.9\ndifferential_evolution step 52: f(x)= 15775.5\ndifferential_evolution step 53: f(x)= 15775.5\ndifferential_evolution step 54: f(x)= 15775.5\n\n\n     fun: 15699.843571126987\n     jac: array([-1.44409568,  0.10204531, -0.67611836,  2.73339538, -9.39544402,\n       -0.16279955,  0.14388206,  0.        ,  1.54122972, -0.43346518,\n       -0.0165528 , -0.82709448,  0.79999154])\n message: 'Optimization terminated successfully.'\n    nfev: 15359\n     nit: 54\n success: True\n       x: array([8.39314620e-01, 1.63618629e-01, 9.23936537e-01, 1.55520938e-02,\n       1.00000000e+00, 5.23221127e-01, 2.35838577e+00, 9.93512212e+01,\n       2.25120491e-02, 1.04097390e-01, 2.06938199e+00, 9.93062277e-01,\n       1.54392061e-01])\n\n\n\nexperiment_count = 1000\n\n# static_trcmr_result = np.array([8.38888699e-01, 5.11712122e-04, 9.42814559e-01, 6.43556899e-03, # temporarily set to 0\n#        6.45214269e-01, 4.81647177e-01, 4.00131827e+00, 1.98252428e+00,\n#        2.83758432e-02, 9.60298353e-02, 1.58539149e+00, 9.99999887e-01,\n#        3.61675064e-01])\n\n# trcmr_fitted_parameters = Dict.empty(\n#         key_type=types.unicode_type, value_type=types.float64)\n# for j in range(len(static_trcmr_result)):\n#     trcmr_fitted_parameters[trcmr_free_parameters[j]] = static_trcmr_result[j]\n\ntrcmr_fitted_parameters = Dict.empty(\n        key_type=types.unicode_type, value_type=types.float64)\nfor j in range(len(trcmr_result.x)):\n    trcmr_fitted_parameters[trcmr_free_parameters[j]] = trcmr_result.x[j]\n    \ntrcmr_fitted_parameters['choice_sensitivity'] = 1\ntrcmr_fitted_parameters['feature_sensitivity'] = 1\n# trcmr_fitted_parameters['shared_support'] = 0\ntrcmr_fitted_parameters['context_reinstatement'] = 0\n#trcmr_fitted_parameters['feature_drift_rate'] = 1\n\nprint(Trace_Reinstatement_CMR(20, 20, trcmr_fitted_parameters).items[0]) \n\nitem_count = np.max(presentations[0])+1\nmodel = Trace_Reinstatement_CMR(item_count, list_length, trcmr_fitted_parameters)\nmodel.experience(model.items[presentations[0]])\nicmr_memory_heatmap(model, just_experimental=True, just_context=False)\nprint(presentations[0])\n\nmodel = Trace_Reinstatement_CMR(list_length, list_length, trcmr_fitted_parameters)\nsimulate_df(model, 1)\nicmr_memory_heatmap(model, just_experimental=True, just_context=False)\n\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 11 12 13 14 15 16  9 17 18 19 18 20\n 21 22 19 23 24 25 21 26 27 23 28 29 30 31 32 33]"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#benchmark-evaluation",
    "href": "library\\model_analysis\\Model_Characterization.html#benchmark-evaluation",
    "title": "compmemlearn",
    "section": "Benchmark Evaluation",
    "text": "InstanceCMR\n\nmodel_name = 'InstanceCMR'\n\n# identify first recalled item for each trial\nfirst_recalls = np.zeros(len(presentations), dtype=int)\nfor i in range(len(presentations)):\n    first_recalls[i] = presentations[i][trials[i, 0]-1]+1\n\nicmr_trials = simulate_array_from_presentations(\n    init_icmr, icmr_fitted_parameters, presentations[list_types==condition], experiment_count, first_recalls[list_types==condition])\nicmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\nicmr_spc = flex_mixed_spc(icmr_trials, icmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(icmr_spc)), icmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\nicmr_pfr = flex_mixed_pfr(icmr_trials, icmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(icmr_pfr)), icmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\nicmr_crp = flex_mixed_crp(icmr_trials, icmr_presentations)\nicmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(icmr_crp)), icmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\nicmr_rpl = fast_rpl(\n    icmr_trials, icmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = icmr_rpl[0]\nbinned[1] = icmr_rpl[1]\nbinned[2] = (icmr_rpl[2] + icmr_rpl[3])/2\nbinned[3] = (icmr_rpl[4] + icmr_rpl[5] + icmr_rpl[6])/3\nbinned[4] = (icmr_rpl[7] + icmr_rpl[8] + icmr_rpl[9])/3\nicmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(icmr_rpl)), icmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\nicmr_altcrp = alternative_contiguity(\n    icmr_trials, icmr_presentations, 6, 2)\nicmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), icmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), icmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n\n\n\n\nmodel_name = 'InstanceCMR'\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10), sharey='row')\n\n# alt contiguity (data)\nlag_range = len(presentations[0])-1\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\ncmr_altcrp = alternative_contiguity(\n    icmr_trials, icmr_presentations, 6, 2)\ncmr_altcrp[:, lag_range] = np.nan\n\naxes.plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4]-data_altcrp[1][lag_range-3:lag_range+4], label='Data')\naxes.plot(np.arange(7), cmr_altcrp[0][lag_range-3:lag_range+4]-cmr_altcrp[1][lag_range-3:lag_range+4], label=model_name)\naxes.set_xticks(np.arange(7))\naxes.set_xticklabels(np.arange(7) - 3)\naxes.set_title('Positional Contiguity Difference')\n\naxes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x22a9946fa90>\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\nmodel_name = 'Trace Reinstatement CMR'\n\nfirst_recalls = np.zeros(len(presentations), dtype=int)\nfor i in range(len(presentations)):\n    first_recalls[i] = presentations[i][trials[i, 0]-1]+1\n    \ntrcmr_trials = simulate_array_from_presentations(\n    init_trcmr, trcmr_fitted_parameters, presentations[list_types==condition], experiment_count, first_recalls[list_types==condition])\ntrcmr_presentations = np.matlib.repmat(presentations[list_types==condition], experiment_count, 1)\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), sharey='row')\n\n# spc\ndata_spc= flex_mixed_spc(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_spc = flex_mixed_spc(trcmr_trials, trcmr_presentations)\naxes[0, 0].plot(np.arange(len(data_spc)), data_spc, label='Data')\naxes[0, 0].plot(np.arange(len(trcmr_spc)), trcmr_spc, label=model_name)\naxes[0, 0].set_title('SPC')\n\n# pfr\ndata_pfr = flex_mixed_pfr(trials[list_types==condition], presentations[list_types==condition])\ntrcmr_pfr = flex_mixed_pfr(trcmr_trials, trcmr_presentations)\naxes[1, 1].plot(np.arange(len(data_pfr)), data_pfr, label='Data')\naxes[1, 1].plot(np.arange(len(trcmr_pfr)), trcmr_pfr, label=model_name)\naxes[1, 1].set_title('PFR')\n\n# crp\nlag_range = len(presentations[0])-1\ndata_crp= flex_mixed_crp(trials[list_types==condition], presentations[list_types==condition])\ndata_crp[lag_range] = np.nan\ntrcmr_crp = flex_mixed_crp(trcmr_trials, trcmr_presentations)\ntrcmr_crp[lag_range] = np.nan\naxes[1, 0].plot(np.arange(len(data_crp)), data_crp, label='Data')\naxes[1, 0].plot(np.arange(len(trcmr_crp)), trcmr_crp, label=model_name)\naxes[1, 0].set_xticks(np.arange(0, len(data_crp), 4))\naxes[1, 0].set_xticklabels(np.arange(0, len(data_crp), 4) - lag_range)\naxes[1, 0].set_title('CRP')\n\n# rpl\ndata_rpl = fast_rpl(\n    trials[list_types==condition], presentations[list_types==condition], max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = data_rpl[0]\nbinned[1] = data_rpl[1]\nbinned[2] = (data_rpl[2] + data_rpl[3])/2\nbinned[3] = (data_rpl[4] + data_rpl[5] + data_rpl[6])/3\nbinned[4] = (data_rpl[7] + data_rpl[8] + data_rpl[9])/3\ndata_rpl = binned.copy()\n\ntrcmr_rpl = fast_rpl(\n    trcmr_trials, trcmr_presentations, max_lag=8)\nbinned = np.zeros(5)\nbinned[0] = trcmr_rpl[0]\nbinned[1] = trcmr_rpl[1]\nbinned[2] = (trcmr_rpl[2] + trcmr_rpl[3])/2\nbinned[3] = (trcmr_rpl[4] + trcmr_rpl[5] + trcmr_rpl[6])/3\nbinned[4] = (trcmr_rpl[7] + trcmr_rpl[8] + trcmr_rpl[9])/3\ntrcmr_rpl = binned.copy()\n\naxes[0, 1].plot(np.arange(len(data_rpl)), data_rpl, label='Data')\naxes[0, 1].plot(np.arange(len(trcmr_rpl)), trcmr_rpl, label=model_name)\naxes[0, 1].set_title('Recall Probability by Lag')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\naxes[2, 0].plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 0].plot(np.arange(7), data_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 0].set_xticks(np.arange(7))\naxes[2, 0].set_xticklabels(np.arange(7) - 3)\naxes[2, 0].set_title('Repetition Contiguity -- Data')\n\ntrcmr_altcrp = alternative_contiguity(\n    trcmr_trials, trcmr_presentations, 6, 2)\ntrcmr_altcrp[:, lag_range] = np.nan\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[0][lag_range-3:lag_range+4], label='First Presentation')\naxes[2, 1].plot(np.arange(7), trcmr_altcrp[1][lag_range-3:lag_range+4], label='Second Presentation')\naxes[2, 1].set_xticks(np.arange(7))\naxes[2, 1].set_xticklabels(np.arange(7) - 3)\naxes[2, 1].set_title('Repetition Contiguity -- ' + model_name)\n\naxes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\naxes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n\n\n\n\n\nmodel_name = 'Trace Reinstatement CMR'\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10), sharey='row')\n\n# alt contiguity (data)\ndata_altcrp = alternative_contiguity(\n    trials[list_types==condition], presentations[list_types==condition], 6, 2)\ndata_altcrp[:, lag_range] = np.nan\ncmr_altcrp = alternative_contiguity(\n    trcmr_trials, trcmr_presentations, 6, 2)\ncmr_altcrp[:, lag_range] = np.nan\n\naxes.plot(np.arange(7), data_altcrp[0][lag_range-3:lag_range+4]-data_altcrp[1][lag_range-3:lag_range+4], label='Data')\naxes.plot(np.arange(7), cmr_altcrp[0][lag_range-3:lag_range+4]-cmr_altcrp[1][lag_range-3:lag_range+4], label=model_name)\naxes.set_xticks(np.arange(7))\naxes.set_xticklabels(np.arange(7) - 3)\naxes.set_title('Positional Contiguity Difference')\n\naxes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n<matplotlib.legend.Legend at 0x22a9ee6a6d0>\n\n\n\n\n\n\n\nTrace-Reinstatement CMR"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#latent-mfc-mcf-mfc---mcf-mff",
    "href": "library\\model_analysis\\Model_Characterization.html#latent-mfc-mcf-mfc---mcf-mff",
    "title": "compmemlearn",
    "section": "Latent MFC, MCF, MFC -> MCF (MFF)",
    "text": "This time, we’ll do extra processing to the model’s internal representations to build a more accurate picture of the proximate input into the next step of model processing. For MFC, this mainly means adding a normalization step to processing. For MCF, we assume stop_probability is 0 and no items have been recalled and compute what would be the corresponding output from model.outcome_probabilities. This means scaling retrieved activations with the exponent model.choice_sensitivity and enforcing the sum of entries in the output vector to the value 1.\nFurthermore, we wish to characterize how MFC and MCF interact to determine transitions during simulated free recall. This is found by passing each relevant item feature representation through MFC to retrieve contextual associations and then further passing these retrieved contextual associations through MCF to obtain correspond item feature associations. Though in practice the extent of the link between MFC and MCF is mediated by the contextual drift rate parameter, this provides insight into the directional effect of recalling an item on the probability distribution for the following one. We’ll call this mapping MFF.\n\nInstanceCMR\n\nmodel = Instance_CMR(list_length, list_length, icmr_fitted_parameters)\nsimulate_df(model, 1)\n\nlatent_mfc, latent_mcf, latent_mff = latent_mfc_mcf_mff(model, model.items)\n\nmfc_heatmap(latent_mfc)\nmfc_heatmap(latent_mcf)\nmfc_heatmap(latent_mff)\n\n\n\n\n\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\nmodel = Trace_Reinstatement_CMR(list_length, list_length, trcmr_fitted_parameters)\nsimulate_df(model, 1)\n\nlatent_mfc, latent_mcf, latent_mff = latent_mfc_mcf_mff(model, model.recall_items)\n\nmfc_heatmap(latent_mfc)\nmfc_heatmap(latent_mcf)\nmfc_heatmap(latent_mff)"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#memory-connectivity-by-lag",
    "href": "library\\model_analysis\\Model_Characterization.html#memory-connectivity-by-lag",
    "title": "compmemlearn",
    "section": "Memory Connectivity by Lag",
    "text": "InstanceCMR\n\n# configure parameters\nmodel_class = Instance_CMR\nmodel_parameters = icmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [1, 2, 3, 4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mff_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n        lag_range = item_count+1\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            #TODO: Update for Classic CMR\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n        else:\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities[0] += mixed_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities[0] += mixed_connectivity_by_lag(mcf_connections, presentation)\n        mff_alternative_connectivities[0] += mixed_connectivity_by_lag(mff_connections, presentation)\n\n        #mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)[:, lag_range-3:lag_range+4]\n        #mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)[:, lag_range-3:lag_range+4]\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mff_alternative_connectivity = mff_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n    mff_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5), sharey=True)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    #axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    #axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[1].set_title('MFC')\n\n    # same for MFF\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[0])), mff_alternative_connectivity[0], label='First Presentation')\n    #axes[2].plot(np.arange(len(mff_alternative_connectivity[1])), mff_alternative_connectivity[1], label='Second Presentation')\n    axes[2].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[2].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[2].set_title('MFF')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\n# configure parameters\nmodel_class = Trace_Reinstatement_CMR\nmodel_parameters = trcmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [1, 2, 3, 4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mff_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n        lag_range = item_count+1\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            #TODO: Update for Classic CMR\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n        else:\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.recall_items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities[0] += mixed_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities[0] += mixed_connectivity_by_lag(mcf_connections, presentation)\n        mff_alternative_connectivities[0] += mixed_connectivity_by_lag(mff_connections, presentation)\n\n        #mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)[:, lag_range-3:lag_range+4]\n        #mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)[:, lag_range-3:lag_range+4]\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mff_alternative_connectivity = mff_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n    mff_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5), sharey=True)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    #axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    #axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[1].set_title('MFC')\n\n    # same for MFF\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[0])), mff_alternative_connectivity[0], label='First Presentation')\n    #axes[2].plot(np.arange(len(mff_alternative_connectivity[1])), mff_alternative_connectivity[1], label='Second Presentation')\n    axes[2].set_xticks(np.arange(0, global_lag_range * 2 + 1, 2))\n    axes[2].set_xticklabels(np.arange(0, global_lag_range * 2 + 1, 2) - global_lag_range)\n    axes[2].set_title('MFF')"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#pure-list-parameter-shifting",
    "href": "library\\model_analysis\\Model_Characterization.html#pure-list-parameter-shifting",
    "title": "compmemlearn",
    "section": "Pure List Parameter Shifting",
    "text": "InstanceCMR\n\nmax_lag = 13\n\nscore_ranges = {\n    'encoding_drift_rate': np.arange(.001, .99, .1),\n    'recall_drift_rate': np.arange(.001, .99, .1),\n    'shared_support': np.arange(.001, .005, .001),\n    'item_support': np.arange(.001, .01, .001),\n    'learning_rate': np.arange(.001, .99, .1),\n    'choice_sensitivity': np.arange(.001, 5, .5),\n}\n\nfor model_class in [Instance_CMR]:\n\n    if model_class.__name__ == 'Classic_CMR':\n        parameters = cmr_fitted_parameters\n    elif model_class.__name__ == 'Instance_CMR':\n        parameters = icmr_fitted_parameters\n    else:\n        parameters = trcmr_fitted_parameters\n\n    for varied_parameter in score_ranges.keys():\n        print(varied_parameter, parameters[varied_parameter])\n        crps = []\n        spcs = []\n        pfrs = []\n        mfc_connectivities = []\n        mcf_connectivities = []\n        mff_connectivities = []\n\n        for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n\n            # simulate data with this parameter value modified\n            sub_params = parameters.copy()\n            sub_params[varied_parameter] = parameter_value\n            model = model_class(list_length, list_length, sub_params)\n            simulation = simulate_array(model, 10000)\n\n            # accumulate spcs, crps, pfrs\n            spc = fast_spc(simulation, list_length)\n            spc = pd.DataFrame(\n                {'Study Position': np.arange(len(spc)), 'Recall Rate': spc, varied_parameter: parameter_value})\n            spcs.append(spc)\n\n            crp = fast_crp(simulation, list_length)\n            crp[list_length-1] = np.nan\n            crp = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': crp[list_length-max_lag-1:list_length+max_lag], varied_parameter: parameter_value})\n            crps.append(crp)\n\n            pfr = fast_pfr(simulation, list_length)\n            pfr = pd.DataFrame(\n                {'Study Position': np.arange(len(pfr)), 'First Recall Rate': pfr, varied_parameter: parameter_value})\n            pfrs.append(pfr)\n            \n            if model_class.__name__ == 'Classic_CMR':\n                #TODO: still have to update for CMR\n                mfc_connectivity = connectivity_by_lag(model.mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(model.mcf[1:-1, :], model.item_count)\n            else:\n                latent_mfc, latent_mcf, latent_mff = latent_mfc_mcf_mff(model, model.items)\n                mfc_connectivity = connectivity_by_lag(latent_mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(latent_mcf, model.item_count)\n                mff_connectivity = connectivity_by_lag(latent_mff, model.item_count)\n                \n\n            mfc_connectivity[model.item_count-1] = np.nan\n            mcf_connectivity[model.item_count-1] = np.nan\n            mfc_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mfc_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mcf_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mcf_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mff_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mff_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mfc_connectivities.append(mfc_connectivity)\n            mcf_connectivities.append(mcf_connectivity)\n            mff_connectivities.append(mff_connectivity)\n\n        # concatenate result into a single table\n        spc = pd.concat(spcs).reset_index()\n        crp = pd.concat(crps).reset_index()\n        pfr = pd.concat(pfrs).reset_index()\n        mfc_connectivity = pd.concat(mfc_connectivities).reset_index()\n        mcf_connectivity = pd.concat(mcf_connectivities).reset_index()\n        mff_connectivity = pd.concat(mff_connectivities).reset_index()\n\n        sns.set(style='darkgrid')\n        fig, axes = plt.subplots(6, 1, figsize=(10, 20), sharey=False)\n\n        sns.lineplot(ax=axes[0], data=spc, x='Study Position', y='Recall Rate', hue=varied_parameter, ci=None)\n        #axes[0].set_xlabel('Study Position')\n        #axes[0].set_ylabel('Probability Recall')\n        axes[0].set_title('SPC')\n        axes[0].legend(np.round(score_ranges[varied_parameter], 5), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n        sns.lineplot(ax=axes[1], data=pfr, x='Study Position', y='First Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[1].set_title('PFR')\n\n        filt_neg = f'{-max_lag} <= Lag < 0'\n        filt_pos = f'0 < Lag <= {max_lag}'\n        sns.lineplot(ax=axes[2], data=crp.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[2], data=crp.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[2].set_title('Lag-CRP')\n\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[3].set_title('MFC Lag-Connectivity')\n        \n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[4].set_title('MCF Lag-Connectivity')\n\n        sns.lineplot(ax=axes[5], data=mff_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[5], data=mff_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[5].set_title('MFF Lag-Connectivity')\n\n        plt.tight_layout(pad=2)\n\n        fig.suptitle(varied_parameter.replace('_', ' ').upper())\n        #plt.savefig('results/{}_{}.svg'.format(model.__name__, varied_parameter))\n        plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nencoding_drift_rate 0.8363715433354807\n\n\n100%|██████████| 10/10 [00:15<00:00,  1.59s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nrecall_drift_rate 0.9600175060232444\n\n\n100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n\n\n\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n\n\nshared_support 0.0008229999076557321\n\n\n100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n\n\n\n\n\n  0%|          | 0/9 [00:00<?, ?it/s]\n\n\nitem_support 0.09131365030360249\n\n\n100%|██████████| 9/9 [00:14<00:00,  1.57s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nlearning_rate 0.1915496572645179\n\n\n100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nchoice_sensitivity 1.0\n\n\n100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\nmax_lag = 13\n\nscore_ranges = {\n    'encoding_drift_rate': np.arange(.001, .99, .1),\n    'recall_drift_rate': np.arange(.001, .99, .1),\n    'shared_support': np.arange(.001, .005, .001),\n    'item_support': np.arange(.001, .01, .001),\n    'learning_rate': np.arange(.001, .99, .1),\n    'choice_sensitivity': np.arange(.001, 5, .5),\n}\n\nfor model_class in [Trace_Reinstatement_CMR]:\n\n    if model_class.__name__ == 'Classic_CMR':\n        parameters = cmr_fitted_parameters\n    elif model_class.__name__ == 'Instance_CMR':\n        parameters = icmr_fitted_parameters\n    else:\n        parameters = trcmr_fitted_parameters\n\n    for varied_parameter in score_ranges.keys():\n        print(varied_parameter, parameters[varied_parameter])\n        crps = []\n        spcs = []\n        pfrs = []\n        mfc_connectivities = []\n        mcf_connectivities = []\n        mff_connectivities = []\n\n        for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n\n            # simulate data with this parameter value modified\n            sub_params = parameters.copy()\n            sub_params[varied_parameter] = parameter_value\n            model = model_class(list_length, list_length, sub_params)\n            simulation = simulate_array(model, 10000)\n\n            # accumulate spcs, crps, pfrs\n            spc = fast_spc(simulation, list_length)\n            spc = pd.DataFrame(\n                {'Study Position': np.arange(len(spc)), 'Recall Rate': spc, varied_parameter: parameter_value})\n            spcs.append(spc)\n\n            crp = fast_crp(simulation, list_length)\n            crp[list_length-1] = np.nan\n            crp = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': crp[list_length-max_lag-1:list_length+max_lag], varied_parameter: parameter_value})\n            crps.append(crp)\n\n            pfr = fast_pfr(simulation, list_length)\n            pfr = pd.DataFrame(\n                {'Study Position': np.arange(len(pfr)), 'First Recall Rate': pfr, varied_parameter: parameter_value})\n            pfrs.append(pfr)\n            \n            if model_class.__name__ == 'Classic_CMR':\n                #TODO: still have to update for CMR\n                mfc_connectivity = connectivity_by_lag(model.mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(model.mcf[1:-1, :], model.item_count)\n            else:\n                latent_mfc, latent_mcf, latent_mff = latent_mfc_mcf_mff(model, model.recall_items)\n                mfc_connectivity = connectivity_by_lag(latent_mfc[:, 1:-1], model.item_count)\n                mcf_connectivity = connectivity_by_lag(latent_mcf, model.item_count)\n                mff_connectivity = connectivity_by_lag(latent_mff, model.item_count)\n                \n\n            mfc_connectivity[model.item_count-1] = np.nan\n            mcf_connectivity[model.item_count-1] = np.nan\n            mfc_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mfc_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mcf_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mcf_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mff_connectivity = pd.DataFrame(\n                {'Lag': np.arange(max_lag*2 + 1, dtype=int)-max_lag, 'Recall Rate': mff_connectivity[model.item_count-max_lag-1:model.item_count+max_lag], varied_parameter: parameter_value}\n            )\n            mfc_connectivities.append(mfc_connectivity)\n            mcf_connectivities.append(mcf_connectivity)\n            mff_connectivities.append(mff_connectivity)\n\n        # concatenate result into a single table\n        spc = pd.concat(spcs).reset_index()\n        crp = pd.concat(crps).reset_index()\n        pfr = pd.concat(pfrs).reset_index()\n        mfc_connectivity = pd.concat(mfc_connectivities).reset_index()\n        mcf_connectivity = pd.concat(mcf_connectivities).reset_index()\n        mff_connectivity = pd.concat(mff_connectivities).reset_index()\n\n        sns.set(style='darkgrid')\n        fig, axes = plt.subplots(6, 1, figsize=(10, 20), sharey=False)\n\n        sns.lineplot(ax=axes[0], data=spc, x='Study Position', y='Recall Rate', hue=varied_parameter, ci=None)\n        #axes[0].set_xlabel('Study Position')\n        #axes[0].set_ylabel('Probability Recall')\n        axes[0].set_title('SPC')\n        axes[0].legend(np.round(score_ranges[varied_parameter], 5), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n        sns.lineplot(ax=axes[1], data=pfr, x='Study Position', y='First Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[1].set_title('PFR')\n\n        filt_neg = f'{-max_lag} <= Lag < 0'\n        filt_pos = f'0 < Lag <= {max_lag}'\n        sns.lineplot(ax=axes[2], data=crp.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[2], data=crp.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[2].set_title('Lag-CRP')\n\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[3], data=mfc_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[3].set_title('MFC Lag-Connectivity')\n        \n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[4], data=mcf_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[4].set_title('MCF Lag-Connectivity')\n\n        sns.lineplot(ax=axes[5], data=mff_connectivity.query(filt_neg), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        sns.lineplot(ax=axes[5], data=mff_connectivity.query(filt_pos), x='Lag', y='Recall Rate', hue=varied_parameter, ci=None, legend=False)\n        axes[5].set_title('MFF Lag-Connectivity')\n\n        plt.tight_layout(pad=2)\n\n        fig.suptitle(varied_parameter.replace('_', ' ').upper())\n        #plt.savefig('results/{}_{}.svg'.format(model.__name__, varied_parameter))\n        plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nencoding_drift_rate 0.8015623810548206\n\n\n100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nrecall_drift_rate 0.959155432955042\n\n\n100%|██████████| 10/10 [00:16<00:00,  1.63s/it]\n\n\n\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n\n\nshared_support 0.008429792074646641\n\n\n100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n\n\n\n\n\n  0%|          | 0/9 [00:00<?, ?it/s]\n\n\nitem_support 0.7496270374441739\n\n\n100%|██████████| 9/9 [00:14<00:00,  1.61s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nlearning_rate 0.5079383408154622\n\n\n100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nchoice_sensitivity 1.0\n\n\n100%|██████████| 10/10 [00:16<00:00,  1.64s/it]"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#repetition-lag-connectivity-analysis",
    "href": "library\\model_analysis\\Model_Characterization.html#repetition-lag-connectivity-analysis",
    "title": "compmemlearn",
    "section": "Repetition Lag-Connectivity Analysis",
    "text": "InstanceCMR\n\n# configure parameters\nmodel_class = Instance_CMR\nmodel_parameters = icmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mff_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            #TODO: More classic_cmr stuff to update\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n            if trial_index == 0:\n                mfc_heatmap(model.mfc)\n                mfc_heatmap(model.mcf)\n                print(presentation)\n        else:\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.items)\n\n        # track alternative connectivity\n        mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)\n        mff_alternative_connectivities += alternative_connectivity_by_lag(mff_connections, presentation)\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mff_alternative_connectivity = mff_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n    mff_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # focus on +/- 3 lags\n    plotting_lag_range = 3\n    mfc_alternative_connectivity = mfc_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mcf_alternative_connectivity = mcf_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mff_alternative_connectivity = mff_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5), sharey=False)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[1].set_title('MFC')\n\n    # same for MFF\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[0])), mff_alternative_connectivity[0], label='First Presentation')\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[1])), mff_alternative_connectivity[1], label='Second Presentation')\n    axes[2].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[2].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[2].set_title('MFF')\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\n# configure parameters\nmodel_class = Trace_Reinstatement_CMR\nmodel_parameters = trcmr_fitted_parameters\n\n# track results\nglobal_lag_range = 39\n\nfor list_type in [4]:\n    mfc_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mcf_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n    mff_alternative_connectivities = np.zeros((2, global_lag_range * 2 + 1))\n\n    # loop through presentations\n    for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n        # simulate list study\n        item_count = np.max(presentation)+1\n        model = model_class(item_count, len(presentation), model_parameters)\n        model.experience(model.items[presentation])\n\n        # extract item connections\n        if model_class.__name__ == 'Classic_CMR':\n            #TODO: More classic_cmr stuff to update\n            mfc_connections = model.mfc[:, 1:-1]\n            mcf_connections = model.mcf[1:-1, :]\n            if trial_index == 0:\n                mfc_heatmap(model.mfc)\n                mfc_heatmap(model.mcf)\n                print(presentation)\n        else:\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.recall_items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n        # track alternative connectivity\n        mfc_alternative_connectivities += alternative_connectivity_by_lag(mfc_connections, presentation)\n        mcf_alternative_connectivities += alternative_connectivity_by_lag(mcf_connections, presentation)\n        mff_alternative_connectivities += alternative_connectivity_by_lag(mff_connections, presentation)\n\n    # reduce sum to mean\n    mfc_alternative_connectivity = mfc_alternative_connectivities / (trial_index+1)\n    mcf_alternative_connectivity = mcf_alternative_connectivities / (trial_index+1)\n    mff_alternative_connectivity = mff_alternative_connectivities / (trial_index+1)\n    mfc_alternative_connectivity[:, global_lag_range] = np.nan\n    mcf_alternative_connectivity[:, global_lag_range] = np.nan\n    mff_alternative_connectivity[:, global_lag_range] = np.nan\n\n    # focus on +/- 3 lags\n    plotting_lag_range = 3\n    mfc_alternative_connectivity = mfc_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mcf_alternative_connectivity = mcf_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n    mff_alternative_connectivity = mff_alternative_connectivity[\n        :, global_lag_range-plotting_lag_range:global_lag_range+plotting_lag_range+1]\n\n    # plot results\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5), sharey=False)\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[0])), mcf_alternative_connectivity[0], label='First Presentation')\n    axes[0].plot(np.arange(len(mcf_alternative_connectivity[1])), mcf_alternative_connectivity[1], label='Second Presentation')\n    axes[0].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[0].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[0].set_title('MCF')\n\n    # same for MFC\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[0])), mfc_alternative_connectivity[0], label='First Presentation')\n    axes[1].plot(np.arange(len(mfc_alternative_connectivity[1])), mfc_alternative_connectivity[1], label='Second Presentation')\n    axes[1].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[1].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[1].set_title('MFC')\n\n    # same for MFF\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[0])), mff_alternative_connectivity[0], label='First Presentation')\n    axes[2].plot(np.arange(len(mff_alternative_connectivity[1])), mff_alternative_connectivity[1], label='Second Presentation')\n    axes[2].set_xticks(np.arange(0, plotting_lag_range * 2 + 1, 2))\n    axes[2].set_xticklabels(np.arange(0, plotting_lag_range * 2 + 1, 2) - plotting_lag_range)\n    axes[2].set_title('MFF')"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#mixed-list-parameter-shifting",
    "href": "library\\model_analysis\\Model_Characterization.html#mixed-list-parameter-shifting",
    "title": "compmemlearn",
    "section": "Mixed List Parameter Shifting",
    "text": "Instance_CMR\n\n# configure parameters\nmodel_class = init_icmr\nmodel_name = 'Instance_CMR'\nparameters = icmr_fitted_parameters\nlist_type = 4\n\n# track results\nglobal_lag_range = list_length-1\ncrp_max_lag = 13\nrep_crp_max_lag = 3\nexperiment_count = 100\n\nscore_ranges = {\n    'encoding_drift_rate': np.arange(.001, .99, .1),\n    'recall_drift_rate': np.arange(.001, .99, .1),\n    'shared_support': np.arange(.001, .005, .001),\n    'item_support': np.arange(.001, .01, .001),\n    'learning_rate': np.arange(.001, .99, .1),\n    'choice_sensitivity': np.arange(.001, 5, .5),\n}\n\nfor varied_parameter in score_ranges.keys():\n\n    print(varied_parameter, parameters[varied_parameter])\n    spcs = []\n    pfrs = []\n    rpls = []\n    crps = []\n    rep_crps = []\n    lag_mcfs = []\n    lag_mfcs = []\n    lag_mffs = []\n    rep_lag_mcfs = []\n    rep_lag_mfcs = []\n    rep_lag_mffs = []\n\n    # loop through parameter values\n    for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n        # simulate data with this parameter value modified\n        model_parameters = parameters.copy()\n        model_parameters[varied_parameter] = parameter_value\n        model_trials = simulate_array_from_presentations(\n            model_class, model_parameters, presentations[list_types==list_type], experiment_count)\n        model_presentations = np.matlib.repmat(presentations[list_types==list_type], experiment_count, 1)\n\n        # spc\n        spc = flex_mixed_spc(model_trials, model_presentations)\n        spc = pd.DataFrame(\n                {'Study Position': np.arange(len(spc)), 'Recall Rate': spc, varied_parameter: parameter_value})\n        spcs.append(spc)\n\n        # pfr\n        pfr = flex_mixed_pfr(model_trials, model_presentations)\n        pfr = pd.DataFrame(\n            {'Study Position': np.arange(len(pfr)), 'First Recall Rate': pfr, varied_parameter: parameter_value})\n        pfrs.append(pfr)\n\n        # rpl\n        rpl = fast_rpl(\n            model_trials, model_presentations, max_lag=8)\n        binned = np.zeros(5)\n        binned[0] = rpl[0]\n        binned[1] = rpl[1]\n        binned[2] = (rpl[2] + rpl[3])/2\n        binned[3] = (rpl[4] + rpl[5] + rpl[6])/3\n        binned[4] = (rpl[7] + rpl[8] + rpl[9])/3\n        rpl = binned.copy()\n        rpl = pd.DataFrame(\n            {'Spacing': ['N/A', '0', '1-2', '3-5', '6-8'], 'Recall Rate': rpl, varied_parameter: parameter_value})\n        rpls.append(rpl)\n\n        # crp\n        crp = flex_mixed_crp(model_trials, model_presentations)\n        crp[global_lag_range] = np.nan\n        crp = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2 + 1, dtype=int)-crp_max_lag, \n            'Recall Rate': crp[list_length-crp_max_lag-1:list_length+crp_max_lag], \n            varied_parameter: parameter_value})\n        crps.append(crp)\n\n        # rep crp\n        rep_crp = alternative_contiguity(\n            model_trials, model_presentations, 6, 2)\n        rep_crp[:, global_lag_range] = np.nan\n        rep_crp = pd.DataFrame({\n                \"Lag\": np.arange(rep_crp_max_lag * 2 + 1, dtype=int) - rep_crp_max_lag,\n                \"Differential Recall Rate\": rep_crp[0, list_length-rep_crp_max_lag-1:list_length + rep_crp_max_lag]\n                    - rep_crp[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n                varied_parameter: parameter_value,\n            })\n        rep_crps.append(rep_crp)\n\n        # lag-connectivity requires more involved computations...\n        total_lag_mfc = np.zeros((global_lag_range * 2 + 1))\n        total_lag_mcf = np.zeros((global_lag_range * 2 + 1))\n        total_lag_mff = np.zeros((global_lag_range * 2 + 1))\n        total_rep_lag_mfc = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mcf = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mff = np.zeros((2, global_lag_range * 2 + 1))\n\n        # loop through presentations\n        for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n            # simulate list study\n            item_count = np.max(presentation)+1\n            model = model_class(item_count, len(presentation), model_parameters)\n            model.experience(model.items[presentation])\n\n            # extract item connections\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n            # track repetition connectivity\n            total_rep_lag_mfc += alternative_connectivity_by_lag(mfc_connections, presentation)\n            total_rep_lag_mcf += alternative_connectivity_by_lag(mcf_connections, presentation)\n            total_rep_lag_mff += alternative_connectivity_by_lag(mff_connections, presentation)\n\n            # and overall connectivity\n            total_lag_mfc += mixed_connectivity_by_lag(mfc_connections, presentation)\n            total_lag_mcf += mixed_connectivity_by_lag(mcf_connections, presentation)\n            total_lag_mff += mixed_connectivity_by_lag(mff_connections, presentation)\n\n        # reduce sum to mean\n        # repetition connectivity\n        rep_lag_mfc = total_rep_lag_mfc / (trial_index+1)\n        rep_lag_mcf = total_rep_lag_mcf / (trial_index+1)\n        rep_lag_mff = total_rep_lag_mff / (trial_index+1)\n        rep_lag_mfc[:, global_lag_range] = np.nan\n        rep_lag_mcf[:, global_lag_range] = np.nan\n        rep_lag_mff[:, global_lag_range] = np.nan\n\n        # overall connectivity\n        lag_mfc = total_lag_mfc / (trial_index+1)\n        lag_mcf = total_lag_mcf / (trial_index+1)\n        lag_mff = total_lag_mff / (trial_index+1)\n        lag_mfc[global_lag_range] = np.nan\n        lag_mcf[global_lag_range] = np.nan\n        lag_mff[global_lag_range] = np.nan\n\n        # aggregate for dataframe\n        rep_lag_mfc = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mfc[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mfc[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n        rep_lag_mcf = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mcf[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mcf[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n        rep_lag_mff = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mff[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mff[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n\n        lag_mfc = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mfc[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n        lag_mcf = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mcf[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n        lag_mff = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mff[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n\n        rep_lag_mcfs.append(rep_lag_mcf)\n        rep_lag_mffs.append(rep_lag_mff)\n        rep_lag_mfcs.append(rep_lag_mfc)\n        lag_mcfs.append(lag_mcf)\n        lag_mffs.append(lag_mff)\n        lag_mfcs.append(lag_mfc)\n    \n    # aggregate fitting results\n    spc = pd.concat(spcs).reset_index()\n    pfr = pd.concat(pfrs).reset_index()\n    crp = pd.concat(crps).reset_index()\n    rpl = pd.concat(rpls).reset_index()\n    rep_crp = pd.concat(rep_crps).reset_index()\n    lag_mcf = pd.concat(lag_mcfs).reset_index()\n    lag_mff = pd.concat(lag_mffs).reset_index()\n    lag_mfc = pd.concat(lag_mfcs).reset_index()\n    rep_lag_mcf = pd.concat(rep_lag_mcfs).reset_index()\n    rep_lag_mff = pd.concat(rep_lag_mffs).reset_index()\n    rep_lag_mfc = pd.concat(rep_lag_mfcs).reset_index()\n\n    # plot results\n    sns.set(style='darkgrid')\n    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(25, 25), sharey=False)\n\n    # spc\n    sns.lineplot(ax=axes[0, 0], data=spc, x='Study Position', y='Recall Rate', hue=varied_parameter, ci=None)\n    axes[0, 0].set_title('SPC')\n\n    # pfr\n    sns.lineplot(ax=axes[0, 1], data=pfr, x='Study Position', y='First Recall Rate', hue=varied_parameter, ci=None, legend=False)\n    axes[0, 1].set_title('PFR')\n\n    # rpl (should be rps)\n    sns.lineplot(ax=axes[0, 2], data=rpl, x='Spacing', y='Recall Rate', hue=varied_parameter, ci=None)\n    axes[0, 2].set_title('Recall Probability by Spacing')\n\n    # crp\n    filt_neg = f'{-crp_max_lag} <= Lag < 0'\n    filt_pos = f'0 < Lag <= {crp_max_lag}'\n    sns.lineplot(ax=axes[1, 0], data=crp.query(filt_neg), x='Lag', y='Recall Rate', \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[1, 0], data=crp.query(filt_pos), x='Lag', y='Recall Rate', \n        hue=varied_parameter, ci=None, legend=False)\n    axes[1, 0].set_title('CRP')\n\n    # rep_crp\n    sns.lineplot(ax=axes[1, 1], data=rep_crp.query(filt_neg), x='Lag', y=\"Differential Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[1, 1], data=rep_crp.query(filt_pos), x='Lag', y=\"Differential Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    axes[1, 1].set_title('Repetition Lag-Contiguity Difference')\n\n    # lag_mcf\n    sns.lineplot(ax=axes[2, 0], data=lag_mcf.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 0], data=lag_mcf.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # lag_mfc\n    sns.lineplot(ax=axes[2, 1], data=lag_mfc.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 1], data=lag_mfc.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # lag_mff\n    sns.lineplot(ax=axes[2, 2], data=lag_mff.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 2], data=lag_mff.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mcf\n    sns.lineplot(ax=axes[3, 0], data=rep_lag_mcf.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 0], data=rep_lag_mcf.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mfc\n    sns.lineplot(ax=axes[3, 1], data=rep_lag_mfc.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 1], data=rep_lag_mfc.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mff\n    sns.lineplot(ax=axes[3, 2], data=rep_lag_mff.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 2], data=rep_lag_mff.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # render result\n    plt.show()\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nencoding_drift_rate 0.832809463\n\n\n100%|██████████| 10/10 [03:15<00:00, 19.51s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nrecall_drift_rate 0.961140398\n\n\n100%|██████████| 10/10 [03:13<00:00, 19.32s/it]\n\n\n\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n\n\nshared_support 0.0011990853\n\n\n100%|██████████| 4/4 [01:15<00:00, 18.94s/it]\n\n\n\n\n\n  0%|          | 0/9 [00:00<?, ?it/s]\n\n\nitem_support 0.131621512\n\n\n100%|██████████| 9/9 [02:51<00:00, 19.06s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nlearning_rate 0.245449928\n\n\n100%|██████████| 10/10 [03:20<00:00, 20.04s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nchoice_sensitivity 1.0\n\n\n100%|██████████| 10/10 [03:24<00:00, 20.41s/it]\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\n# configure parameters\nmodel_class = init_trcmr\nmodel_name = 'Trace-Reinstatement CMR'\nparameters = trcmr_fitted_parameters\nlist_type = 4\n\n# track results\nglobal_lag_range = list_length-1\ncrp_max_lag = 13\nrep_crp_max_lag = 3\nexperiment_count = 100\n\nscore_ranges = {\n    'feature_drift_rate': np.arange(.001, 1, .1),\n}\n\nfor varied_parameter in score_ranges.keys():\n\n    print(varied_parameter, parameters[varied_parameter])\n    print(score_ranges[varied_parameter])\n    spcs = []\n    pfrs = []\n    rpls = []\n    crps = []\n    rep_crps = []\n    lag_mcfs = []\n    lag_mfcs = []\n    lag_mffs = []\n    rep_lag_mcfs = []\n    rep_lag_mfcs = []\n    rep_lag_mffs = []\n\n    # loop through parameter values\n    for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n        # simulate data with this parameter value modified\n        model_parameters = parameters.copy()\n        model_parameters[varied_parameter] = parameter_value\n        model_trials = simulate_array_from_presentations(\n            model_class, model_parameters, presentations[list_types==list_type], experiment_count)\n        model_presentations = np.matlib.repmat(presentations[list_types==list_type], experiment_count, 1)\n\n        # spc\n        spc = flex_mixed_spc(model_trials, model_presentations)\n        spc = pd.DataFrame(\n                {'Study Position': np.arange(len(spc)), 'Recall Rate': spc, varied_parameter: parameter_value})\n        spcs.append(spc)\n\n        # pfr\n        pfr = flex_mixed_pfr(model_trials, model_presentations)\n        pfr = pd.DataFrame(\n            {'Study Position': np.arange(len(pfr)), 'First Recall Rate': pfr, varied_parameter: parameter_value})\n        pfrs.append(pfr)\n\n        # rpl\n        rpl = fast_rpl(\n            model_trials, model_presentations, max_lag=8)\n        binned = np.zeros(5)\n        binned[0] = rpl[0]\n        binned[1] = rpl[1]\n        binned[2] = (rpl[2] + rpl[3])/2\n        binned[3] = (rpl[4] + rpl[5] + rpl[6])/3\n        binned[4] = (rpl[7] + rpl[8] + rpl[9])/3\n        rpl = binned.copy()\n        rpl = pd.DataFrame(\n            {'Spacing': ['N/A', '0', '1-2', '3-5', '6-8'], 'Recall Rate': rpl, varied_parameter: parameter_value})\n        rpls.append(rpl)\n\n        # crp\n        crp = flex_mixed_crp(model_trials, model_presentations)\n        crp[global_lag_range] = np.nan\n        crp = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2 + 1, dtype=int)-crp_max_lag, \n            'Recall Rate': crp[list_length-crp_max_lag-1:list_length+crp_max_lag], \n            varied_parameter: parameter_value})\n        crps.append(crp)\n\n        # rep crp\n        rep_crp = alternative_contiguity(\n            model_trials, model_presentations, 6, 2)\n        rep_crp[:, global_lag_range] = np.nan\n        rep_crp = pd.DataFrame({\n                \"Lag\": np.arange(rep_crp_max_lag * 2 + 1, dtype=int) - rep_crp_max_lag,\n                \"Differential Recall Rate\": rep_crp[0, list_length-rep_crp_max_lag-1:list_length + rep_crp_max_lag]\n                    - rep_crp[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n                varied_parameter: parameter_value,\n            })\n        rep_crps.append(rep_crp)\n\n        # lag-connectivity requires more involved computations...\n        total_lag_mfc = np.zeros((global_lag_range * 2 + 1))\n        total_lag_mcf = np.zeros((global_lag_range * 2 + 1))\n        total_lag_mff = np.zeros((global_lag_range * 2 + 1))\n        total_rep_lag_mfc = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mcf = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mff = np.zeros((2, global_lag_range * 2 + 1))\n\n        # loop through presentations\n        for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n            # simulate list study\n            item_count = np.max(presentation)+1\n            model = model_class(item_count, len(presentation), model_parameters)\n            model.experience(model.items[presentation])\n\n            # extract item connections\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.recall_items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n            # track repetition connectivity\n            total_rep_lag_mfc += alternative_connectivity_by_lag(mfc_connections, presentation)\n            total_rep_lag_mcf += alternative_connectivity_by_lag(mcf_connections, presentation)\n            total_rep_lag_mff += alternative_connectivity_by_lag(mff_connections, presentation)\n\n            # and overall connectivity\n            total_lag_mfc += mixed_connectivity_by_lag(mfc_connections, presentation)\n            total_lag_mcf += mixed_connectivity_by_lag(mcf_connections, presentation)\n            total_lag_mff += mixed_connectivity_by_lag(mff_connections, presentation)\n\n        # reduce sum to mean\n        # repetition connectivity\n        rep_lag_mfc = total_rep_lag_mfc / (trial_index+1)\n        rep_lag_mcf = total_rep_lag_mcf / (trial_index+1)\n        rep_lag_mff = total_rep_lag_mff / (trial_index+1)\n        rep_lag_mfc[:, global_lag_range] = np.nan\n        rep_lag_mcf[:, global_lag_range] = np.nan\n        rep_lag_mff[:, global_lag_range] = np.nan\n\n        # overall connectivity\n        lag_mfc = total_lag_mfc / (trial_index+1)\n        lag_mcf = total_lag_mcf / (trial_index+1)\n        lag_mff = total_lag_mff / (trial_index+1)\n        lag_mfc[global_lag_range] = np.nan\n        lag_mcf[global_lag_range] = np.nan\n        lag_mff[global_lag_range] = np.nan\n\n        # aggregate for dataframe\n        rep_lag_mfc = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mfc[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mfc[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n        rep_lag_mcf = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mcf[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mcf[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n        rep_lag_mff = pd.DataFrame(\n            {'Lag': np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag,\n            'Differential Recall Rate': rep_lag_mff[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag]\n            - rep_lag_mff[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag],\n            varied_parameter: parameter_value})\n\n        lag_mfc = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mfc[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n        lag_mcf = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mcf[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n        lag_mff = pd.DataFrame(\n            {'Lag': np.arange(crp_max_lag*2+1, dtype=int)-crp_max_lag,\n            'Recall Rate': lag_mff[list_length-crp_max_lag-1:list_length+crp_max_lag],\n            varied_parameter: parameter_value})\n\n        rep_lag_mcfs.append(rep_lag_mcf)\n        rep_lag_mffs.append(rep_lag_mff)\n        rep_lag_mfcs.append(rep_lag_mfc)\n        lag_mcfs.append(lag_mcf)\n        lag_mffs.append(lag_mff)\n        lag_mfcs.append(lag_mfc)\n    \n    # aggregate fitting results\n    spc = pd.concat(spcs).reset_index()\n    pfr = pd.concat(pfrs).reset_index()\n    crp = pd.concat(crps).reset_index()\n    rpl = pd.concat(rpls).reset_index()\n    rep_crp = pd.concat(rep_crps).reset_index()\n    lag_mcf = pd.concat(lag_mcfs).reset_index()\n    lag_mff = pd.concat(lag_mffs).reset_index()\n    lag_mfc = pd.concat(lag_mfcs).reset_index()\n    rep_lag_mcf = pd.concat(rep_lag_mcfs).reset_index()\n    rep_lag_mff = pd.concat(rep_lag_mffs).reset_index()\n    rep_lag_mfc = pd.concat(rep_lag_mfcs).reset_index()\n\n    # plot results\n    sns.set(style='darkgrid')\n    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(25, 25), sharey=False)\n\n    # spc\n    sns.lineplot(ax=axes[0, 0], data=spc, x='Study Position', y='Recall Rate', hue=varied_parameter, ci=None)\n    axes[0, 0].set_title('SPC')\n\n    # pfr\n    sns.lineplot(ax=axes[0, 1], data=pfr, x='Study Position', y='First Recall Rate', hue=varied_parameter, ci=None, legend=False)\n    axes[0, 1].set_title('PFR')\n\n    # rpl (should be rps)\n    sns.lineplot(ax=axes[0, 2], data=rpl, x='Spacing', y='Recall Rate', hue=varied_parameter, ci=None)\n    axes[0, 2].set_title('Recall Probability by Spacing')\n\n    # crp\n    filt_neg = f'{-crp_max_lag} <= Lag < 0'\n    filt_pos = f'0 < Lag <= {crp_max_lag}'\n    sns.lineplot(ax=axes[1, 0], data=crp.query(filt_neg), x='Lag', y='Recall Rate', \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[1, 0], data=crp.query(filt_pos), x='Lag', y='Recall Rate', \n        hue=varied_parameter, ci=None, legend=False)\n    axes[1, 0].set_title('CRP')\n\n    # rep_crp\n    sns.lineplot(ax=axes[1, 1], data=rep_crp.query(filt_neg), x='Lag', y=\"Differential Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[1, 1], data=rep_crp.query(filt_pos), x='Lag', y=\"Differential Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    axes[1, 1].set_title('Repetition Lag-Contiguity Difference')\n\n    # lag_mcf\n    sns.lineplot(ax=axes[2, 0], data=lag_mcf.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 0], data=lag_mcf.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # lag_mfc\n    sns.lineplot(ax=axes[2, 1], data=lag_mfc.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 1], data=lag_mfc.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # lag_mff\n    sns.lineplot(ax=axes[2, 2], data=lag_mff.query(filt_neg), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[2, 2], data=lag_mff.query(filt_pos), x='Lag', y='Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mcf\n    sns.lineplot(ax=axes[3, 0], data=rep_lag_mcf.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 0], data=rep_lag_mcf.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mfc\n    sns.lineplot(ax=axes[3, 1], data=rep_lag_mfc.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 1], data=rep_lag_mfc.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # rep_lag_mff\n    sns.lineplot(ax=axes[3, 2], data=rep_lag_mff.query(filt_neg), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[3, 2], data=rep_lag_mff.query(filt_pos), x='Lag', y='Differential Recall Rate',\n        hue=varied_parameter, ci=None, legend=False)\n\n    # render result\n    plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nfeature_drift_rate 1.0\n[0.001 0.101 0.201 0.301 0.401 0.501 0.601 0.701 0.801 0.901]\n\n\n100%|██████████| 10/10 [04:29<00:00, 26.98s/it]"
  },
  {
    "objectID": "library\\model_analysis\\Model_Characterization.html#repetition-contiguity-parameter-shifting-experiments",
    "href": "library\\model_analysis\\Model_Characterization.html#repetition-contiguity-parameter-shifting-experiments",
    "title": "compmemlearn",
    "section": "Repetition Contiguity Parameter Shifting Experiments",
    "text": "InstanceCMR\n\n# configure parameters\nmodel_class = init_icmr\nmodel_name = 'Instance_CMR'\nparameters = icmr_fitted_parameters\nlist_type = 4\n\n# track results\nglobal_lag_range = list_length-1\ncrp_max_lag = 13\nrep_crp_max_lag = 3\nexperiment_count = 100\n\nscore_ranges = {\n    'encoding_drift_rate': np.arange(.001, .99, .1),\n    'recall_drift_rate': np.arange(.001, .99, .1),\n    'shared_support': np.arange(.001, .005, .001),\n    'item_support': np.arange(.001, .01, .001),\n    'learning_rate': np.arange(.001, .99, .1),\n    'choice_sensitivity': np.arange(.001, 5, .5),\n}\n\nfor varied_parameter in score_ranges.keys():\n\n    print(varied_parameter, parameters[varied_parameter])\n    rep_crps = []\n    rep_lag_mcfs = []\n    rep_lag_mfcs = []\n    rep_lag_mffs = []\n\n    # loop through parameter values\n    for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n        # simulate data with this parameter value modified\n        model_parameters = parameters.copy()\n        model_parameters[varied_parameter] = parameter_value\n        model_trials = simulate_array_from_presentations(\n            model_class, model_parameters, presentations[list_types==list_type], experiment_count)\n        model_presentations = np.matlib.repmat(presentations[list_types==list_type], experiment_count, 1)\n\n        # rep crp\n        rep_crp = alternative_contiguity(\n            model_trials, model_presentations, 6, 2)\n        rep_crp[:, global_lag_range] = np.nan\n        rep_crp = pd.DataFrame({\n                \"Lag\": (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n                \"Recall Rate\": rep_crp[0, list_length-rep_crp_max_lag-1:list_length + rep_crp_max_lag].tolist()\n                    + rep_crp[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n                varied_parameter: parameter_value,\n                \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)\n            })\n        rep_crps.append(rep_crp)\n\n        # lag-connectivity requires more involved computations...\n        total_rep_lag_mfc = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mcf = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mff = np.zeros((2, global_lag_range * 2 + 1))\n\n        # loop through presentations\n        for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n            # simulate list study\n            item_count = np.max(presentation)+1\n            model = model_class(item_count, len(presentation), model_parameters)\n            model.experience(model.items[presentation])\n\n            # extract item connections\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n            # track repetition connectivity\n            total_rep_lag_mfc += alternative_connectivity_by_lag(mfc_connections, presentation)\n            total_rep_lag_mcf += alternative_connectivity_by_lag(mcf_connections, presentation)\n            total_rep_lag_mff += alternative_connectivity_by_lag(mff_connections, presentation)\n\n        # reduce sum to mean\n        # repetition connectivity\n        rep_lag_mfc = total_rep_lag_mfc / (trial_index+1)\n        rep_lag_mcf = total_rep_lag_mcf / (trial_index+1)\n        rep_lag_mff = total_rep_lag_mff / (trial_index+1)\n        rep_lag_mfc[:, global_lag_range] = np.nan\n        rep_lag_mcf[:, global_lag_range] = np.nan\n        rep_lag_mff[:, global_lag_range] = np.nan\n\n        # aggregate for dataframe\n        rep_lag_mfc = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mfc[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mfc[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n        rep_lag_mcf = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mcf[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mcf[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n        rep_lag_mff = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mff[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mff[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n\n        rep_lag_mcfs.append(rep_lag_mcf)\n        rep_lag_mffs.append(rep_lag_mff)\n        rep_lag_mfcs.append(rep_lag_mfc)\n\n    # aggregate fitting results\n    rep_crp = pd.concat(rep_crps).reset_index()\n    rep_lag_mcf = pd.concat(rep_lag_mcfs).reset_index()\n    rep_lag_mff = pd.concat(rep_lag_mffs).reset_index()\n    rep_lag_mfc = pd.concat(rep_lag_mfcs).reset_index()\n\n    # plot results\n    sns.set(style='darkgrid')\n    fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(25, 25), sharey=True)\n\n    # rep_crp\n    sns.lineplot(ax=axes[0, 0], data=rep_crp[rep_crp['Position']=='First'].query(filt_neg), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 0], data=rep_crp[rep_crp['Position']=='First'].query(filt_pos), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 1], data=rep_crp[rep_crp['Position']=='Second'].query(filt_neg), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 1], data=rep_crp[rep_crp['Position']=='Second'].query(filt_pos), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    axes[0, 0].set_title('First Repetition Lag-Contiguity')\n    axes[0, 1].set_title('Second Repetition Lag-Contiguity')\n    \n    # rep_lag_mcf\n    sns.lineplot(\n        ax=axes[1, 0], data=rep_lag_mcf[rep_lag_mcf['Position']=='First'].query(filt_neg), \n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 0], data=rep_lag_mcf[rep_lag_mcf['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 1], data=rep_lag_mcf[rep_lag_mcf['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 1], data=rep_lag_mcf[rep_lag_mcf['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[1, 0].set_title('MCF First Repetition Lag-Contiguity')\n    axes[1, 1].set_title('MCF Second Repetition Lag-Contiguity')\n\n    # rep_lag_mfc\n    sns.lineplot(\n        ax=axes[2, 0], data=rep_lag_mfc[rep_lag_mfc['Position']=='First'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 0], data=rep_lag_mfc[rep_lag_mfc['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 1], data=rep_lag_mfc[rep_lag_mfc['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 1], data=rep_lag_mfc[rep_lag_mfc['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[2, 0].set_title('MFC First Repetition Lag-Contiguity')\n    axes[2, 1].set_title('MFC Second Repetition Lag-Contiguity')\n\n    # rep_lag_mff\n    sns.lineplot(\n        ax=axes[3, 0], data=rep_lag_mff[rep_lag_mff['Position']=='First'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 0], data=rep_lag_mff[rep_lag_mff['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 1], data=rep_lag_mff[rep_lag_mff['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 1], data=rep_lag_mff[rep_lag_mff['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[3, 0].set_title('MFF First Repetition Lag-Contiguity')\n    axes[3, 1].set_title('MFF Second Repetition Lag-Contiguity')\n\n    # render result\n    plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nencoding_drift_rate 0.832809463\n\n\n100%|██████████| 10/10 [02:42<00:00, 16.25s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nrecall_drift_rate 0.961140398\n\n\n100%|██████████| 10/10 [02:30<00:00, 15.09s/it]\n\n\n\n\n\n  0%|          | 0/4 [00:00<?, ?it/s]\n\n\nshared_support 0.0011990853\n\n\n100%|██████████| 4/4 [01:00<00:00, 15.11s/it]\n\n\n\n\n\n  0%|          | 0/9 [00:00<?, ?it/s]\n\n\nitem_support 0.131621512\n\n\n100%|██████████| 9/9 [02:15<00:00, 15.10s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nlearning_rate 0.245449928\n\n\n100%|██████████| 10/10 [02:34<00:00, 15.40s/it]\n\n\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nchoice_sensitivity 1.0\n\n\n100%|██████████| 10/10 [02:36<00:00, 15.65s/it]\n\n\n\n\n\n\n\nTrace-Reinstatement CMR\n\n# configure parameters\nmodel_class = init_trcmr\nmodel_name = 'Trace-Reinstatement CMR'\nparameters = trcmr_fitted_parameters\nlist_type = 4\n\n# track results\nglobal_lag_range = list_length-1\ncrp_max_lag = 13\nrep_crp_max_lag = 3\nexperiment_count = 100\n\nscore_ranges = {\n    'feature_drift_rate': np.arange(.001, 1, .1),\n}\n\nfor varied_parameter in score_ranges.keys():\n\n    print(varied_parameter, parameters[varied_parameter])\n    rep_crps = []\n    rep_lag_mcfs = []\n    rep_lag_mfcs = []\n    rep_lag_mffs = []\n\n    # loop through parameter values\n    for parameter_value in tqdm(score_ranges[varied_parameter]):\n\n        # simulate data with this parameter value modified\n        model_parameters = parameters.copy()\n        model_parameters[varied_parameter] = parameter_value\n        model_trials = simulate_array_from_presentations(\n            model_class, model_parameters, presentations[list_types==list_type], experiment_count)\n        model_presentations = np.matlib.repmat(presentations[list_types==list_type], experiment_count, 1)\n\n        # rep crp\n        rep_crp = alternative_contiguity(\n            model_trials, model_presentations, 6, 2)\n        rep_crp[:, global_lag_range] = np.nan\n        rep_crp = pd.DataFrame({\n                \"Lag\": (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n                \"Recall Rate\": rep_crp[0, list_length-rep_crp_max_lag-1:list_length + rep_crp_max_lag].tolist()\n                    + rep_crp[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n                varied_parameter: parameter_value,\n                \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)\n            })\n        rep_crps.append(rep_crp)\n\n        # lag-connectivity requires more involved computations...\n        total_rep_lag_mfc = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mcf = np.zeros((2, global_lag_range * 2 + 1))\n        total_rep_lag_mff = np.zeros((2, global_lag_range * 2 + 1))\n\n        # loop through presentations\n        for trial_index, presentation in enumerate(presentations[list_types==list_type]):\n\n            # simulate list study\n            item_count = np.max(presentation)+1\n            model = model_class(item_count, len(presentation), model_parameters)\n            model.experience(model.items[presentation])\n\n            # extract item connections\n            latent_mfc, mcf_connections, mff_connections = latent_mfc_mcf_mff(model, model.recall_items)\n            mfc_connections = latent_mfc[:, 1:-1]\n\n            # track repetition connectivity\n            total_rep_lag_mfc += alternative_connectivity_by_lag(mfc_connections, presentation)\n            total_rep_lag_mcf += alternative_connectivity_by_lag(mcf_connections, presentation)\n            total_rep_lag_mff += alternative_connectivity_by_lag(mff_connections, presentation)\n\n        # reduce sum to mean\n        # repetition connectivity\n        rep_lag_mfc = total_rep_lag_mfc / (trial_index+1)\n        rep_lag_mcf = total_rep_lag_mcf / (trial_index+1)\n        rep_lag_mff = total_rep_lag_mff / (trial_index+1)\n        rep_lag_mfc[:, global_lag_range] = np.nan\n        rep_lag_mcf[:, global_lag_range] = np.nan\n        rep_lag_mff[:, global_lag_range] = np.nan\n\n        # aggregate for dataframe\n        rep_lag_mfc = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mfc[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mfc[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n        rep_lag_mcf = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mcf[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mcf[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n        rep_lag_mff = pd.DataFrame(\n            {'Lag': (np.arange(rep_crp_max_lag*2+1, dtype=int)-rep_crp_max_lag).tolist() * 2,\n            'Recall Rate': rep_lag_mff[0, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist()\n            + rep_lag_mff[1, list_length-rep_crp_max_lag-1:list_length+rep_crp_max_lag].tolist(),\n            varied_parameter: parameter_value,\n            \"Position\": ['First'] * (rep_crp_max_lag*2+1) + ['Second'] * (rep_crp_max_lag*2+1)})\n\n        rep_lag_mcfs.append(rep_lag_mcf)\n        rep_lag_mffs.append(rep_lag_mff)\n        rep_lag_mfcs.append(rep_lag_mfc)\n\n    # aggregate fitting results\n    rep_crp = pd.concat(rep_crps).reset_index()\n    rep_lag_mcf = pd.concat(rep_lag_mcfs).reset_index()\n    rep_lag_mff = pd.concat(rep_lag_mffs).reset_index()\n    rep_lag_mfc = pd.concat(rep_lag_mfcs).reset_index()\n\n    # plot results\n    sns.set(style='darkgrid')\n    fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 25), sharey=True)\n\n    # rep_crp\n    sns.lineplot(ax=axes[0, 0], data=rep_crp[rep_crp['Position']=='First'].query(filt_neg), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 0], data=rep_crp[rep_crp['Position']=='First'].query(filt_pos), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 1], data=rep_crp[rep_crp['Position']=='Second'].query(filt_neg), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(ax=axes[0, 1], data=rep_crp[rep_crp['Position']=='Second'].query(filt_pos), x='Lag', y=\"Recall Rate\", \n        hue=varied_parameter, ci=None, legend=False)\n    axes[0, 0].set_title('First Repetition Lag-Contiguity')\n    axes[0, 1].set_title('Second Repetition Lag-Contiguity')\n    \n    # rep_lag_mcf\n    sns.lineplot(\n        ax=axes[1, 0], data=rep_lag_mcf[rep_lag_mcf['Position']=='First'].query(filt_neg), \n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 0], data=rep_lag_mcf[rep_lag_mcf['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 1], data=rep_lag_mcf[rep_lag_mcf['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[1, 1], data=rep_lag_mcf[rep_lag_mcf['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[1, 0].set_title('MCF First Repetition Lag-Contiguity')\n    axes[1, 1].set_title('MCF Second Repetition Lag-Contiguity')\n\n    # rep_lag_mfc\n    sns.lineplot(\n        ax=axes[2, 0], data=rep_lag_mfc[rep_lag_mfc['Position']=='First'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 0], data=rep_lag_mfc[rep_lag_mfc['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 1], data=rep_lag_mfc[rep_lag_mfc['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[2, 1], data=rep_lag_mfc[rep_lag_mfc['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[2, 0].set_title('MFC First Repetition Lag-Contiguity')\n    axes[2, 1].set_title('MFC Second Repetition Lag-Contiguity')\n\n    # rep_lag_mff\n    sns.lineplot(\n        ax=axes[3, 0], data=rep_lag_mff[rep_lag_mff['Position']=='First'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 0], data=rep_lag_mff[rep_lag_mff['Position']=='First'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 1], data=rep_lag_mff[rep_lag_mff['Position']=='Second'].query(filt_neg),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    sns.lineplot(\n        ax=axes[3, 1], data=rep_lag_mff[rep_lag_mff['Position']=='Second'].query(filt_pos),\n        x='Lag', y=\"Recall Rate\", hue=varied_parameter, ci=None, legend=False)\n    axes[3, 0].set_title('MFF First Repetition Lag-Contiguity')\n    axes[3, 1].set_title('MFF Second Repetition Lag-Contiguity')\n\n    # render result\n    plt.show()\n\n  0%|          | 0/10 [00:00<?, ?it/s]\n\n\nfeature_drift_rate 1.0\n\n\n100%|██████████| 10/10 [03:52<00:00, 23.28s/it]"
  },
  {
    "objectID": "library\\model_analysis\\Model_Visualization.html#parameter-configuration",
    "href": "library\\model_analysis\\Model_Visualization.html#parameter-configuration",
    "title": "compmemlearn",
    "section": "Parameter Configuration",
    "text": "Pick some parameters for Instance_CMR and CMR to organize comparisons."
  },
  {
    "objectID": "library\\model_analysis\\Model_Visualization.html#encoding",
    "href": "library\\model_analysis\\Model_Visualization.html#encoding",
    "title": "compmemlearn",
    "section": "Encoding",
    "text": "First we create simulations and visualizations to track model state throughout encoding of new memories. To do this, we produce two parallel functions, encoding_states and plot_states that collect and visualize encoding states, respectively. An additional wrapper function called encoding_visualizations plots these states in addition to the final overall state of model memory.\nicmr_parameters = {\n}\n\ncmr_parameters = {\n}\n#hide \n\nimport numpy as np\n\ndef encoding_states(model):\n    \"\"\"\n    Tracks state of context, and item supports across encoding. Model is also advanced to a state of fully encoded\n    memories.\n\n    **Required model attributes**:  \n    - item_count: specifies number of items encoded into memory  \n    - context: vector representing an internal contextual state  \n    - experience: adding a new trace to the memory model  \n    - activations: function returning item activations given a vector probe  \n    - outcome_probabilities: function returning item supports given a set of activations\n\n    **Returns** array representations of context and support for retrieval of each item at each increment of item\n    encoding. Each has shape model.item_count by model.item_count + 1.\n    \"\"\"\n    \n    experiences = np.eye(model.item_count, model.item_count + 1, 1)\n    cmr_experiences = np.eye(model.item_count, model.item_count)\n    encoding_contexts, encoding_supports = model.context, []\n\n    # track model state across experiences\n    for i in range(len(experiences)):\n        try:\n            model.experience(experiences[i].reshape((1, -1)))\n        except ValueError:\n            # special case for CMR\n            model.experience(cmr_experiences[i].reshape((1, -1)))\n\n        # track model contexts and item supports\n        encoding_contexts = np.vstack((encoding_contexts, model.context))\n\n        if model.__class__.__name__ == 'CMR':\n            activation_cue = lambda model: model.context\n        else:\n            activation_cue = lambda model: np.hstack((np.zeros(model.item_count + 1), model.context))\n\n        if len(encoding_supports) > 0:\n            encoding_supports = np.vstack((encoding_supports, model.outcome_probabilities(activation_cue(model))))\n        else:\n            encoding_supports = model.outcome_probabilities(activation_cue(model))\n    \n    return encoding_contexts, encoding_supports\nshow_doc(encoding_states, title_level=3)\n# hide\n\n# collapse_input\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_states(matrix, title, figsize=(15, 15), savefig=False):\n    \"\"\"\n    Plots an array of model states as a value-annotated heatmap with an arbitrary title.\n\n    **Arguments**:  \n    - matrix: an array of model states, ideally with columns representing unique feature indices and rows\n        representing unique update indices  \n    - title: a title for the generated plot, ideally conveying what array values represent at each entry  \n    - savefig: boolean deciding whether generated figure is saved (True if Yes)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(matrix, annot=True, linewidths=.5)\n    plt.title(title)\n    plt.xlabel('Feature Index')\n    plt.ylabel('Update Index')\n    if savefig:\n        plt.savefig('figures/{}.jpeg'.format(title).replace(' ', '_').lower(), bbox_inches='tight')\n    plt.show()\nshow_doc(plot_states, title_level=3)\n\ndef encoding_visualizations(model, savefig=True):\n    \"\"\"\n    Plots encoding contexts, encoding supports as heatmaps.\n\n    **Required model attributes**:  \n    - item_count: specifies number of items encoded into memory  \n    - context: vector representing an internal contextual state  \n    - experience: adding a new trace to the memory model  \n    - activations: function returning item activations given a vector probe  \n    - outcome_probabilities: function returning item supports given a set of activations\n    - memory: a unitary representation of the current state of memory\n\n    **Also** requires savefig:  boolean deciding if generated figure is saved\n    \"\"\"\n    \n    encoding_contexts, encoding_supports = encoding_states(model)\n    plot_states(encoding_contexts, 'Encoding Contexts', savefig=savefig)\n    plot_states(encoding_supports, 'Supports For Each Item At Each Increment of Encoding', savefig=savefig)\ntry:\n    show_doc(encoding_visualizations, title_level=3)\nexcept:\n    pass\n\nDemo\n\nICMR\nfrom instance_cmr.models import InstanceCMR\n\nmodel = InstanceCMR(**icmr_parameters)\nencoding_visualizations(model)\n \n\n\nCMR\nfrom instance_cmr.models import CMR\n\nmodel = CMR(**cmr_parameters)\nencoding_visualizations(model)"
  },
  {
    "objectID": "library\\model_analysis\\Model_Visualization.html#latent-mfcmcf",
    "href": "library\\model_analysis\\Model_Visualization.html#latent-mfcmcf",
    "title": "compmemlearn",
    "section": "Latent Mfc/Mcf",
    "text": "def latent_mfc_mcf(model):\n    \n    \"\"\"\n    Generates the latent $M^{FC}$ and $M^{CF}$ in the specified ICMR instance.\n    For exploring and demonstrating model equivalence, we can calculate for any state of ICMR's dual-store memory \n    array $M$ a corresponding $M^{FC}$ (or $M^{CF}$) by computing for each orthogonal $f_i$ (or $c_i$) the model's \n    corresponding echo representation. \n    \"\"\"\n\n    encoding_states(model)\n    \n    # start by finding latent mfc: the contextual representation cued when each orthogonal $f_i$ is cued\n    latent_mfc = np.zeros((model.item_count, model.item_count+1))\n    cue = np.zeros(model.item_count*2 + 2)\n    for i in range(model.item_count):\n        cue *= 0\n        cue[i+1] = 1\n        latent_mfc[i] = model.echo(cue)[model.item_count + 1:]\n\n    # now the latent mcf\n    latent_mcf = np.zeros((model.item_count+1, model.item_count))\n    for i in range(model.item_count+1):\n        cue *= 0\n        cue[model.item_count+1+i] = 1\n        latent_mcf[i] = model.echo(cue)[1:model.item_count + 1] # start at 1 due to dummy column in F\n\n    # plotting\n    return latent_mfc, latent_mcf\nif True:\n    # ICMR\n    model = InstanceCMR(**parameters)\n    latent_mfc, latent_mcf = latent_mfc_mcf(model)\n    print(model.__class__.__name__)\n    plot_states(model.memory, 'ICMR Memory')\n    plot_states(latent_mfc, 'ICMR Latent Mfc')\n    plot_states(latent_mcf, 'ICMR Latent Mcf')\n\n    # CMR\n    model = CMR(**parameters)\n    encoding_states(model)\n    print(model.__class__.__name__)\n    plot_states(model.mfc, 'CMR Mfc')\n    plot_states(model.mcf, 'CMR Mcf')"
  },
  {
    "objectID": "library\\model_analysis\\Model_Visualization.html#retrieval",
    "href": "library\\model_analysis\\Model_Visualization.html#retrieval",
    "title": "compmemlearn",
    "section": "Retrieval",
    "text": "Tracking model state across each step of retrieval. Since it’s stochastic, these values change with each random seed. An additional optional parameter first_recall_item can control which item is recalled first by the model (0 denotes termination of recall while actual items are 1-indexed); it is useful for testing hypotheses about model dynamics during recall. We leave the parameter set at None, for now, indicating no controlled first recall.\n\nimport numpy as np\n\ndef retrieval_states(model, first_recall_item=None):\n    \"\"\"\n    Tracks state of context, and item supports across retrieval. Model is also advanced into a state of\n    completed free recall.\n\n    **Required model attributes**:\n    - item_count: specifies number of items encoded into memory\n    - context: vector representing an internal contextual state\n    - experience: adding a new trace to the memory model\n    - activations: function returning item activations given a vector probe\n    - outcome_probabilities: function returning item supports given a set of activations\n    - free_recall: function that freely recalls a given number of items or until recall stops\n    - state: indicates whether model is encoding or engaged in recall with a string\n\n    **Also** optionally uses first_recall_item: can specify an item for first recall\n\n    **Returns** array representations of context and support for retrieval of each item at each increment of item\n    retrieval. Also returns recall train associated with simulation.\n    \"\"\"\n\n    if model.__class__.__name__ == 'CMR':\n        activation_cue = lambda model: model.context\n    else:\n        activation_cue = lambda model: np.hstack((np.zeros(model.item_count + 1), model.context))\n\n    # encoding items, presuming model is freshly initialized\n    encoding_states(model)\n    retrieval_contexts, retrieval_supports = model.context, model.outcome_probabilities(activation_cue(model))\n\n    # pre-retrieval distraction\n    model.free_recall(0)\n    retrieval_contexts = np.vstack((retrieval_contexts, model.context))\n    retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(activation_cue(model))))\n\n    # optional forced first item recall\n    if first_recall_item is not None:\n        model.force_recall(first_recall_item)\n        retrieval_contexts = np.vstack((retrieval_contexts, model.context))\n        retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(activation_cue(model))))\n\n    # actual recall\n    while model.retrieving:\n        model.free_recall(1)\n        retrieval_contexts = np.vstack((retrieval_contexts, model.context))\n        retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(activation_cue(model))))\n\n    return retrieval_contexts, retrieval_supports, model.recall[:model.recall_total]\ntry:\n    show_doc(retrieval_states, title_level=3)\nexcept:\n    pass\n\ndef outcome_probs_at_index(model, support_index_to_plot=1, savefig=True):\n    \"\"\"\n    Plots outcome probability distribution at a specific index of free recall.\n\n    **Required model attributes**:\n    - item_count: specifies number of items encoded into memory  \n    - context: vector representing an internal contextual state  \n    - experience: adding a new trace to the memory model  \n    - activations: function returning item activations given a vector probe  \n    - outcome_probabilities: function returning item supports given a set of activations  \n    - free_recall: function that freely recalls a given number of items or until recall stops  \n    - state: indicates whether model is encoding or engaged in recall with a string\n\n    **Other arguments**:  \n    - support_index_to_plot: index of retrieval to plot  \n    - savefig: whether to save or display the figure of interest\n\n    **Generates** a plot of outcome probabilities as a line graph. Also returns vector representation of the\n    generated probabilities.\n    \"\"\"\n\n    retrieval_supports = retrieval_states(model)[1]\n    plt.plot(np.arange(model.item_count + 1), retrieval_supports[support_index_to_plot])\n    plt.xlabel('Choice Index')\n    plt.ylabel('Outcome Probability')\n    plt.title('Outcome Probabilities At Recall Index {}'.format(support_index_to_plot))\n    plt.show()\n    return retrieval_supports[support_index_to_plot]\ntry:\n    show_doc(outcome_probs_at_index, title_level=3)\nexcept:\n    pass\n\ndef retrieval_visualizations(model, savefig=True):\n    \"\"\"\n    Plots incremental retrieval contexts and supports, as heatmaps, and prints recalled items.\n\n    **Required model attributes**:\n    - item_count: specifies number of items encoded into memory\n    - context: vector representing an internal contextual state\n    - experience: adding a new trace to the memory model\n    - activations: function returning item activations given a vector probe\n    - outcome_probabilities: function returning item supports given a set of activations\n\n    **Also** uses savefig: boolean deciding whether figures are saved (True) or displayed\n    \"\"\"\n    \n    retrieval_contexts, retrieval_supports, recall = retrieval_states(model)\n    plot_states(retrieval_contexts, 'Retrieval Contexts', savefig=savefig)\n    plot_states(retrieval_supports, 'Supports For Each Item At Each Increment of Retrieval', \n                savefig=savefig)\n    return recall\ntry:\n    show_doc(retrieval_visualizations, title_level=3)\nexcept:\n    pass\n\nDemo\n\nICMR\nmodel = InstanceCMR(**icmr_parameters)\nretrieval_visualizations(model)\nOutputs can look like…\n \n\n\nCMR\nmodel = CMR(**cmr_parameters)\nretrieval_visualizations(model)"
  },
  {
    "objectID": "library\\model_analysis\\Model_Visualization.html#organizational-analyses",
    "href": "library\\model_analysis\\Model_Visualization.html#organizational-analyses",
    "title": "compmemlearn",
    "section": "Organizational Analyses",
    "text": "Upon completion, the psifr toolbox is used to generate three plots corresponding to the contents of Figure 4 in Morton & Polyn, 2016: 1. Recall probability as a function of serial position 2. Probability of starting recall with each serial position 3. Conditional response probability as a function of lag\nWhereas previous visualizations were based on an arbitrary model simulation, the current figures are based on averages over a simulation of the model some specified amount of times.\n\nimport pandas as pd\nfrom psifr import fr\n\ndef temporal_organization_analyses(model, experiment_count, savefig=False, figsize=(15, 15), first_recall_item=None):\n    \"\"\"\n    Visualization of the outcomes of a trio of organizational analyses of model performance on a free recall\n    task.\n\n    **Required model attributes**:\n    - item_count: specifies number of items encoded into memory  \n    - context: vector representing an internal contextual state  \n    - experience: adding a new trace to the memory model  \n    - free_recall: function that freely recalls a given number of items or until recall stops  \n\n    **Other arguments**:  \n    - experiment_count: number of simulations to compute curves over  \n    - savefig: whether to save or display the figure of interest\n\n    **Returns** three plots corresponding to the contents of Figure 4 in Morton & Polyn, 2016:  \n    1. Recall probability as a function of serial position  \n    2. Probability of starting recall with each serial position  \n    3. Conditional response probability as a function of lag  \n    \"\"\"\n    \n    # encode items\n    try:\n        model.experience(np.eye(model.item_count, model.item_count + 1, 1))\n    except ValueError:\n        # so we can apply to CMR\n        model.experience(np.eye(model.item_count, model.item_count))\n    \n    # simulate retrieval for the specified number of times, tracking results in df\n    data = []\n    for experiment in range(experiment_count):\n        data += [[experiment, 0, 'study', i + 1, i] for i in range(model.item_count)]\n    for experiment in range(experiment_count):\n        if first_recall_item is not None:\n            model.force_recall(first_recall_item)\n        data += [[experiment, 0, 'recall', i + 1, o] for i, o in enumerate(model.free_recall())]\n    data = pd.DataFrame(data, columns=['subject', 'list', 'trial_type', 'position', 'item'])\n    merged = fr.merge_free_recall(data)\n    \n    # visualizations\n    # spc\n    recall = fr.spc(merged)\n    g = fr.plot_spc(recall)\n    plt.title('Serial Position Curve')\n    if savefig:\n        plt.savefig('figures/spc.jpeg', bbox_inches='tight')\n    else:\n        plt.show()\n\n    # P(Start Recall) For Each Serial Position\n    prob = fr.pnr(merged)\n    pfr = prob.query('output <= 1')\n    g = fr.plot_spc(pfr).add_legend()\n    plt.title('Probability of Starting Recall With Each Serial Position')\n    if savefig:\n        plt.savefig('figures/pfr.jpeg', bbox_inches='tight')\n    else:\n        plt.show()\n\n    # Conditional response probability as a function of lag\n    crp = fr.lag_crp(merged)\n    g = fr.plot_lag_crp(crp)\n    plt.title('Conditional Response Probability')\n    if savefig:\n        plt.savefig('figures/crp.jpeg', bbox_inches='tight')\n    else:\n        plt.show()\ntry:\n    show_doc(temporal_organization_analyses, title_level=3)\nexcept:\n    pass\n\nDemo\nfrom instance_cmr.models import InstanceCMR\n\nmodel = InstanceCMR(**icmr_parameters)\ntemporal_organization_analyses(model, 100, True)\nfrom instance_cmr.models import CMR\n\nmodel = CMR(**cmr_parameters)\ntemporal_organization_analyses(model, 100, True)"
  }
]